From 2a05e4a6db281947434fc470e023e74bb14e05dd Mon Sep 17 00:00:00 2001
From: jony zhang <jony.zhang@nxp.com>
Date: Wed, 16 Apr 2025 21:43:33 +0800
Subject: [PATCH 1/3] enetc4: port enetc4

Fix compile error, kernel dependency issues, exports duplicate symbol error and register_netdevice error
Continue to debug enetc4 driver on IGH

Signed-off-by: jony zhang <jony.zhang@nxp.com>
Upstream-Status: Pending
---
 devices/enetc/Kbuild.in          |   71 +-
 devices/enetc/Makefile.am        |   28 +-
 devices/enetc/enetc.c            | 2826 +++++++++++++++++++++++++-----
 devices/enetc/enetc.h            |  444 ++++-
 devices/enetc/enetc4_hw.h        |  586 +++++++
 devices/enetc/enetc4_pf.c        | 1788 +++++++++++++++++++
 devices/enetc/enetc_cbdr.c       |  191 +-
 devices/enetc/enetc_debugfs.c    |  463 +++++
 devices/enetc/enetc_devlink.c    |  256 +++
 devices/enetc/enetc_devlink.h    |   30 +
 devices/enetc/enetc_ethtool.c    |  717 +++++++-
 devices/enetc/enetc_hw.h         |  359 ++--
 devices/enetc/enetc_msg.c        |  177 ++
 devices/enetc/enetc_msg.h        |  242 +++
 devices/enetc/enetc_pf.c         |  678 +++----
 devices/enetc/enetc_pf.h         |  139 +-
 devices/enetc/enetc_pf_common.c  | 1757 +++++++++++++++++++
 devices/enetc/enetc_ptp.c        |  145 ++
 devices/enetc/enetc_qos.c        |  984 ++++++++---
 devices/enetc/enetc_vf.c         |   24 +-
 devices/enetc/netc_blk_ctrl.c    |  892 ++++++++++
 devices/enetc/netc_debugfs_lib.c |  616 +++++++
 devices/enetc/netc_tc_lib.c      | 1480 ++++++++++++++++
 devices/enetc/ntmp.c             | 2690 ++++++++++++++++++++++++++++
 devices/enetc/ntmp_private.h     |  503 ++++++
 devices/generic.c                |    3 -
 26 files changed, 16772 insertions(+), 1317 deletions(-)
 create mode 100755 devices/enetc/enetc4_hw.h
 create mode 100755 devices/enetc/enetc4_pf.c
 create mode 100755 devices/enetc/enetc_debugfs.c
 create mode 100755 devices/enetc/enetc_devlink.c
 create mode 100755 devices/enetc/enetc_devlink.h
 create mode 100755 devices/enetc/enetc_msg.c
 create mode 100755 devices/enetc/enetc_msg.h
 create mode 100755 devices/enetc/enetc_pf_common.c
 create mode 100755 devices/enetc/enetc_ptp.c
 create mode 100755 devices/enetc/netc_blk_ctrl.c
 create mode 100755 devices/enetc/netc_debugfs_lib.c
 create mode 100755 devices/enetc/netc_tc_lib.c
 create mode 100755 devices/enetc/ntmp.c
 create mode 100755 devices/enetc/ntmp_private.h

diff --git a/devices/enetc/Kbuild.in b/devices/enetc/Kbuild.in
index 2e3f103..989f3c0 100644
--- a/devices/enetc/Kbuild.in
+++ b/devices/enetc/Kbuild.in
@@ -39,26 +39,61 @@ REV := $(shell if test -s $(TOPDIR)/revision; then \
 	fi)
 
 ifeq (@ENABLE_ENETC@,1)
-	EC_ENETC_VF_OBJ := \
-		enetc.o \
-		enetc_vf.o \
-		enetc_cbdr.o \
-		enetc_qos.o \
-		enetc_ethtool.o
+        EC_ENETC_VF_OBJ := \
+                enetc.o \
+                enetc_cbdr.o \
+                enetc_vf.o \
+                enetc_qos.o \
+                enetc_ethtool.o
 
-	obj-m += ec_enetc_vf.o
-	EC_ENETC_OBJ := \
-		enetc.o \
-		enetc_pf.o \
-		enetc_cbdr.o \
-		enetc_qos.o \
-		enetc_ethtool.o
+        obj-m += ec_enetc_vf.o
+
+        EC_ENETC_CORE_OBJ := \
+                enetc.o \
+                enetc_cbdr.o \
+                enetc_ethtool.o
+
+        obj-m += ec_enetc_core.o
+
+        EC_ENETC_OBJ := \
+                enetc.o \
+                enetc_cbdr.o \
+                enetc_qos.o \
+                enetc_msg.o \
+                enetc_pf_common.o \
+                enetc_ethtool.o \
+                enetc_pf.o 
+
+        obj-m += ec_enetc.o
+
+        EC_ENETC4_OBJ := \
+				enetc.o \
+                enetc_cbdr.o \
+                enetc_qos.o \
+                enetc_pf_common.o \
+                enetc4_pf.o \
+                enetc_devlink.o \
+                enetc_ethtool.o \
+                enetc_msg.o
+
+        obj-m += ec_enetc4.o
+
+        EC_NETC_BLK_CTRL_OBJ := \
+                netc_blk_ctrl.o
+
+        obj-m += ec_netc_blk_ctrl.o
+
+        ec_enetc_core-objs := $(EC_ENETC_CORE_OBJ)
+        ec_netc_blk_ctrl-objs := $(EC_NETC_BLK_CTRL_OBJ)
+        ec_enetc4-objs := $(EC_ENETC4_OBJ)
+        ec_enetc-objs := $(EC_ENETC_OBJ)
+        ec_enetc_vf-objs := $(EC_ENETC_VF_OBJ)
+        CFLAGS_$(EC_ENETC_VF_OBJ) = -DREV=$(REV)
+        CFLAGS_$(EC_ENETC_OBJ) = -DREV=$(REV)
+        CFLAGS_$(EC_ENETC4_OBJ) = -DREV=$(REV)
+        CFLAGS_$(EC_NETC_BLK_CTRL_OBJ) = -DREV=$(REV)
+        CFLAGS_$(EC_ENETC_CORE_OBJ) = -DREV=$(REV)
 
-	obj-m += ec_enetc.o
-	ec_enetc-objs := $(EC_ENETC_OBJ)
-	ec_enetc_vf-objs := $(EC_ENETC_VF_OBJ)
-	CFLAGS_$(EC_ENETC_VF_OBJ) = -DREV=$(REV)
-	CFLAGS_$(EC_ENETC_OBJ) = -DREV=$(REV)
 endif
 
 KBUILD_EXTRA_SYMBOLS := \
diff --git a/devices/enetc/Makefile.am b/devices/enetc/Makefile.am
index 96cf5b4..8889be7 100644
--- a/devices/enetc/Makefile.am
+++ b/devices/enetc/Makefile.am
@@ -28,12 +28,28 @@
 #------------------------------------------------------------------------------
 
 EXTRA_DIST = \
-	enetc.c \
-	enetc_ethtool.c \
-	enetc_cbdr.c \
-	enetc_qos.c \
-	enetc_vf.c \
-	enetc_pf.c
+        enetc.c \
+        enetc_hw.h \
+        enetc_ethtool.c \
+        enetc_cbdr.c \
+        enetc_qos.c \
+        enetc_vf.c \
+        enetc_pf.c \
+        enetc_pf.h \
+        netc_blk_ctrl.c \
+        enetc_pf_common.c \
+        enetc_devlink.c \
+        enetc_devlink.h \
+        enetc_msg.c \
+        enetc_msg.h \
+        enetc_cbdr.c \
+        ntmp.c \
+        ntmp_private.h \
+        netc_debugfs_lib.c \
+        netc_tc_lib.c \
+        enetc_ptp.c \
+        enetc4_hw.h \
+        enetc4_pf.c
 
 BUILT_SOURCES = \
 	Kbuild
diff --git a/devices/enetc/enetc.c b/devices/enetc/enetc.c
index 3e04a67..04f83b6 100644
--- a/devices/enetc/enetc.c
+++ b/devices/enetc/enetc.c
@@ -2,10 +2,15 @@
 /* Copyright 2017-2019 NXP */
 
 #include "enetc.h"
+#include <linux/clk.h>
 #include <linux/tcp.h>
 #include <linux/udp.h>
 #include <linux/vmalloc.h>
+#include <linux/ptp_classify.h>
 #include <linux/ptp_clock_kernel.h>
+#include <net/ip6_checksum.h>
+#include <net/pkt_sched.h>
+#include <net/tso.h>
 #include "../ecdev.h"
 
 /* ENETC overhead: optional extension BD + 1 BD gap */
@@ -14,150 +19,1042 @@
 #define ENETC_MAX_SKB_FRAGS	13
 #define ENETC_TXBDS_MAX_NEEDED	ENETC_TXBDS_NEEDED(ENETC_MAX_SKB_FRAGS + 1)
 
-static int enetc_map_tx_buffs(struct enetc_bdr *tx_ring, struct sk_buff *skb,
-			      struct enetc_ndev_priv *priv);
+u32 ec_enetc_port_mac_rd(struct enetc_si *si, u32 reg)
+{
+    if (si->hw_features & ENETC_SI_F_PPM)
+        return 0;
+
+    return enetc_port_rd(&si->hw, reg);
+}
 
-netdev_tx_t enetc_xmit(struct sk_buff *skb, struct net_device *ndev)
+void ec_enetc_port_mac_wr(struct enetc_si *si, u32 reg, u32 val)
 {
-	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-	struct enetc_bdr *tx_ring;
-	int count;
+    if (si->hw_features & ENETC_SI_F_PPM)
+        return;
 
-	tx_ring = priv->tx_ring[0];
+    enetc_port_wr(&si->hw, reg, val);
+    if (si->hw_features & ENETC_SI_F_QBU)
+        enetc_port_wr(&si->hw, reg + si->pmac_offset, val);
+}
 
-	enetc_lock_mdio();
-	count = enetc_map_tx_buffs(tx_ring, skb, priv);
-	enetc_unlock_mdio();
-	return NETDEV_TX_OK;
+void ec_enetc_change_preemptible_tcs(struct enetc_ndev_priv *priv,
+                  u8 preemptible_tcs)
+{
+    if (!(priv->si->hw_features & ENETC_SI_F_QBU))
+        return;
+
+    priv->preemptible_tcs = preemptible_tcs;
+    ec_enetc_mm_commit_preemptible_tcs(priv);
 }
 
-static bool enetc_tx_csum(struct sk_buff *skb, union enetc_tx_bd *txbd)
+static int ec_enetc_mac_addr_hash_idx(const u8 *addr)
 {
-	int l3_start, l3_hsize;
-	u16 l3_flags, l4_flags;
+    u64 fold = __swab64(ether_addr_to_u64(addr)) >> 16;
+    u64 mask = 0;
+    int res = 0;
+    int i;
 
-	if (skb->ip_summed != CHECKSUM_PARTIAL)
-		return false;
+    for (i = 0; i < 8; i++)
+        mask |= BIT_ULL(i * 6);
 
-	switch (skb->csum_offset) {
-	case offsetof(struct tcphdr, check):
-		l4_flags = ENETC_TXBD_L4_TCP;
-		break;
-	case offsetof(struct udphdr, check):
-		l4_flags = ENETC_TXBD_L4_UDP;
-		break;
-	default:
-		skb_checksum_help(skb);
-		return false;
-	}
+    for (i = 0; i < 6; i++)
+        res |= (hweight64(fold & (mask << i)) & 0x1) << i;
 
-	l3_start = skb_network_offset(skb);
-	l3_hsize = skb_network_header_len(skb);
+    return res;
+}
 
-	l3_flags = 0;
-	if (skb->protocol == htons(ETH_P_IPV6))
-		l3_flags = ENETC_TXBD_L3_IPV6;
+void ec_enetc_reset_mac_addr_filter(struct enetc_mac_filter *filter)
+{
+    filter->mac_addr_cnt = 0;
 
-	/* write BD fields */
-	txbd->l3_csoff = enetc_txbd_l3_csoff(l3_start, l3_hsize, l3_flags);
-	txbd->l4_csoff = l4_flags;
+    bitmap_zero(filter->mac_hash_table,
+            ENETC_MADDR_HASH_TBL_SZ);
+}
 
-	return true;
+void ec_enetc_add_mac_addr_ht_filter(struct enetc_mac_filter *filter,
+                  const unsigned char *addr)
+{
+    int idx = ec_enetc_mac_addr_hash_idx(addr);
+
+    /* add hash table entry */
+    __set_bit(idx, filter->mac_hash_table);
+    filter->mac_addr_cnt++;
+}
+
+int ec_enetc_vid_hash_idx(unsigned int vid)
+{
+    int res = 0;
+    int i;
+
+    for (i = 0; i < 6; i++)
+        res |= (hweight8(vid & (BIT(i) | BIT(i + 6))) & 0x1) << i;
+
+    return res;
+}
+
+void ec_enetc_refresh_vlan_ht_filter(struct enetc_si *si)
+{
+    int i;
+
+    bitmap_zero(si->vlan_ht_filter, ENETC_VLAN_HT_SIZE);
+    for_each_set_bit(i, si->active_vlans, VLAN_N_VID) {
+        int hidx = ec_enetc_vid_hash_idx(i);
+
+        __set_bit(hidx, si->vlan_ht_filter);
+    }
+}
+
+static int enetc_num_stack_tx_queues(struct enetc_ndev_priv *priv)
+{
+    int num_tx_rings = priv->num_tx_rings;
+
+    if (priv->xdp_prog && !priv->shared_tx_rings)
+        return num_tx_rings - num_possible_cpus();
+
+    return num_tx_rings;
+}
+
+static struct enetc_bdr *enetc_rx_ring_from_xdp_tx_ring(struct enetc_ndev_priv *priv,
+                            struct enetc_bdr *tx_ring)
+{
+    int index = &priv->tx_ring[tx_ring->index] - priv->xdp_tx_ring;
+
+    return priv->rx_ring[index];
+}
+
+static struct sk_buff *enetc_tx_swbd_get_skb(struct enetc_tx_swbd *tx_swbd)
+{
+    if (tx_swbd->is_xdp_tx || tx_swbd->is_xdp_redirect)
+        return NULL;
+
+    return tx_swbd->skb;
+}
+
+static struct xdp_frame *
+enetc_tx_swbd_get_xdp_frame(struct enetc_tx_swbd *tx_swbd)
+{
+    if (tx_swbd->is_xdp_redirect)
+        return tx_swbd->xdp_frame;
+
+    return NULL;
 }
 
 static void enetc_unmap_tx_buff(struct enetc_bdr *tx_ring,
-				struct enetc_tx_swbd *tx_swbd)
+                struct enetc_tx_swbd *tx_swbd)
 {
-	if (tx_swbd->is_dma_page)
-		dma_unmap_page(tx_ring->dev, tx_swbd->dma,
-			       tx_swbd->len, DMA_TO_DEVICE);
-	else
-		dma_unmap_single(tx_ring->dev, tx_swbd->dma,
-				 tx_swbd->len, DMA_TO_DEVICE);
-	tx_swbd->dma = 0;
+    /* For XDP_TX, pages come from RX, whereas for the other contexts where
+     * we have is_dma_page_set, those come from skb_frag_dma_map. We need
+     * to match the DMA mapping length, so we need to differentiate those.
+     */
+    if (tx_swbd->is_dma_page)
+        dma_unmap_page(tx_ring->dev, tx_swbd->dma,
+                   tx_swbd->is_xdp_tx ? PAGE_SIZE : tx_swbd->len,
+                   tx_swbd->dir);
+    else
+        dma_unmap_single(tx_ring->dev, tx_swbd->dma,
+                 tx_swbd->len, tx_swbd->dir);
+    tx_swbd->dma = 0;
 }
 
-static void enetc_free_tx_skb(struct enetc_bdr *tx_ring,
-			      struct enetc_tx_swbd *tx_swbd)
+static void enetc_free_tx_frame(struct enetc_bdr *tx_ring,
+                struct enetc_tx_swbd *tx_swbd)
 {
-	if (tx_swbd->dma)
-		enetc_unmap_tx_buff(tx_ring, tx_swbd);
+    struct xdp_frame *xdp_frame = enetc_tx_swbd_get_xdp_frame(tx_swbd);
+    struct sk_buff *skb = enetc_tx_swbd_get_skb(tx_swbd);
+
+    if (tx_swbd->dma)
+        enetc_unmap_tx_buff(tx_ring, tx_swbd);
+
+    if (xdp_frame) {
+        xdp_return_frame(tx_swbd->xdp_frame);
+        tx_swbd->xdp_frame = NULL;
+    } else if (skb) {
+        dev_kfree_skb_any(skb);
+        tx_swbd->skb = NULL;
+    }
+}
 
-	tx_swbd->skb = NULL;
+/* Let H/W know BD ring has been updated */
+static void enetc_update_tx_ring_tail(struct enetc_bdr *tx_ring)
+{
+    /* includes wmb() */
+    enetc_wr_reg_hot(tx_ring->tpir, tx_ring->next_to_use);
 }
 
-static int enetc_map_tx_buffs(struct enetc_bdr *tx_ring, struct sk_buff *skb,
-			      struct enetc_ndev_priv *priv)
+static int enetc_ptp_parse(struct sk_buff *skb, u8 *udp,
+               u8 *msgtype, u8 *twostep,
+               u16 *correction_offset, u16 *body_offset)
 {
-	struct enetc_tx_swbd *tx_swbd;
-	int len = skb_headlen(skb);
-	union enetc_tx_bd temp_bd;
-	union enetc_tx_bd *txbd;
-	int i, count = 0;
-	dma_addr_t dma;
-	u8 flags = 0;
+    unsigned int ptp_class;
+    struct ptp_header *hdr;
+    unsigned int type;
+    u8 *base;
 
-	i = tx_ring->next_to_use;
-	txbd = ENETC_TXBD(*tx_ring, i);
-	prefetchw(txbd);
+    ptp_class = ptp_classify_raw(skb);
+    if (ptp_class == PTP_CLASS_NONE)
+        return -EINVAL;
 
-	dma = dma_map_single(tx_ring->dev, skb->data, len, DMA_TO_DEVICE);
-	if (unlikely(dma_mapping_error(tx_ring->dev, dma)))
-		goto dma_err;
+    hdr = ptp_parse_header(skb, ptp_class);
+    if (!hdr)
+        return -EINVAL;
 
-	temp_bd.addr = cpu_to_le64(dma);
-	temp_bd.buf_len = cpu_to_le16(len);
-	temp_bd.lstatus = 0;
+    type = ptp_class & PTP_CLASS_PMASK;
+    if (type == PTP_CLASS_IPV4 || type == PTP_CLASS_IPV6)
+        *udp = 1;
+    else
+        *udp = 0;
 
-	tx_swbd = &tx_ring->tx_swbd[i];
-	tx_swbd->dma = dma;
-	tx_swbd->len = len;
-	tx_swbd->is_dma_page = 0;
-	count++;
+    *msgtype = ptp_get_msgtype(hdr, ptp_class);
+    *twostep = hdr->flag_field[0] & 0x2;
+
+    base = skb_mac_header(skb);
+    *correction_offset = (u8 *)&hdr->correction - base;
+    *body_offset = (u8 *)hdr + sizeof(struct ptp_header) - base;
+
+    return 0;
+}
+
+static void enetc_set_one_step_ts(struct enetc_si *si, bool udp, int offset)
+{
+    u32 val = ENETC_PM0_SINGLE_STEP_EN;
 
+    val |= ENETC_SET_SINGLE_STEP_OFFSET(offset);
+    val = u32_replace_bits(val, udp ? 1 : 0, ENETC_PM0_SINGLE_STEP_CH);
 
-	if (enetc_tx_csum(skb, &temp_bd))
-		flags |= ENETC_TXBD_FLAGS_CSUM | ENETC_TXBD_FLAGS_L4CS;
-	else if (tx_ring->tsd_enable)
-		flags |= ENETC_TXBD_FLAGS_TSE | ENETC_TXBD_FLAGS_TXSTART;
+    /* the "Correction" field of a packet is updated based on the
+     * current time and the timestamp provided
+     */
+    ec_enetc_port_mac_wr(si, ENETC_PM0_SINGLE_STEP, val);
+}
 
-	/* first BD needs frm_len and offload flags set */
-	temp_bd.frm_len = cpu_to_le16(skb->len);
-	temp_bd.flags = flags;
+static void enetc4_set_one_step_ts(struct enetc_si *si, bool udp, int offset)
+{
+    u32 val = PM_SINGLE_STEP_EN;
 
-	if (flags & ENETC_TXBD_FLAGS_TSE) {
-		u32 temp;
+    val |= PM_SINGLE_STEP_OFFSET(offset);
+    val = u32_replace_bits(val, udp ? 1 : 0, PM_SINGLE_STEP_CH);
+    ec_enetc_port_mac_wr(si, ENETC4_PM_SINGLE_STEP(0), val);
+}
 
-		temp = (skb->skb_mstamp_ns >> 5 & ENETC_TXBD_TXSTART_MASK)
-			| (flags << ENETC_TXBD_FLAGS_OFFSET);
-		temp_bd.txstart = cpu_to_le32(temp);
-	}
+static bool enetc_tx_csum_offload_check(struct sk_buff *skb)
+{
+    if (ip_hdr(skb)->version == 4)
+        return ip_hdr(skb)->protocol == IPPROTO_TCP ||
+               ip_hdr(skb)->protocol == IPPROTO_UDP;
+    else
+        return ipv6_hdr(skb)->nexthdr == NEXTHDR_TCP ||
+               ipv6_hdr(skb)->nexthdr == NEXTHDR_UDP;
+}
+
+static bool enetc_skb_is_tcp(struct sk_buff *skb)
+{
+    if (ip_hdr(skb)->version == 4)
+        return ip_hdr(skb)->protocol == IPPROTO_TCP;
+    else
+        return ipv6_hdr(skb)->nexthdr == NEXTHDR_TCP;
+}
+
+static int enetc_map_tx_buffs(struct enetc_bdr *tx_ring, struct sk_buff *skb);
+
+static void enetc_map_tx_tso_hdr(struct enetc_bdr *tx_ring, struct sk_buff *skb,
+                 struct enetc_tx_swbd *tx_swbd,
+                 union enetc_tx_bd *txbd, int *i, int hdr_len,
+                 int data_len)
+{
+    union enetc_tx_bd txbd_tmp;
+    u8 flags = 0, e_flags = 0;
+    dma_addr_t addr;
+
+    enetc_clear_tx_bd(&txbd_tmp);
+    addr = tx_ring->tso_headers_dma + *i * TSO_HEADER_SIZE;
+
+    if (skb_vlan_tag_present(skb))
+        flags |= ENETC_TXBD_FLAGS_EX;
+
+    txbd_tmp.addr = cpu_to_le64(addr);
+    txbd_tmp.buf_len = cpu_to_le16(hdr_len);
+
+    /* first BD needs frm_len and offload flags set */
+    txbd_tmp.frm_len = cpu_to_le16(hdr_len + data_len);
+    txbd_tmp.flags = flags;
+
+    /* For the TSO header we do not set the dma address since we do not
+     * want it unmapped when we do cleanup. We still set len so that we
+     * count the bytes sent.
+     */
+    tx_swbd->len = hdr_len;
+    tx_swbd->do_twostep_tstamp = false;
+    tx_swbd->check_wb = false;
+
+    /* Actually write the header in the BD */
+    *txbd = txbd_tmp;
+
+    /* Add extension BD for VLAN */
+    if (flags & ENETC_TXBD_FLAGS_EX) {
+        /* Get the next BD */
+        enetc_bdr_idx_inc(tx_ring, i);
+        txbd = ENETC_TXBD(*tx_ring, *i);
+        tx_swbd = &tx_ring->tx_swbd[*i];
+        prefetchw(txbd);
+
+        /* Setup the VLAN fields */
+        enetc_clear_tx_bd(&txbd_tmp);
+        txbd_tmp.ext.vid = cpu_to_le16(skb_vlan_tag_get(skb));
+        txbd_tmp.ext.tpid = 0; /* < C-TAG */
+        e_flags |= ENETC_TXBD_E_FLAGS_VLAN_INS;
+
+        /* Write the BD */
+        txbd_tmp.ext.e_flags = e_flags;
+        *txbd = txbd_tmp;
+    }
+}
+
+static int enetc_map_tx_tso_data(struct enetc_bdr *tx_ring, struct sk_buff *skb,
+                 struct enetc_tx_swbd *tx_swbd,
+                 union enetc_tx_bd *txbd, char *data,
+                 int size, bool last_bd)
+{
+    union enetc_tx_bd txbd_tmp;
+    dma_addr_t addr;
+    u8 flags = 0;
+
+    enetc_clear_tx_bd(&txbd_tmp);
+
+    addr = dma_map_single(tx_ring->dev, data, size, DMA_TO_DEVICE);
+    if (unlikely(dma_mapping_error(tx_ring->dev, addr))) {
+        netdev_err(tx_ring->ndev, "DMA map error\n");
+        return -ENOMEM;
+    }
+
+    if (last_bd) {
+        flags |= ENETC_TXBD_FLAGS_F;
+        tx_swbd->is_eof = 1;
+    }
+
+    txbd_tmp.addr = cpu_to_le64(addr);
+    txbd_tmp.buf_len = cpu_to_le16(size);
+    txbd_tmp.flags = flags;
+
+    tx_swbd->dma = addr;
+    tx_swbd->len = size;
+    tx_swbd->dir = DMA_TO_DEVICE;
+
+    *txbd = txbd_tmp;
+
+    return 0;
+}
+
+static __wsum enetc_tso_hdr_csum(struct tso_t *tso, struct sk_buff *skb,
+                 char *hdr, int hdr_len, int *l4_hdr_len)
+{
+    char *l4_hdr = hdr + skb_transport_offset(skb);
+    int mac_hdr_len = skb_network_offset(skb);
+
+    if (tso->tlen != sizeof(struct udphdr)) {
+        struct tcphdr *tcph = (struct tcphdr *)(l4_hdr);
+
+        tcph->check = 0;
+    } else {
+        struct udphdr *udph = (struct udphdr *)(l4_hdr);
+
+        udph->check = 0;
+    }
+
+    /* Compute the IP checksum. This is necessary since tso_build_hdr()
+     * already incremented the IP ID field.
+     */
+    if (!tso->ipv6) {
+        struct iphdr *iph = (void *)(hdr + mac_hdr_len);
+
+        iph->check = 0;
+        iph->check = ip_fast_csum((unsigned char *)iph, iph->ihl);
+    }
+
+    /* Compute the checksum over the L4 header. */
+    *l4_hdr_len = hdr_len - skb_transport_offset(skb);
+    return csum_partial(l4_hdr, *l4_hdr_len, 0);
+}
+
+static void enetc_tso_complete_csum(struct enetc_bdr *tx_ring, struct tso_t *tso,
+                    struct sk_buff *skb, char *hdr, int len,
+                    __wsum sum)
+{
+    char *l4_hdr = hdr + skb_transport_offset(skb);
+    __sum16 csum_final;
+
+    /* Complete the L4 checksum by appending the pseudo-header to the
+     * already computed checksum.
+     */
+    if (!tso->ipv6)
+        csum_final = csum_tcpudp_magic(ip_hdr(skb)->saddr,
+                           ip_hdr(skb)->daddr,
+                           len, ip_hdr(skb)->protocol, sum);
+    else
+        csum_final = csum_ipv6_magic(&ipv6_hdr(skb)->saddr,
+                         &ipv6_hdr(skb)->daddr,
+                         len, ipv6_hdr(skb)->nexthdr, sum);
+
+    if (tso->tlen != sizeof(struct udphdr)) {
+        struct tcphdr *tcph = (struct tcphdr *)(l4_hdr);
+
+        tcph->check = csum_final;
+    } else {
+        struct udphdr *udph = (struct udphdr *)(l4_hdr);
+
+        udph->check = csum_final;
+    }
+}
+
+/* Calculate expected number of TX descriptors */
+static inline int enetc_lso_count_descs(const struct sk_buff *skb)
+{
+    /* Why add 3 ?
+     * 3 = 1 BD for LSO header + 1 BD for extended BD + 1 BD
+     * for linear area data but not include LSO header, namely
+     * skb_headlen(skb) - lso_hdr_len.
+     */
+    return skb_shinfo(skb)->nr_frags + 3;
+}
+
+static int enetc_lso_get_hdr_len(const struct sk_buff *skb)
+{
+    int hdr_len, tlen;
 
-	/* last BD needs 'F' bit set */
-	flags |= ENETC_TXBD_FLAGS_F;
-	temp_bd.flags = flags;
-	*txbd = temp_bd;
+    tlen = skb_is_gso_tcp(skb) ? tcp_hdrlen(skb) : sizeof(struct udphdr);
+    hdr_len = skb_transport_offset(skb) + tlen;
 
-	tx_ring->tx_swbd[i].skb = skb;
+    return hdr_len;
+}
 
-	enetc_bdr_idx_inc(tx_ring, &i);
-	tx_ring->next_to_use = i;
+static void enetc_lso_start(struct sk_buff *skb, struct enetc_lso_t *lso)
+{
+    lso->lso_seg_size = skb_shinfo(skb)->gso_size;
+    lso->ipv6 = vlan_get_protocol(skb) == htons(ETH_P_IPV6);
+    lso->tcp = skb_is_gso_tcp(skb);
+    lso->l3_hdr_len = skb_network_header_len(skb);
+    lso->l3_start = skb_network_offset(skb);
+    lso->hdr_len = enetc_lso_get_hdr_len(skb);
+    lso->total_len = skb->len - lso->hdr_len;
+}
 
-	/* let H/W know BD ring has been updated */
-	enetc_wr_reg_hot(tx_ring->tpir, i); /* includes wmb() */
+static void enetc_lso_map_hdr(struct enetc_bdr *tx_ring, struct sk_buff *skb,
+                  int *i, struct enetc_lso_t *lso)
+{
+    union enetc_tx_bd txbd_tmp, *txbd;
+    struct enetc_tx_swbd *tx_swbd;
+    u16 frm_len, frm_len_ext;
+    u8 flags, e_flags = 0;
+    dma_addr_t addr;
+    char *hdr;
+
+    /* Get the fisrt BD of the LSO BDs chain. */
+    txbd = ENETC_TXBD(*tx_ring, *i);
+    tx_swbd = &tx_ring->tx_swbd[*i];
+    prefetchw(txbd);
+
+    /* Prepare LSO header: MAC + IP + TCP/UDP */
+    hdr = tx_ring->tso_headers + *i * TSO_HEADER_SIZE;
+    memcpy(hdr, skb->data, lso->hdr_len);
+    addr = tx_ring->tso_headers_dma + *i * TSO_HEADER_SIZE;
+
+    frm_len = lso->total_len & 0xffff;
+    frm_len_ext = (lso->total_len >> 16) & 0xf;
+
+    /* Set the flags of the first BD. */
+    flags = ENETC_TXBD_FLAGS_EX | ENETC_TXBD_FLAGS_CSUM_LSO |
+        ENETC_TXBD_FLAGS_LSO | ENETC_TXBD_FLAGS_L4CS;
+
+    enetc_clear_tx_bd(&txbd_tmp);
+    txbd_tmp.addr = cpu_to_le64(addr);
+    txbd_tmp.hdr_len = cpu_to_le16(lso->hdr_len);
+
+    /* first BD needs frm_len and offload flags set */
+    txbd_tmp.frm_len = cpu_to_le16(frm_len);
+    txbd_tmp.flags = flags;
+
+    if (lso->tcp)
+        txbd_tmp.l4t = ENETC_TXBD_L4T_TCP;
+    else
+        txbd_tmp.l4t = ENETC_TXBD_L4T_UDP;
+
+    if (lso->ipv6)
+        txbd_tmp.l3t = 1;
+    else
+        txbd_tmp.ipcs = 1;
+
+    /*  l3_hdr_size in 32-bits (4 bytes) */
+    txbd_tmp.l3_hdr_size = lso->l3_hdr_len / 4;
+    txbd_tmp.l3_start = lso->l3_start;
+
+    /* For the LSO header we do not set the dma address since
+     * we do not want it unmapped when we do cleanup. We still
+     * set len so that we count the bytes sent.
+     */
+    tx_swbd->len = lso->hdr_len;
+    tx_swbd->do_twostep_tstamp = false;
+    tx_swbd->check_wb = false;
+    tx_swbd->check_wb = false;
+
+    /* Actually write the header in the BD */
+    *txbd = txbd_tmp;
+
+    /* Get the next BD, and the next BD is extended BD. */
+    enetc_bdr_idx_inc(tx_ring, i);
+    txbd = ENETC_TXBD(*tx_ring, *i);
+    tx_swbd = &tx_ring->tx_swbd[*i];
+    prefetchw(txbd);
+
+    enetc_clear_tx_bd(&txbd_tmp);
+    if (skb_vlan_tag_present(skb)) {
+        /* Setup the VLAN fields */
+        txbd_tmp.ext.vid = cpu_to_le16(skb_vlan_tag_get(skb));
+        txbd_tmp.ext.tpid = 0; /* < C-TAG */
+        e_flags = ENETC_TXBD_E_FLAGS_VLAN_INS;
+    }
+
+    /* Write the BD */
+    txbd_tmp.ext.e_flags = e_flags;
+    txbd_tmp.ext.lso_sg_size = cpu_to_le16(lso->lso_seg_size);
+    txbd_tmp.ext.frm_len_ext = cpu_to_le16(frm_len_ext);
+    *txbd = txbd_tmp;
+}
 
-	return count;
+static int enetc_lso_map_data(struct enetc_bdr *tx_ring, struct sk_buff *skb,
+                  int *i, struct enetc_lso_t *lso, int *count)
+{
+    union enetc_tx_bd txbd_tmp, *txbd = NULL;
+    struct enetc_tx_swbd *tx_swbd;
+    skb_frag_t *frag;
+    dma_addr_t dma;
+    u8 flags = 0;
+    int len, f;
+
+    len = skb_headlen(skb) - lso->hdr_len;
+    if (len > 0) {
+        dma = dma_map_single(tx_ring->dev, skb->data + lso->hdr_len,
+                     len, DMA_TO_DEVICE);
+        if (unlikely(dma_mapping_error(tx_ring->dev, dma))) {
+            netdev_err(tx_ring->ndev, "DMA map error\n");
+            goto dma_err;
+        }
+
+        enetc_bdr_idx_inc(tx_ring, i);
+        txbd = ENETC_TXBD(*tx_ring, *i);
+        tx_swbd = &tx_ring->tx_swbd[*i];
+        prefetchw(txbd);
+        *count += 1;
+
+        enetc_clear_tx_bd(&txbd_tmp);
+        txbd_tmp.addr = cpu_to_le64(dma);
+        txbd_tmp.buf_len = cpu_to_le16(len);
+
+        tx_swbd->dma = dma;
+        tx_swbd->len = len;
+        tx_swbd->is_dma_page = 0;
+        tx_swbd->dir = DMA_TO_DEVICE;
+    }
+
+    frag = &skb_shinfo(skb)->frags[0];
+    for (f = 0; f < skb_shinfo(skb)->nr_frags; f++, frag++) {
+        if (txbd)
+            *txbd = txbd_tmp;
+
+        len = skb_frag_size(frag);
+        dma = skb_frag_dma_map(tx_ring->dev, frag, 0, len,
+                       DMA_TO_DEVICE);
+        if (unlikely(dma_mapping_error(tx_ring->dev, dma))) {
+            netdev_err(tx_ring->ndev, "DMA map error\n");
+            goto dma_err;
+        }
+
+        /* Get the next BD */
+        enetc_bdr_idx_inc(tx_ring, i);
+        txbd = ENETC_TXBD(*tx_ring, *i);
+        tx_swbd = &tx_ring->tx_swbd[*i];
+        prefetchw(txbd);
+        *count += 1;
+
+        enetc_clear_tx_bd(&txbd_tmp);
+        txbd_tmp.addr = cpu_to_le64(dma);
+        txbd_tmp.buf_len = cpu_to_le16(len);
+
+        tx_swbd->dma = dma;
+        tx_swbd->len = len;
+        tx_swbd->is_dma_page = 1;
+        tx_swbd->dir = DMA_TO_DEVICE;
+    }
+
+    /* Last BD needs 'F' bit set */
+    flags |= ENETC_TXBD_FLAGS_F;
+    txbd_tmp.flags = flags;
+    *txbd = txbd_tmp;
+
+    tx_swbd->is_eof = 1;
+    tx_swbd->skb = skb;
+
+    return 0;
 
 dma_err:
-	dev_err(tx_ring->dev, "DMA map error");
-	do {
-		tx_swbd = &tx_ring->tx_swbd[i];
-		enetc_free_tx_skb(tx_ring, tx_swbd);
-		if (i == 0)
-			i = tx_ring->bd_count;
-		i--;
-	} while (count--);
-	return 0;
+    return -ENOMEM;
+}
+
+static int enetc_lso_hw_offload(struct enetc_bdr *tx_ring, struct sk_buff *skb)
+{
+    struct enetc_tx_swbd *tx_swbd;
+    struct enetc_lso_t lso;
+    int err, i, count = 0;
+
+    /* Initialize the LSO handler */
+    enetc_lso_start(skb, &lso);
+    i = tx_ring->next_to_use;
+
+    enetc_lso_map_hdr(tx_ring, skb, &i, &lso);
+    /* First BD and an extend BD */
+    count += 2;
+
+    err = enetc_lso_map_data(tx_ring, skb, &i, &lso, &count);
+    if (err)
+        goto dma_err;
+
+    /* Go to the next BD */
+    enetc_bdr_idx_inc(tx_ring, &i);
+    tx_ring->next_to_use = i;
+    enetc_update_tx_ring_tail(tx_ring);
+
+    return count;
+
+dma_err:
+    do {
+        tx_swbd = &tx_ring->tx_swbd[i];
+        enetc_free_tx_frame(tx_ring, tx_swbd);
+        if (i == 0)
+            i = tx_ring->bd_count;
+        i--;
+    } while (count--);
+
+    return 0;
+}
+
+static int enetc_map_tx_tso_buffs(struct enetc_bdr *tx_ring, struct sk_buff *skb)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(tx_ring->ndev);
+    int hdr_len, total_len, data_len;
+    struct enetc_tx_swbd *tx_swbd;
+    union enetc_tx_bd *txbd;
+    struct tso_t tso;
+    __wsum csum, csum2;
+    int count = 0, pos;
+    int err, i, bd_data_num;
+
+    /* Initialize the TSO handler, and prepare the first payload */
+    hdr_len = tso_start(skb, &tso);
+    total_len = skb->len - hdr_len;
+    i = tx_ring->next_to_use;
+
+    while (total_len > 0) {
+        char *hdr;
+
+        /* Get the BD */
+        txbd = ENETC_TXBD(*tx_ring, i);
+        tx_swbd = &tx_ring->tx_swbd[i];
+        prefetchw(txbd);
+
+        /* Determine the length of this packet */
+        data_len = min_t(int, skb_shinfo(skb)->gso_size, total_len);
+        total_len -= data_len;
+
+        /* prepare packet headers: MAC + IP + TCP */
+        hdr = tx_ring->tso_headers + i * TSO_HEADER_SIZE;
+        tso_build_hdr(skb, hdr, &tso, data_len, total_len == 0);
+
+        /* compute the csum over the L4 header */
+        csum = enetc_tso_hdr_csum(&tso, skb, hdr, hdr_len, &pos);
+        enetc_map_tx_tso_hdr(tx_ring, skb, tx_swbd, txbd, &i, hdr_len, data_len);
+        bd_data_num = 0;
+        count++;
+
+        while (data_len > 0) {
+            int size;
+
+            size = min_t(int, tso.size, data_len);
+
+            /* Advance the index in the BDR */
+            enetc_bdr_idx_inc(tx_ring, &i);
+            txbd = ENETC_TXBD(*tx_ring, i);
+            tx_swbd = &tx_ring->tx_swbd[i];
+            prefetchw(txbd);
+
+            /* Compute the checksum over this segment of data and
+             * add it to the csum already computed (over the L4
+             * header and possible other data segments).
+             */
+            csum2 = csum_partial(tso.data, size, 0);
+            csum = csum_block_add(csum, csum2, pos);
+            pos += size;
+
+            err = enetc_map_tx_tso_data(tx_ring, skb, tx_swbd, txbd,
+                            tso.data, size,
+                            size == data_len);
+            if (err)
+                goto err_map_data;
+
+            data_len -= size;
+            count++;
+            bd_data_num++;
+            tso_build_data(skb, &tso, size);
+
+            if (unlikely(bd_data_num >= priv->max_frags_bd && data_len))
+                goto err_chained_bd;
+        }
+
+        enetc_tso_complete_csum(tx_ring, &tso, skb, hdr, pos, csum);
+
+        if (total_len == 0)
+            tx_swbd->skb = skb;
+
+        /* Go to the next BD */
+        enetc_bdr_idx_inc(tx_ring, &i);
+    }
+
+    tx_ring->next_to_use = i;
+    enetc_update_tx_ring_tail(tx_ring);
+
+    return count;
+
+err_map_data:
+    dev_err(tx_ring->dev, "DMA map error");
+
+err_chained_bd:
+    do {
+        tx_swbd = &tx_ring->tx_swbd[i];
+        enetc_free_tx_frame(tx_ring, tx_swbd);
+        if (i == 0)
+            i = tx_ring->bd_count;
+        i--;
+    } while (count--);
+
+    return 0;
+}
+
+static netdev_tx_t ec_enetc_start_xmit(struct sk_buff *skb,
+                    struct net_device *ndev)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct enetc_bdr *tx_ring;
+    int count;
+
+    /* Queue one-step Sync packet if already locked */
+    if (skb->cb[0] & ENETC_F_TX_ONESTEP_SYNC_TSTAMP) {
+        if (test_and_set_bit_lock(ENETC_TX_ONESTEP_TSTAMP_IN_PROGRESS,
+                      &priv->flags)) {
+            skb_queue_tail(&priv->tx_skbs, skb);
+            return NETDEV_TX_OK;
+        }
+    }
+
+    tx_ring = priv->tx_ring[skb->queue_mapping];
+
+    if (skb_is_gso(skb)) {
+        /* Large Send Offload data lengths of up to 256KB are supported. */
+        if (priv->active_offloads & ENETC_F_LSO &&
+            (skb->len - enetc_lso_get_hdr_len(skb)) <=
+            ENETC_LSO_MAX_DATA_LEN) {
+            /* 1 BD gap */
+            if (enetc_bd_unused(tx_ring) < enetc_lso_count_descs(skb) + 1) {
+                netif_stop_subqueue(ndev, tx_ring->index);
+                return NETDEV_TX_BUSY;
+            }
+
+            enetc_lock_mdio();
+            count = enetc_lso_hw_offload(tx_ring, skb);
+            enetc_unlock_mdio();
+        } else {
+            if (enetc_bd_unused(tx_ring) < tso_count_descs(skb)) {
+                netif_stop_subqueue(ndev, tx_ring->index);
+                return NETDEV_TX_BUSY;
+            }
+
+            enetc_lock_mdio();
+            count = enetc_map_tx_tso_buffs(tx_ring, skb);
+            enetc_unlock_mdio();
+        }
+    } else {
+        if (unlikely(skb_shinfo(skb)->nr_frags > priv->max_frags_bd))
+            if (unlikely(skb_linearize(skb)))
+                goto drop_packet_err;
+
+        count = skb_shinfo(skb)->nr_frags + 1; /* fragments + head */
+        if (enetc_bd_unused(tx_ring) < ENETC_TXBDS_NEEDED(count)) {
+            netif_stop_subqueue(ndev, tx_ring->index);
+            return NETDEV_TX_BUSY;
+        }
+
+        enetc_lock_mdio();
+        count = enetc_map_tx_buffs(tx_ring, skb);
+        enetc_unlock_mdio();
+    }
+
+    if (unlikely(!count))
+        goto drop_packet_err;
+
+    if (enetc_bd_unused(tx_ring) < ENETC_TX_STOP_THRESHOLD)
+        netif_stop_subqueue(ndev, tx_ring->index);
+
+    return NETDEV_TX_OK;
+
+drop_packet_err:
+    dev_kfree_skb_any(skb);
+    return NETDEV_TX_OK;
+}
+
+netdev_tx_t ec_enetc_xmit(struct sk_buff *skb, struct net_device *ndev)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    u8 udp, msgtype, twostep;
+    u16 offset1, offset2;
+
+    /* Mark tx timestamp type on skb->cb[0] if requires */
+    if ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&
+        (priv->active_offloads & ENETC_F_TX_TSTAMP_MASK)) {
+        skb->cb[0] = priv->active_offloads & ENETC_F_TX_TSTAMP_MASK;
+    } else {
+        skb->cb[0] = 0;
+    }
+
+    /* Fall back to two-step timestamp if not one-step Sync packet */
+    if (skb->cb[0] & ENETC_F_TX_ONESTEP_SYNC_TSTAMP) {
+        if (enetc_ptp_parse(skb, &udp, &msgtype, &twostep,
+                    &offset1, &offset2) ||
+            msgtype != PTP_MSGTYPE_SYNC || twostep != 0)
+            skb->cb[0] = ENETC_F_TX_TSTAMP;
+    }
+
+    return ec_enetc_start_xmit(skb, ndev);
+}
+
+static void enetc_free_tx_skb(struct enetc_bdr *tx_ring,
+			      struct enetc_tx_swbd *tx_swbd)
+{
+	if (tx_swbd->dma)
+		enetc_unmap_tx_buff(tx_ring, tx_swbd);
+
+	tx_swbd->skb = NULL;
+}
+
+static int enetc_map_tx_buffs(struct enetc_bdr *tx_ring, struct sk_buff *skb)
+{
+    bool do_vlan, do_onestep_tstamp = false, do_twostep_tstamp = false;
+    struct enetc_ndev_priv *priv = netdev_priv(tx_ring->ndev);
+    struct enetc_si *si = priv->si;
+    struct enetc_hw *hw = &si->hw;
+    struct enetc_tx_swbd *tx_swbd;
+    int len = skb_headlen(skb);
+    union enetc_tx_bd temp_bd;
+    u8 msgtype, twostep, udp;
+    union enetc_tx_bd *txbd;
+    u16 offset1, offset2;
+    int i, count = 0;
+    skb_frag_t *frag;
+    unsigned int f;
+    dma_addr_t dma;
+    u8 flags = 0;
+    int err;
+
+    enetc_clear_tx_bd(&temp_bd);
+    if (skb->ip_summed == CHECKSUM_PARTIAL) {
+        /* Can not support TSD and checksum offload at the same time */
+        if (priv->active_offloads & ENETC_F_CHECKSUM &&
+            enetc_tx_csum_offload_check(skb) && !tx_ring->tsd_enable) {
+            bool is_ipv6 = ip_hdr(skb)->version != 4;
+            bool is_tcp = enetc_skb_is_tcp(skb);
+
+            temp_bd.l3_start = skb_network_offset(skb);
+            temp_bd.ipcs = is_ipv6 ? 0 : 1;
+            temp_bd.l3_hdr_size = skb_network_header_len(skb) / 4;
+            temp_bd.l3t = is_ipv6 ? 1 : 0;
+            temp_bd.l4t = is_tcp ? ENETC_TXBD_L4T_TCP : ENETC_TXBD_L4T_UDP;
+            flags |= ENETC_TXBD_FLAGS_CSUM_LSO | ENETC_TXBD_FLAGS_L4CS;
+        } else {
+            err = skb_checksum_help(skb);
+            if (err) {
+                dev_err(tx_ring->dev, "skb_checksum_help err : %d.\n", err);
+                return 0;
+            }
+        }
+    }
+
+    i = tx_ring->next_to_use;
+    txbd = ENETC_TXBD(*tx_ring, i);
+    prefetchw(txbd);
+
+    dma = dma_map_single(tx_ring->dev, skb->data, len, DMA_TO_DEVICE);
+    if (unlikely(dma_mapping_error(tx_ring->dev, dma)))
+        goto dma_err;
+
+    temp_bd.addr = cpu_to_le64(dma);
+    temp_bd.buf_len = cpu_to_le16(len);
+
+    tx_swbd = &tx_ring->tx_swbd[i];
+    tx_swbd->dma = dma;
+    tx_swbd->len = len;
+    tx_swbd->is_dma_page = 0;
+    tx_swbd->dir = DMA_TO_DEVICE;
+    count++;
+
+    do_vlan = skb_vlan_tag_present(skb);
+    if (skb->cb[0] & ENETC_F_TX_ONESTEP_SYNC_TSTAMP) {
+        if (enetc_ptp_parse(skb, &udp, &msgtype, &twostep, &offset1,
+                    &offset2) ||
+            msgtype != PTP_MSGTYPE_SYNC || twostep)
+            WARN_ONCE(1, "Bad packet for one-step timestamping\n");
+        else
+            do_onestep_tstamp = true;
+    } else if (skb->cb[0] & ENETC_F_TX_TSTAMP) {
+        do_twostep_tstamp = true;
+    }
+
+    tx_swbd->do_twostep_tstamp = do_twostep_tstamp;
+    tx_swbd->qbv_en = !!(priv->active_offloads & ENETC_F_QBV);
+    tx_swbd->check_wb = tx_swbd->do_twostep_tstamp || tx_swbd->qbv_en;
+
+    if (do_vlan || do_onestep_tstamp || do_twostep_tstamp)
+        flags |= ENETC_TXBD_FLAGS_EX;
+
+    if (tx_ring->tsd_enable)
+        flags |= ENETC_TXBD_FLAGS_TSE | ENETC_TXBD_FLAGS_TXSTART;
+
+    /* first BD needs frm_len and offload flags set */
+    temp_bd.frm_len = cpu_to_le16(skb->len);
+    temp_bd.flags = flags;
+
+    if (flags & ENETC_TXBD_FLAGS_TSE)
+        temp_bd.txstart = enetc_txbd_set_tx_start(skb->skb_mstamp_ns,
+                              flags);
+
+    if (flags & ENETC_TXBD_FLAGS_EX) {
+        u8 e_flags = 0;
+        *txbd = temp_bd;
+        enetc_clear_tx_bd(&temp_bd);
+
+        /* add extension BD for VLAN and/or timestamping */
+        flags = 0;
+        tx_swbd++;
+        txbd++;
+        i++;
+        if (unlikely(i == tx_ring->bd_count)) {
+            i = 0;
+            tx_swbd = tx_ring->tx_swbd;
+            txbd = ENETC_TXBD(*tx_ring, 0);
+        }
+        prefetchw(txbd);
+
+        if (do_vlan) {
+            temp_bd.ext.vid = cpu_to_le16(skb_vlan_tag_get(skb));
+            temp_bd.ext.tpid = 0; /* < C-TAG */
+            e_flags |= ENETC_TXBD_E_FLAGS_VLAN_INS;
+        }
+
+        /* For the moment, only PF supports one-step timestamp. */
+        if (do_onestep_tstamp && enetc_si_is_pf(si)) {
+            u32 lo, hi;
+            u64 sec, nsec;
+            u8 *data;
+
+            lo = enetc_rd_hot(hw, ENETC_SICTR0);
+            hi = enetc_rd_hot(hw, ENETC_SICTR1);
+            sec = (u64)hi << 32 | lo;
+            nsec = do_div(sec, 1000000000);
+
+            /* Configure extension BD */
+            temp_bd.ext.tstamp = cpu_to_le32(lo & 0x3fffffff);
+            e_flags |= ENETC_TXBD_E_FLAGS_ONE_STEP_PTP;
+
+            /* Update originTimestamp field of Sync packet
+             * - 48 bits seconds field
+             * - 32 bits nanseconds field
+             */
+            data = skb_mac_header(skb);
+            *(__be16 *)(data + offset2) =
+                htons((sec >> 32) & 0xffff);
+            *(__be32 *)(data + offset2 + 2) =
+                htonl(sec & 0xffffffff);
+            *(__be32 *)(data + offset2 + 6) = htonl(nsec);
+
+            /* Configure single-step register */
+            if (is_enetc_rev1(si))
+                enetc_set_one_step_ts(si, !!udp, offset1);
+            else
+                enetc4_set_one_step_ts(si, !!udp, offset1);
+        } else if (do_twostep_tstamp) {
+            skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+            e_flags |= ENETC_TXBD_E_FLAGS_TWO_STEP_PTP;
+        }
+
+        temp_bd.ext.e_flags = e_flags;
+        count++;
+    }
+
+    frag = &skb_shinfo(skb)->frags[0];
+    for (f = 0; f < skb_shinfo(skb)->nr_frags; f++, frag++) {
+        len = skb_frag_size(frag);
+        dma = skb_frag_dma_map(tx_ring->dev, frag, 0, len,
+                       DMA_TO_DEVICE);
+        if (dma_mapping_error(tx_ring->dev, dma))
+            goto dma_err;
+
+        *txbd = temp_bd;
+        enetc_clear_tx_bd(&temp_bd);
+
+        flags = 0;
+        tx_swbd++;
+        txbd++;
+        i++;
+        if (unlikely(i == tx_ring->bd_count)) {
+            i = 0;
+            tx_swbd = tx_ring->tx_swbd;
+            txbd = ENETC_TXBD(*tx_ring, 0);
+        }
+        prefetchw(txbd);
+
+        temp_bd.addr = cpu_to_le64(dma);
+        temp_bd.buf_len = cpu_to_le16(len);
+
+        tx_swbd->dma = dma;
+        tx_swbd->len = len;
+        tx_swbd->is_dma_page = 1;
+        tx_swbd->dir = DMA_TO_DEVICE;
+        count++;
+    }
+
+    /* last BD needs 'F' bit set */
+    flags |= ENETC_TXBD_FLAGS_F;
+    temp_bd.flags = flags;
+    *txbd = temp_bd;
+
+    tx_ring->tx_swbd[i].is_eof = true;
+    tx_ring->tx_swbd[i].skb = skb;
+
+    enetc_bdr_idx_inc(tx_ring, &i);
+    tx_ring->next_to_use = i;
+
+    skb_tx_timestamp(skb);
+
+    enetc_update_tx_ring_tail(tx_ring);
+
+    return count;
+
+dma_err:
+    dev_err(tx_ring->dev, "DMA map error");
+
+    do {
+        tx_swbd = &tx_ring->tx_swbd[i];
+        enetc_free_tx_frame(tx_ring, tx_swbd);
+        if (i == 0)
+            i = tx_ring->bd_count;
+        i--;
+    } while (count--);
+
+    return 0;
 }
 
 static bool enetc_clean_tx_ring(struct enetc_bdr *tx_ring);
@@ -468,7 +1365,9 @@ static int enetc_clean_rx_ring(struct enetc_bdr *rx_ring)
 		}
 
 		rx_byte_cnt += skb->len;
-		ecdev_receive(priv->ecdev, skb->data, size);
+		if ((__force u16)skb->protocol == (__force u16)htons(0x88a4)) {
+			ecdev_receive(priv->ecdev, skb->data - 14, size - 6);
+		}
 		dev_kfree_skb(skb);
 		enetc_unlock_mdio();
 		rx_frm_cnt++;
@@ -482,9 +1381,226 @@ static int enetc_clean_rx_ring(struct enetc_bdr *rx_ring)
 	return rx_frm_cnt;
 }
 
+static void enetc_xdp_map_tx_buff(struct enetc_bdr *tx_ring, int i,
+                  struct enetc_tx_swbd *tx_swbd,
+                  int frm_len)
+{
+    union enetc_tx_bd *txbd = ENETC_TXBD(*tx_ring, i);
+
+    prefetchw(txbd);
+
+    dma_sync_single_range_for_device(tx_ring->dev, tx_swbd->dma,
+                     tx_swbd->page_offset,
+                     tx_swbd->len,
+                     tx_swbd->dir);
+
+    enetc_clear_tx_bd(txbd);
+    txbd->addr = cpu_to_le64(tx_swbd->dma + tx_swbd->page_offset);
+    txbd->buf_len = cpu_to_le16(tx_swbd->len);
+    txbd->frm_len = cpu_to_le16(frm_len);
+
+    memcpy(&tx_ring->tx_swbd[i], tx_swbd, sizeof(*tx_swbd));
+}
+
+/* Puts in the TX ring one XDP frame, mapped as an array of TX software buffer
+ * descriptors.
+ */
+static bool enetc_xdp_tx(struct enetc_bdr *tx_ring,
+             struct enetc_tx_swbd *xdp_tx_arr, int num_tx_swbd)
+{
+    struct enetc_tx_swbd *tmp_tx_swbd = xdp_tx_arr;
+    int i, k, frm_len = tmp_tx_swbd->len;
+
+    if (unlikely(enetc_bd_unused(tx_ring) < ENETC_TXBDS_NEEDED(num_tx_swbd)))
+        return false;
+
+    while (unlikely(!tmp_tx_swbd->is_eof)) {
+        tmp_tx_swbd++;
+        frm_len += tmp_tx_swbd->len;
+    }
+
+    i = tx_ring->next_to_use;
+
+    for (k = 0; k < num_tx_swbd; k++) {
+        struct enetc_tx_swbd *xdp_tx_swbd = &xdp_tx_arr[k];
+
+        enetc_xdp_map_tx_buff(tx_ring, i, xdp_tx_swbd, frm_len);
+
+        /* last BD needs 'F' bit set */
+        if (xdp_tx_swbd->is_eof) {
+            union enetc_tx_bd *txbd = ENETC_TXBD(*tx_ring, i);
+
+            txbd->flags = ENETC_TXBD_FLAGS_F;
+        }
+
+        enetc_bdr_idx_inc(tx_ring, &i);
+    }
+
+    tx_ring->next_to_use = i;
+
+    return true;
+}
+
+static int enetc_xdp_frame_to_xdp_tx_swbd(struct enetc_bdr *tx_ring,
+                      struct enetc_tx_swbd *xdp_tx_arr,
+                      struct xdp_frame *xdp_frame)
+{
+    struct enetc_tx_swbd *xdp_tx_swbd = &xdp_tx_arr[0];
+    struct skb_shared_info *shinfo;
+    void *data = xdp_frame->data;
+    int len = xdp_frame->len;
+    skb_frag_t *frag;
+    dma_addr_t dma;
+    unsigned int f;
+    int n = 0;
+
+    dma = dma_map_single(tx_ring->dev, data, len, DMA_TO_DEVICE);
+    if (unlikely(dma_mapping_error(tx_ring->dev, dma))) {
+        netdev_err(tx_ring->ndev, "DMA map error\n");
+        return -1;
+    }
+
+    xdp_tx_swbd->dma = dma;
+    xdp_tx_swbd->dir = DMA_TO_DEVICE;
+    xdp_tx_swbd->len = len;
+    xdp_tx_swbd->is_xdp_redirect = true;
+    xdp_tx_swbd->is_eof = false;
+    xdp_tx_swbd->xdp_frame = NULL;
+
+    n++;
+
+    if (!xdp_frame_has_frags(xdp_frame))
+        goto out;
+
+    xdp_tx_swbd = &xdp_tx_arr[n];
+
+    shinfo = xdp_get_shared_info_from_frame(xdp_frame);
+
+    for (f = 0, frag = &shinfo->frags[0]; f < shinfo->nr_frags;
+         f++, frag++) {
+        data = skb_frag_address(frag);
+        len = skb_frag_size(frag);
+
+        dma = dma_map_single(tx_ring->dev, data, len, DMA_TO_DEVICE);
+        if (unlikely(dma_mapping_error(tx_ring->dev, dma))) {
+            /* Undo the DMA mapping for all fragments */
+            while (--n >= 0)
+                enetc_unmap_tx_buff(tx_ring, &xdp_tx_arr[n]);
+
+            netdev_err(tx_ring->ndev, "DMA map error\n");
+            return -1;
+        }
+
+        xdp_tx_swbd->dma = dma;
+        xdp_tx_swbd->dir = DMA_TO_DEVICE;
+        xdp_tx_swbd->len = len;
+        xdp_tx_swbd->is_xdp_redirect = true;
+        xdp_tx_swbd->is_eof = false;
+        xdp_tx_swbd->xdp_frame = NULL;
+
+        n++;
+        xdp_tx_swbd = &xdp_tx_arr[n];
+    }
+out:
+    xdp_tx_arr[n - 1].is_eof = true;
+    xdp_tx_arr[n - 1].xdp_frame = xdp_frame;
+
+    return n;
+}
+
+static inline void enetc_tx_queue_lock(struct enetc_bdr *tx_ring, int cpu)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(tx_ring->ndev);
+    struct netdev_queue *nq;
+
+    if (priv->shared_tx_rings) {
+        nq = netdev_get_tx_queue(tx_ring->ndev, tx_ring->index);
+        __netif_tx_lock(nq, cpu);
+        txq_trans_cond_update(nq);
+    }
+}
+
+static inline void enetc_tx_queue_unlock(struct enetc_bdr *tx_ring)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(tx_ring->ndev);
+    struct netdev_queue *nq;
+
+    if (priv->shared_tx_rings) {
+        nq = netdev_get_tx_queue(tx_ring->ndev, tx_ring->index);
+        __netif_tx_unlock(nq);
+    }
+}
+
+int ec_enetc_xdp_xmit(struct net_device *ndev, int num_frames,
+           struct xdp_frame **frames, u32 flags)
+{
+    struct enetc_tx_swbd *xdp_redirect_arr __free(kfree);
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct skb_shared_info *shinfo;
+    int cpu = smp_processor_id();
+    struct enetc_bdr *tx_ring;
+    int xdp_tx_bd_cnt, i, k;
+    int xdp_tx_frm_cnt = 0;
+    int max_txbd_num;
+    int ring_index;
+
+    if (unlikely(test_bit(ENETC_TX_DOWN, &priv->flags)))
+        return -ENETDOWN;
+
+    max_txbd_num = ENETC_TXBDS_NEEDED(priv->max_frags_bd);
+    xdp_redirect_arr = kcalloc(max_txbd_num, sizeof(*xdp_redirect_arr),
+                   GFP_ATOMIC);
+    if (unlikely(!xdp_redirect_arr))
+        return -ENOMEM;
+
+    enetc_lock_mdio();
+
+    ring_index = priv->shared_tx_rings ? cpu % priv->num_tx_rings : cpu;
+    tx_ring = priv->xdp_tx_ring[ring_index];
+    enetc_tx_queue_lock(tx_ring, cpu);
+
+    prefetchw(ENETC_TXBD(*tx_ring, tx_ring->next_to_use));
+
+    for (k = 0; k < num_frames; k++) {
+        if (xdp_frame_has_frags(frames[k])) {
+            shinfo = xdp_get_shared_info_from_frame(frames[k]);
+            if (unlikely((shinfo->nr_frags + 1) > max_txbd_num))
+                break;
+        }
+
+        xdp_tx_bd_cnt = enetc_xdp_frame_to_xdp_tx_swbd(tx_ring,
+                                   xdp_redirect_arr,
+                                   frames[k]);
+        if (unlikely(xdp_tx_bd_cnt < 0))
+            break;
+
+        if (unlikely(!enetc_xdp_tx(tx_ring, xdp_redirect_arr,
+                       xdp_tx_bd_cnt))) {
+            for (i = 0; i < xdp_tx_bd_cnt; i++)
+                enetc_unmap_tx_buff(tx_ring,
+                            &xdp_redirect_arr[i]);
+            tx_ring->stats.xdp_tx_drops++;
+            break;
+        }
+
+        xdp_tx_frm_cnt++;
+    }
+
+    if (unlikely((flags & XDP_XMIT_FLUSH) || k != xdp_tx_frm_cnt))
+        enetc_update_tx_ring_tail(tx_ring);
+
+    tx_ring->stats.xdp_tx += xdp_tx_frm_cnt;
+
+    enetc_tx_queue_unlock(tx_ring);
+
+    enetc_unlock_mdio();
+
+    return xdp_tx_frm_cnt;
+}
+
 /* Probing and Init */
 #define ENETC_MAX_RFS_SIZE 64
-void enetc_get_si_caps(struct enetc_si *si)
+void ec_enetc_get_si_caps(struct enetc_si *si)
 {
 	struct enetc_hw *hw = &si->hw;
 	u32 val;
@@ -517,183 +1633,297 @@ void enetc_get_si_caps(struct enetc_si *si)
 		si->hw_features |= ENETC_SI_F_PSFP;
 }
 
-static int enetc_dma_alloc_bdr(struct enetc_bdr *r, size_t bd_size)
+static int enetc_dma_alloc_bdr(struct enetc_bdr_resource *res)
 {
-	r->bd_base = dma_alloc_coherent(r->dev, r->bd_count * bd_size,
-					&r->bd_dma_base, GFP_KERNEL);
-	if (!r->bd_base)
-		return -ENOMEM;
-
-	/* h/w requires 128B alignment */
-	if (!IS_ALIGNED(r->bd_dma_base, 128)) {
-		dma_free_coherent(r->dev, r->bd_count * bd_size, r->bd_base,
-				  r->bd_dma_base);
-		return -EINVAL;
-	}
+    size_t bd_base_size = res->bd_count * res->bd_size;
 
-	return 0;
-}
+    res->bd_base = dma_alloc_coherent(res->dev, bd_base_size,
+                      &res->bd_dma_base, GFP_KERNEL);
+    if (!res->bd_base)
+        return -ENOMEM;
 
-int enetc_alloc_rings(struct enetc_ndev_priv *priv)
-{
-	struct enetc_bdr *bdrs;
-	int i, j;
-	bdrs = kmalloc_array(sizeof(struct enetc_bdr), (priv->num_tx_rings
-				+priv->num_rx_rings), GFP_KERNEL);
-	if (!bdrs) {
-		return -ENOMEM;
-	}
-	for (i = 0; i < priv->num_tx_rings; i++) {
-		bdrs[i].ndev = priv->ndev;
-		bdrs[i].dev = priv->dev;
-		bdrs[i].bd_count = priv->tx_bd_count;
-		priv->tx_ring[i] = &bdrs[i];
-	}
+    /* h/w requires 128B alignment */
+    if (!IS_ALIGNED(res->bd_dma_base, 128)) {
+        dma_free_coherent(res->dev, bd_base_size, res->bd_base,
+                  res->bd_dma_base);
+        return -EINVAL;
+    }
 
-	for (j = 0; j < priv->num_rx_rings; i++, j++) {
-		bdrs[i].ndev = priv->ndev;
-		bdrs[i].dev = priv->dev;
-		bdrs[i].bd_count = priv->tx_bd_count;
-		priv->rx_ring[j] = &bdrs[i];
-	}
-	return 0;
+    return 0;
 }
 
-void enetc_free_rings(struct enetc_ndev_priv *priv)
+static void enetc_dma_free_bdr(const struct enetc_bdr_resource *res)
 {
-	kfree(priv->tx_ring[0]);
+    size_t bd_base_size = res->bd_count * res->bd_size;
+
+    dma_free_coherent(res->dev, bd_base_size, res->bd_base,
+              res->bd_dma_base);
 }
 
-static int enetc_alloc_txbdr(struct enetc_bdr *txr)
+static int enetc_alloc_tx_resource(struct enetc_bdr_resource *res,
+                   struct device *dev, size_t bd_count)
 {
-	int err;
-
-	txr->tx_swbd = vzalloc(txr->bd_count * sizeof(struct enetc_tx_swbd));
-	if (!txr->tx_swbd)
-		return -ENOMEM;
+    int err;
 
-	err = enetc_dma_alloc_bdr(txr, sizeof(union enetc_tx_bd));
-	if (err) {
-		vfree(txr->tx_swbd);
-		return err;
-	}
+    res->dev = dev;
+    res->bd_count = bd_count;
+    res->bd_size = sizeof(union enetc_tx_bd);
 
-	txr->next_to_clean = 0;
-	txr->next_to_use = 0;
+    res->tx_swbd = vcalloc(bd_count, sizeof(*res->tx_swbd));
+    if (!res->tx_swbd)
+        return -ENOMEM;
 
-	return 0;
-}
+    err = enetc_dma_alloc_bdr(res);
+    if (err)
+        goto err_alloc_bdr;
 
-static void enetc_free_txbdr(struct enetc_bdr *txr)
-{
-	int size, i;
+    res->tso_headers = dma_alloc_coherent(dev, bd_count * TSO_HEADER_SIZE,
+                          &res->tso_headers_dma,
+                          GFP_KERNEL);
+    if (!res->tso_headers) {
+        err = -ENOMEM;
+        goto err_alloc_tso;
+    }
 
-	for (i = 0; i < txr->bd_count; i++)
-		enetc_free_tx_skb(txr, &txr->tx_swbd[i]);
+    return 0;
 
-	size = txr->bd_count * sizeof(union enetc_tx_bd);
+err_alloc_tso:
+    enetc_dma_free_bdr(res);
+err_alloc_bdr:
+    vfree(res->tx_swbd);
+    res->tx_swbd = NULL;
 
-	dma_free_coherent(txr->dev, size, txr->bd_base, txr->bd_dma_base);
-	txr->bd_base = NULL;
+    return err;
+}
 
-	vfree(txr->tx_swbd);
-	txr->tx_swbd = NULL;
+static void enetc_free_tx_resource(const struct enetc_bdr_resource *res)
+{
+    dma_free_coherent(res->dev, res->bd_count * TSO_HEADER_SIZE,
+              res->tso_headers, res->tso_headers_dma);
+    enetc_dma_free_bdr(res);
+    vfree(res->tx_swbd);
 }
 
-static int enetc_alloc_tx_resources(struct enetc_ndev_priv *priv)
+static struct enetc_bdr_resource *
+enetc_alloc_tx_resources(struct enetc_ndev_priv *priv)
 {
-	int i, err;
+    struct enetc_bdr_resource *tx_res;
+    int i, err;
 
-	for (i = 0; i < priv->num_tx_rings; i++) {
-		err = enetc_alloc_txbdr(priv->tx_ring[i]);
+    tx_res = kcalloc(priv->num_tx_rings, sizeof(*tx_res), GFP_KERNEL);
+    if (!tx_res)
+        return ERR_PTR(-ENOMEM);
 
-		if (err)
-			goto fail;
-	}
+    for (i = 0; i < priv->num_tx_rings; i++) {
+        struct enetc_bdr *tx_ring = priv->tx_ring[i];
 
-	return 0;
+        err = enetc_alloc_tx_resource(&tx_res[i], tx_ring->dev,
+                          tx_ring->bd_count);
+        if (err)
+            goto fail;
+    }
+
+    return tx_res;
 
 fail:
-	while (i-- > 0)
-		enetc_free_txbdr(priv->tx_ring[i]);
+    while (i-- > 0)
+        enetc_free_tx_resource(&tx_res[i]);
 
-	return err;
+    kfree(tx_res);
+
+    return ERR_PTR(err);
 }
 
-static void enetc_free_tx_resources(struct enetc_ndev_priv *priv)
+static void enetc_free_tx_resources(const struct enetc_bdr_resource *tx_res,
+                    size_t num_resources)
 {
-	int i;
+    size_t i;
 
-	for (i = 0; i < priv->num_tx_rings; i++)
-		enetc_free_txbdr(priv->tx_ring[i]);
+    for (i = 0; i < num_resources; i++)
+        enetc_free_tx_resource(&tx_res[i]);
+
+    kfree(tx_res);
 }
 
-static int enetc_alloc_rxbdr(struct enetc_bdr *rxr, bool extended)
+static int enetc_alloc_rx_resource(struct enetc_bdr_resource *res,
+                   struct device *dev, size_t bd_count,
+                   bool extended)
 {
-	size_t size = sizeof(union enetc_rx_bd);
-	int err;
+    int err;
 
-	rxr->rx_swbd = vzalloc(rxr->bd_count * sizeof(struct enetc_rx_swbd));
-	if (!rxr->rx_swbd)
-		return -ENOMEM;
-
-	if (extended)
-		size *= 2;
+    res->dev = dev;
+    res->bd_count = bd_count;
+    res->bd_size = sizeof(union enetc_rx_bd);
+    if (extended)
+        res->bd_size *= 2;
 
-	err = enetc_dma_alloc_bdr(rxr, size);
-	if (err) {
-		vfree(rxr->rx_swbd);
-		return err;
-	}
+    res->rx_swbd = vcalloc(bd_count, sizeof(struct enetc_rx_swbd));
+    if (!res->rx_swbd)
+        return -ENOMEM;
 
-	rxr->next_to_clean = 0;
-	rxr->next_to_use = 0;
-	rxr->next_to_alloc = 0;
-	rxr->ext_en = extended;
+    err = enetc_dma_alloc_bdr(res);
+    if (err) {
+        vfree(res->rx_swbd);
+        return err;
+    }
 
-	return 0;
+    return 0;
 }
 
-static void enetc_free_rxbdr(struct enetc_bdr *rxr)
+static void enetc_free_rx_resource(const struct enetc_bdr_resource *res)
 {
-	int size;
-
-	size = rxr->bd_count * sizeof(union enetc_rx_bd);
-
-	dma_free_coherent(rxr->dev, size, rxr->bd_base, rxr->bd_dma_base);
-	rxr->bd_base = NULL;
-
-	vfree(rxr->rx_swbd);
-	rxr->rx_swbd = NULL;
+    enetc_dma_free_bdr(res);
+    vfree(res->rx_swbd);
 }
 
-static int enetc_alloc_rx_resources(struct enetc_ndev_priv *priv)
+static struct enetc_bdr_resource *
+enetc_alloc_rx_resources(struct enetc_ndev_priv *priv, bool extended)
 {
-	bool extended = !!(priv->active_offloads & ENETC_F_RX_TSTAMP);
-	int i, err;
+    struct enetc_bdr_resource *rx_res;
+    int i, err;
 
-	for (i = 0; i < priv->num_rx_rings; i++) {
-		err = enetc_alloc_rxbdr(priv->rx_ring[i], extended);
+    rx_res = kcalloc(priv->num_rx_rings, sizeof(*rx_res), GFP_KERNEL);
+    if (!rx_res)
+        return ERR_PTR(-ENOMEM);
 
-		if (err)
-			goto fail;
-	}
+    for (i = 0; i < priv->num_rx_rings; i++) {
+        struct enetc_bdr *rx_ring = priv->rx_ring[i];
 
-	return 0;
+        err = enetc_alloc_rx_resource(&rx_res[i], rx_ring->dev,
+                          rx_ring->bd_count, extended);
+        if (err)
+            goto fail;
+    }
+
+    return rx_res;
 
 fail:
-	while (i-- > 0)
-		enetc_free_rxbdr(priv->rx_ring[i]);
+    while (i-- > 0)
+        enetc_free_rx_resource(&rx_res[i]);
 
-	return err;
+    kfree(rx_res);
+
+    return ERR_PTR(err);
 }
 
-static void enetc_free_rx_resources(struct enetc_ndev_priv *priv)
+static void enetc_free_rx_resources(const struct enetc_bdr_resource *rx_res,
+                    size_t num_resources)
 {
-	int i;
+    size_t i;
 
-	for (i = 0; i < priv->num_rx_rings; i++)
-		enetc_free_rxbdr(priv->rx_ring[i]);
+    for (i = 0; i < num_resources; i++)
+        enetc_free_rx_resource(&rx_res[i]);
+
+    kfree(rx_res);
+}
+
+static void enetc_assign_tx_resource(struct enetc_bdr *tx_ring,
+                     const struct enetc_bdr_resource *res)
+{
+    tx_ring->bd_base = res ? res->bd_base : NULL;
+    tx_ring->bd_dma_base = res ? res->bd_dma_base : 0;
+    tx_ring->tx_swbd = res ? res->tx_swbd : NULL;
+    tx_ring->tso_headers = res ? res->tso_headers : NULL;
+    tx_ring->tso_headers_dma = res ? res->tso_headers_dma : 0;
+}
+
+static void enetc_assign_rx_resource(struct enetc_bdr *rx_ring,
+                     const struct enetc_bdr_resource *res)
+{
+    rx_ring->bd_base = res ? res->bd_base : NULL;
+    rx_ring->bd_dma_base = res ? res->bd_dma_base : 0;
+    rx_ring->rx_swbd = res ? res->rx_swbd : NULL;
+}
+
+static void enetc_assign_tx_resources(struct enetc_ndev_priv *priv,
+                      const struct enetc_bdr_resource *res)
+{
+    int i;
+
+    if (priv->tx_res)
+        enetc_free_tx_resources(priv->tx_res, priv->num_tx_rings);
+
+    for (i = 0; i < priv->num_tx_rings; i++) {
+        enetc_assign_tx_resource(priv->tx_ring[i],
+                     res ? &res[i] : NULL);
+    }
+
+    priv->tx_res = res;
+}
+
+static void enetc_assign_rx_resources(struct enetc_ndev_priv *priv,
+                      const struct enetc_bdr_resource *res)
+{
+    int i;
+
+    if (priv->rx_res)
+        enetc_free_rx_resources(priv->rx_res, priv->num_rx_rings);
+
+    for (i = 0; i < priv->num_rx_rings; i++) {
+        enetc_assign_rx_resource(priv->rx_ring[i],
+                     res ? &res[i] : NULL);
+    }
+
+    priv->rx_res = res;
+}
+
+int enetc_alloc_rings(struct enetc_ndev_priv *priv)
+{
+	struct enetc_bdr *bdrs;
+	int i, j;
+	bdrs = kmalloc_array(sizeof(struct enetc_bdr), (priv->num_tx_rings
+				+priv->num_rx_rings), GFP_KERNEL);
+	if (!bdrs) {
+		return -ENOMEM;
+	}
+	for (i = 0; i < priv->num_tx_rings; i++) {
+		bdrs[i].ndev = priv->ndev;
+		bdrs[i].dev = priv->dev;
+		bdrs[i].bd_count = priv->tx_bd_count;
+		priv->tx_ring[i] = &bdrs[i];
+	}
+
+	for (j = 0; j < priv->num_rx_rings; i++, j++) {
+		bdrs[i].ndev = priv->ndev;
+		bdrs[i].dev = priv->dev;
+		bdrs[i].bd_count = priv->tx_bd_count;
+		priv->rx_ring[j] = &bdrs[i];
+	}
+	return 0;
+}
+
+void enetc_free_rings(struct enetc_ndev_priv *priv)
+{
+	kfree(priv->tx_ring[0]);
+}
+
+static void enetc_free_txbdr(struct enetc_bdr *txr)
+{
+	int size, i;
+
+	for (i = 0; i < txr->bd_count; i++)
+		enetc_free_tx_skb(txr, &txr->tx_swbd[i]);
+
+	size = txr->bd_count * sizeof(union enetc_tx_bd);
+
+	dma_free_coherent(txr->dev, size, txr->bd_base, txr->bd_dma_base);
+	txr->bd_base = NULL;
+
+	vfree(txr->tx_swbd);
+	txr->tx_swbd = NULL;
+}
+
+static void enetc_free_rxbdr(struct enetc_bdr *rxr)
+{
+	int size;
+
+	size = rxr->bd_count * sizeof(union enetc_rx_bd);
+
+	dma_free_coherent(rxr->dev, size, rxr->bd_base, rxr->bd_dma_base);
+	rxr->bd_base = NULL;
+
+	vfree(rxr->rx_swbd);
+	rxr->rx_swbd = NULL;
 }
 
 static void enetc_free_tx_ring(struct enetc_bdr *tx_ring)
@@ -748,60 +1978,6 @@ static void enetc_free_rxtx_rings(struct enetc_ndev_priv *priv)
 		enetc_free_tx_ring(priv->tx_ring[i]);
 }
 
-static int enetc_alloc_cbdr(struct device *dev, struct enetc_cbdr *cbdr)
-{
-	int size = cbdr->bd_count * sizeof(struct enetc_cbd);
-
-	cbdr->bd_base = dma_alloc_coherent(dev, size, &cbdr->bd_dma_base,
-					   GFP_KERNEL);
-	if (!cbdr->bd_base)
-		return -ENOMEM;
-
-	/* h/w requires 128B alignment */
-	if (!IS_ALIGNED(cbdr->bd_dma_base, 128)) {
-		dma_free_coherent(dev, size, cbdr->bd_base, cbdr->bd_dma_base);
-		return -EINVAL;
-	}
-
-	cbdr->next_to_clean = 0;
-	cbdr->next_to_use = 0;
-
-	return 0;
-}
-
-static void enetc_free_cbdr(struct device *dev, struct enetc_cbdr *cbdr)
-{
-	int size = cbdr->bd_count * sizeof(struct enetc_cbd);
-
-	dma_free_coherent(dev, size, cbdr->bd_base, cbdr->bd_dma_base);
-	cbdr->bd_base = NULL;
-}
-
-static void enetc_setup_cbdr(struct enetc_hw *hw, struct enetc_cbdr *cbdr)
-{
-	/* set CBDR cache attributes */
-	enetc_wr(hw, ENETC_SICAR2,
-		 ENETC_SICAR_RD_COHERENT | ENETC_SICAR_WR_COHERENT);
-
-	enetc_wr(hw, ENETC_SICBDRBAR0, lower_32_bits(cbdr->bd_dma_base));
-	enetc_wr(hw, ENETC_SICBDRBAR1, upper_32_bits(cbdr->bd_dma_base));
-	enetc_wr(hw, ENETC_SICBDRLENR, ENETC_RTBLENR_LEN(cbdr->bd_count));
-
-	enetc_wr(hw, ENETC_SICBDRPIR, 0);
-	enetc_wr(hw, ENETC_SICBDRCIR, 0);
-
-	/* enable ring */
-	enetc_wr(hw, ENETC_SICBDRMR, BIT(31));
-
-	cbdr->pir = hw->reg + ENETC_SICBDRPIR;
-	cbdr->cir = hw->reg + ENETC_SICBDRCIR;
-}
-
-static void enetc_clear_cbdr(struct enetc_hw *hw)
-{
-	enetc_wr(hw, ENETC_SICBDRMR, 0);
-}
-
 static int enetc_setup_default_rss_table(struct enetc_si *si, int num_groups)
 {
 	int *rss_table;
@@ -815,93 +1991,134 @@ static int enetc_setup_default_rss_table(struct enetc_si *si, int num_groups)
 	for (i = 0; i < si->num_rss; i++)
 		rss_table[i] = i % num_groups;
 
-	enetc_set_rss_table(si, rss_table, si->num_rss);
+	if (si->set_rss_table)
+		si->set_rss_table(si, rss_table, si->num_rss);
 
+	printk(KERN_ERR "enetc_setup_default_rss_table: %s %s %d\n", __FILE__, __FUNCTION__, __LINE__);
 	kfree(rss_table);
 
 	return 0;
 }
 
-static int enetc_configure_si(struct enetc_ndev_priv *priv)
+static int enetc4_get_rss_table(struct enetc_si *si, u32 *table, int count)
 {
-	struct enetc_si *si = priv->si;
-	struct enetc_hw *hw = &si->hw;
-	int err;
+    return ntmp_rsst_query_or_update_entry(&si->ntmp.cbdrs, table, count, true);
+}
 
-	enetc_setup_cbdr(hw, &si->cbd_ring);
-	/* set SI cache attributes */
-	enetc_wr(hw, ENETC_SICAR0,
-		 ENETC_SICAR_RD_COHERENT | ENETC_SICAR_WR_COHERENT);
-	enetc_wr(hw, ENETC_SICAR1, ENETC_SICAR_MSI);
-	/* enable SI */
-	enetc_wr(hw, ENETC_SIMR, ENETC_SIMR_EN);
-
-	if (si->num_rss) {
-		err = enetc_setup_default_rss_table(si, priv->num_rx_rings);
-		if (err)
-			return err;
-	}
+static int enetc4_set_rss_table(struct enetc_si *si, const u32 *table, int count)
+{
+    return ntmp_rsst_query_or_update_entry(&si->ntmp.cbdrs,
+                           (u32 *)table, count, false);
+}
 
-	return 0;
+static void enetc4_set_lso_flags_mask(struct enetc_hw *hw)
+{
+    enetc_wr(hw, ENETC4_SILSOSFMR0,
+         SILSOSFMR0_VAL_SET(TCP_NL_SEG_FLAGS_DMASK, TCP_NL_SEG_FLAGS_DMASK));
+    enetc_wr(hw, ENETC4_SILSOSFMR1, 0);
 }
 
-void enetc_init_si_rings_params(struct enetc_ndev_priv *priv)
+static int enetc_set_rss(struct net_device *ndev, bool en)
 {
-	struct enetc_si *si = priv->si;
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct enetc_hw *hw = &priv->si->hw;
+    u32 reg;
 
-	priv->tx_bd_count = ENETC_TX_RING_DEFAULT_SIZE;
-	priv->rx_bd_count = ENETC_RX_RING_DEFAULT_SIZE;
+    enetc_wr(hw, ENETC_SIRBGCR, priv->num_rx_rings);
 
-	/* Enable all available TX rings in order to configure as many
-	 * priorities as possible, when needed.
-	 * TODO: Make # of TX rings run-time configurable
-	 */
-	priv->num_rx_rings = 1;
-	priv->num_tx_rings = 1;
-	priv->bdr_int_num = 1;
+    reg = enetc_rd(hw, ENETC_SIMR);
+    reg &= ~ENETC_SIMR_RSSE;
+    reg |= (en) ? ENETC_SIMR_RSSE : 0;
+    enetc_wr(hw, ENETC_SIMR, reg);
 
-	/* SI specific */
-	si->cbd_ring.bd_count = ENETC_CBDR_DEFAULT_SIZE;
+    return 0;
 }
 
-int enetc_alloc_si_resources(struct enetc_ndev_priv *priv)
+int ec_enetc_configure_si(struct enetc_ndev_priv *priv)
 {
-	struct enetc_si *si = priv->si;
-	int err;
-
-	err = enetc_alloc_cbdr(priv->dev, &si->cbd_ring);
-	if (err)
-		return err;
+    struct enetc_si *si = priv->si;
+    struct enetc_hw *hw = &si->hw;
+    int err;
+
+    if (is_enetc_rev1(si)) {
+		printk(KERN_ERR "is_enetc_rev1: %s %s %d\n", __FILE__, __FUNCTION__, __LINE__);
+        si->get_rss_table = ec_enetc_get_rss_table;
+        si->set_rss_table = ec_enetc_set_rss_table;
+        si->clk_freq = ENETC_CLK;
+    } else {
+		printk(KERN_ERR "!!!is_enetc_rev1: %s %s %d\n", __FILE__, __FUNCTION__, __LINE__);
+        si->get_rss_table = enetc4_get_rss_table;
+        si->set_rss_table = enetc4_set_rss_table;
+        si->clk_freq = ENETC4_CLK;
+
+        /* Set TCP flags mask for LSO. */
+        enetc4_set_lso_flags_mask(hw);
+    }
+
+    mutex_init(&si->msg_lock);
+
+    /* set SI cache attributes */
+    enetc_wr(hw, ENETC_SICAR0,
+         ENETC_SICAR_RD_COHERENT | ENETC_SICAR_WR_COHERENT);
+    enetc_wr(hw, ENETC_SICAR1, ENETC_SICAR_MSI);
+    /* enable SI */
+    enetc_wr(hw, ENETC_SIMR, ENETC_SIMR_EN);
+
+    if (si->num_rss) {
+		printk(KERN_ERR "si->num_rss: %s %s %d\n", __FILE__, __FUNCTION__, __LINE__);
+        err = enetc_setup_default_rss_table(si, priv->num_rx_rings);
+        if (err)
+            return err;
+    }
+
+    if (priv->ndev->features & NETIF_F_RXHASH)
+        enetc_set_rss(priv->ndev, true);
+
+    return 0;
+}
 
-	priv->cls_rules = kcalloc(si->num_fs_entries, sizeof(*priv->cls_rules),
-				  GFP_KERNEL);
-	if (!priv->cls_rules) {
-		err = -ENOMEM;
-		goto err_alloc_cls;
-	}
+void ec_enetc_init_si_rings_params(struct enetc_ndev_priv *priv)
+{
+    struct enetc_si *si = priv->si;
+    int cpus = num_online_cpus();
+
+    priv->tx_bd_count = ENETC_TX_RING_DEFAULT_SIZE;
+    priv->rx_bd_count = ENETC_RX_RING_DEFAULT_SIZE;
+
+    /* Enable all available TX rings in order to configure as many
+     * priorities as possible, when needed.
+     * TODO: Make # of TX rings run-time configurable
+     */
+    priv->num_rx_rings = min_t(int, cpus, si->num_rx_rings);
+    priv->num_tx_rings = si->num_tx_rings;
+    if (is_enetc_rev1(si)) {
+        priv->bdr_int_num = cpus;
+        priv->tx_ictt = ENETC_TXIC_TIMETHR;
+    } else {
+        priv->bdr_int_num = priv->num_rx_rings;
+        priv->tx_ictt = ENETC4_TXIC_TIMETHR;
+    }
+
+    priv->ic_mode = ENETC_IC_RX_ADAPTIVE | ENETC_IC_TX_MANUAL;
+}
 
-	err = enetc_configure_si(priv);
-	if (err)
-		goto err_config_si;
+int ec_enetc_alloc_si_resources(struct enetc_ndev_priv *priv)
+{
+    struct enetc_si *si = priv->si;
 
-	return 0;
+    if (!si->num_fs_entries)
+        return 0;
 
-err_config_si:
-	kfree(priv->cls_rules);
-err_alloc_cls:
-	enetc_clear_cbdr(&si->hw);
-	enetc_free_cbdr(priv->dev, &si->cbd_ring);
+    priv->cls_rules = kcalloc(si->num_fs_entries, sizeof(*priv->cls_rules),
+                  GFP_KERNEL);
+    if (!priv->cls_rules)
+        return -ENOMEM;
 
-	return err;
+    return 0;
 }
 
-void enetc_free_si_resources(struct enetc_ndev_priv *priv)
+void ec_enetc_free_si_resources(struct enetc_ndev_priv *priv)
 {
-	struct enetc_si *si = priv->si;
-
-	enetc_clear_cbdr(&si->hw);
-	enetc_free_cbdr(priv->dev, &si->cbd_ring);
-
 	kfree(priv->cls_rules);
 }
 
@@ -939,54 +2156,172 @@ static void enetc_setup_txbdr(struct enetc_hw *hw, struct enetc_bdr *tx_ring)
 	tx_ring->idr = hw->reg + ENETC_SITXIDR;
 }
 
-static void enetc_setup_rxbdr(struct enetc_hw *hw, struct enetc_bdr *rx_ring)
+static void enetc_setup_rxbdr(struct enetc_hw *hw, struct enetc_bdr *rx_ring,
+                  bool extended)
 {
-	int idx = rx_ring->index;
-	u32 rbmr;
+    struct enetc_ndev_priv *priv = netdev_priv(rx_ring->ndev);
+    int idx = rx_ring->index;
+    u32 rbmr = 0;
+
+    enetc_rxbdr_wr(hw, idx, ENETC_RBBAR0,
+               lower_32_bits(rx_ring->bd_dma_base));
+
+    enetc_rxbdr_wr(hw, idx, ENETC_RBBAR1,
+               upper_32_bits(rx_ring->bd_dma_base));
+
+    WARN_ON(!IS_ALIGNED(rx_ring->bd_count, 64)); /* multiple of 64 */
+    enetc_rxbdr_wr(hw, idx, ENETC_RBLENR,
+               ENETC_RTBLENR_LEN(rx_ring->bd_count));
+
+    if (rx_ring->xdp.prog)
+        enetc_rxbdr_wr(hw, idx, ENETC_RBBSR, ENETC_RXB_DMA_SIZE_XDP);
+    else
+        enetc_rxbdr_wr(hw, idx, ENETC_RBBSR, ENETC_RXB_DMA_SIZE);
+
+    /* Also prepare the consumer index in case page allocation never
+     * succeeds. In that case, hardware will never advance producer index
+     * to match consumer index, and will drop all frames.
+     */
+    enetc_rxbdr_wr(hw, idx, ENETC_RBPIR, 0);
+    enetc_rxbdr_wr(hw, idx, ENETC_RBCIR, 1);
+
+    /* enable Rx ints by setting pkt thr to 1 */
+    enetc_rxbdr_wr(hw, idx, ENETC_RBICR0, ENETC_RBICR0_ICEN | 0x1);
+
+    rx_ring->ext_en = extended;
+    if (rx_ring->ext_en)
+        rbmr |= ENETC_RBMR_BDS;
+
+    if (rx_ring->ndev->features & NETIF_F_HW_VLAN_CTAG_RX)
+        rbmr |= ENETC_RBMR_VTE;
+
+    rx_ring->rcir = hw->reg + ENETC_BDR(RX, idx, ENETC_RBCIR);
+    rx_ring->idr = hw->reg + ENETC_SIRXIDR;
+
+    rx_ring->next_to_clean = 0;
+    rx_ring->next_to_use = 0;
+    rx_ring->next_to_alloc = 0;
+
+    enetc_lock_mdio();
+    enetc_refill_rx_ring(rx_ring, enetc_bd_unused(rx_ring));
+    enetc_unlock_mdio();
+
+    enetc_rxbdr_wr(hw, idx, ENETC_RBMR, rbmr);
+    if (rx_ring->ext_en && priv->active_offloads & ENETC_F_RSC)
+        enetc_rxbdr_wr(hw, idx, ENETC_RBRSCR, ENETC_RBRSCR_EN |
+                   ENETC_RBRSCR_SIZE(ENETC_RS_MAX_BYTES));
+    else
+        enetc_rxbdr_wr(hw, idx, ENETC_RBRSCR, 0x0);
+}
 
-	enetc_rxbdr_wr(hw, idx, ENETC_RBBAR0,
-		       lower_32_bits(rx_ring->bd_dma_base));
+static void enetc_setup_bdrs(struct enetc_ndev_priv *priv, bool extended)
+{
+    struct enetc_hw *hw = &priv->si->hw;
+    int i;
 
-	enetc_rxbdr_wr(hw, idx, ENETC_RBBAR1,
-		       upper_32_bits(rx_ring->bd_dma_base));
+    for (i = 0; i < priv->num_tx_rings; i++)
+        enetc_setup_txbdr(hw, priv->tx_ring[i]);
 
-	WARN_ON(!IS_ALIGNED(rx_ring->bd_count, 64)); /* multiple of 64 */
-	enetc_rxbdr_wr(hw, idx, ENETC_RBLENR,
-		       ENETC_RTBLENR_LEN(rx_ring->bd_count));
+    for (i = 0; i < priv->num_rx_rings; i++)
+        enetc_setup_rxbdr(hw, priv->rx_ring[i], extended);
+}
 
-	enetc_rxbdr_wr(hw, idx, ENETC_RBBSR, ENETC_RXB_DMA_SIZE);
+static void enetc_enable_txbdr(struct enetc_hw *hw, struct enetc_bdr *tx_ring)
+{
+    int idx = tx_ring->index;
+    u32 tbmr;
+
+    tbmr = enetc_txbdr_rd(hw, idx, ENETC_TBMR);
+    tbmr |= ENETC_TBMR_EN;
+    enetc_txbdr_wr(hw, idx, ENETC_TBMR, tbmr);
+}
 
-	enetc_rxbdr_wr(hw, idx, ENETC_RBPIR, 0);
+static void enetc_enable_rxbdr(struct enetc_hw *hw, struct enetc_bdr *rx_ring)
+{
+    int idx = rx_ring->index;
+    u32 rbmr;
 
-	/* enable Rx ints by setting pkt thr to 1 */
-	enetc_rxbdr_wr(hw, idx, ENETC_RBICR0, ENETC_RBICR0_ICEN | 0x1);
+    rbmr = enetc_rxbdr_rd(hw, idx, ENETC_RBMR);
+    rbmr |= ENETC_RBMR_EN;
+    enetc_rxbdr_wr(hw, idx, ENETC_RBMR, rbmr);
+}
 
-	rbmr = ENETC_RBMR_EN;
+static void enetc_enable_rx_bdrs(struct enetc_ndev_priv *priv)
+{
+    struct enetc_hw *hw = &priv->si->hw;
+    int i;
 
-	if (rx_ring->ext_en)
-		rbmr |= ENETC_RBMR_BDS;
+    for (i = 0; i < priv->num_rx_rings; i++)
+        enetc_enable_rxbdr(hw, priv->rx_ring[i]);
+}
 
-	if (rx_ring->ndev->features & NETIF_F_HW_VLAN_CTAG_RX)
-		rbmr |= ENETC_RBMR_VTE;
+static void enetc_enable_tx_bdrs(struct enetc_ndev_priv *priv)
+{
+    struct enetc_hw *hw = &priv->si->hw;
+    int i;
 
-	rx_ring->rcir = hw->reg + ENETC_BDR(RX, idx, ENETC_RBCIR);
-	rx_ring->idr = hw->reg + ENETC_SIRXIDR;
+    for (i = 0; i < priv->num_tx_rings; i++)
+        enetc_enable_txbdr(hw, priv->tx_ring[i]);
+}
 
-	enetc_refill_rx_ring(rx_ring, enetc_bd_unused(rx_ring));
-	enetc_wr(hw, ENETC_SIRXIDR, rx_ring->next_to_use);
+static void enetc_disable_rxbdr(struct enetc_hw *hw, struct enetc_bdr *rx_ring)
+{
+    int idx = rx_ring->index;
 
-	/* enable ring */
-	enetc_rxbdr_wr(hw, idx, ENETC_RBMR, rbmr);
+    /* disable EN bit on ring */
+    enetc_rxbdr_wr(hw, idx, ENETC_RBMR, 0);
 }
 
-static void enetc_setup_bdrs(struct enetc_ndev_priv *priv)
+static void enetc_disable_txbdr(struct enetc_hw *hw, struct enetc_bdr *rx_ring)
 {
-	int i;
-	for (i = 0; i < priv->num_tx_rings; i++)
-		enetc_setup_txbdr(&priv->si->hw, priv->tx_ring[i]);
+    int idx = rx_ring->index;
 
-	for (i = 0; i < priv->num_rx_rings; i++)
-		enetc_setup_rxbdr(&priv->si->hw, priv->rx_ring[i]);
+    /* disable EN bit on ring */
+    enetc_txbdr_wr(hw, idx, ENETC_TBMR, 0);
+}
+
+static void enetc_disable_rx_bdrs(struct enetc_ndev_priv *priv)
+{
+    struct enetc_hw *hw = &priv->si->hw;
+    int i;
+
+    for (i = 0; i < priv->num_rx_rings; i++)
+        enetc_disable_rxbdr(hw, priv->rx_ring[i]);
+}
+
+static void enetc_disable_tx_bdrs(struct enetc_ndev_priv *priv)
+{
+    struct enetc_hw *hw = &priv->si->hw;
+    int i;
+
+    for (i = 0; i < priv->num_tx_rings; i++)
+        enetc_disable_txbdr(hw, priv->tx_ring[i]);
+}
+
+static void enetc_wait_txbdr(struct enetc_hw *hw, struct enetc_bdr *tx_ring)
+{
+    int delay = 8, timeout = 100;
+    int idx = tx_ring->index;
+
+    /* wait for busy to clear */
+    while (delay < timeout &&
+           enetc_txbdr_rd(hw, idx, ENETC_TBSR) & ENETC_TBSR_BUSY) {
+        msleep(delay);
+        delay *= 2;
+    }
+
+    if (delay >= timeout)
+        netdev_warn(tx_ring->ndev, "timeout for tx ring #%d clear\n",
+                idx);
+}
+
+static void enetc_wait_bdrs(struct enetc_ndev_priv *priv)
+{
+    struct enetc_hw *hw = &priv->si->hw;
+    int i;
+
+    for (i = 0; i < priv->num_tx_rings; i++)
+        enetc_wait_txbdr(hw, priv->tx_ring[i]);
 }
 
 static void enetc_clear_rxbdr(struct enetc_hw *hw, struct enetc_bdr *rx_ring)
@@ -1041,10 +2376,20 @@ static void enetc_clear_interrupts(struct enetc_ndev_priv *priv)
 		enetc_rxbdr_wr(&priv->si->hw, i, ENETC_RBIER, 0);
 }
 
+static int enetc_ec_disable_eee(struct net_device *ndev)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct ethtool_keee *eee = &priv->eee;
+    eee->eee_enabled = 0;
+    eee->tx_lpi_enabled = 0;
+
+    return phylink_ethtool_set_eee(priv->phylink, eee);
+}
+
 static int enetc_phylink_connect(struct net_device *ndev)
 {
 	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-	struct ethtool_eee edata;
+	struct ethtool_keee *edata = &priv->eee;
 	int err;
 	if (!priv->phylink)
 		return 0; /* phy-less mode */
@@ -1058,83 +2403,705 @@ static int enetc_phylink_connect(struct net_device *ndev)
 	}
 
 	/* disable EEE autoneg, until ENETC driver supports it */
-	memset(&edata, 0, sizeof(struct ethtool_eee));
-	phylink_ethtool_set_eee(priv->phylink, &edata);
+	phylink_ethtool_set_eee(priv->phylink, edata);
 	rtnl_unlock();
 	return 0;
 }
 
-void enetc_start(struct net_device *ndev)
+static void enetc_tx_onestep_tstamp(struct work_struct *work)
 {
-	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct enetc_ndev_priv *priv;
+    struct sk_buff *skb;
 
-	if (priv->phylink) {
-		rtnl_lock();
-		phylink_start(priv->phylink);
-		rtnl_unlock();
-	}
+    priv = container_of(work, struct enetc_ndev_priv, tx_onestep_tstamp);
+
+    netif_tx_lock_bh(priv->ndev);
+
+    clear_bit_unlock(ENETC_TX_ONESTEP_TSTAMP_IN_PROGRESS, &priv->flags);
+    skb = skb_dequeue(&priv->tx_skbs);
+    if (skb)
+        ec_enetc_start_xmit(skb, priv->ndev);
+
+    netif_tx_unlock_bh(priv->ndev);
 }
 
-int enetc_open(struct net_device *ndev)
+static void enetc_tx_onestep_tstamp_init(struct enetc_ndev_priv *priv)
 {
-	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-	int err;
+    INIT_WORK(&priv->tx_onestep_tstamp, enetc_tx_onestep_tstamp);
+    skb_queue_head_init(&priv->tx_skbs);
+}
 
-	enetc_clear_interrupts(priv);
+void ec_enetc_start(struct net_device *ndev)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    int i;
 
-	err = enetc_phylink_connect(ndev);
-	if (err)
-		goto err_phy_connect;
-	err = enetc_alloc_tx_resources(priv);
-	if (err)
-		goto err_alloc_tx;
+    enetc_enable_tx_bdrs(priv);
 
-	err = enetc_alloc_rx_resources(priv);
-	if (err)
-		goto err_alloc_rx;
+    enetc_enable_rx_bdrs(priv);
 
-	enetc_setup_bdrs(priv);
-	enetc_start(ndev);
-	return 0;
+    netif_tx_start_all_queues(ndev);
+
+    clear_bit(ENETC_TX_DOWN, &priv->flags);
+}
+
+static void enetc_set_eee(struct net_device *ndev)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct ethtool_keee *eee = &priv->eee;
+
+    if (is_enetc_rev1(priv->si) || !enetc_si_is_pf(priv->si))
+        return;
+
+    if (eee->eee_enabled && eee->tx_lpi_enabled && eee->tx_lpi_timer)
+        ec_enetc_eee_mode_set(ndev, true);
+    else
+        ec_enetc_eee_mode_set(ndev, false);
+}
+
+int ec_enetc_open(struct net_device *ndev)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct enetc_bdr_resource *tx_res, *rx_res;
+    struct enetc_si *si = priv->si;
+    bool extended;
+    int err;
+
+    extended = !!(priv->active_offloads & ENETC_F_RX_TSTAMP ||
+              priv->active_offloads & ENETC_F_RSC);
+
+    err = clk_prepare_enable(priv->ref_clk);
+    if (err)
+        return err;
+
+    err = enetc_phylink_connect(ndev);
+    if (err)
+        goto err_phy_connect;
+
+    tx_res = enetc_alloc_tx_resources(priv);
+    if (IS_ERR(tx_res)) {
+        err = PTR_ERR(tx_res);
+        goto err_alloc_tx;
+    }
+
+    rx_res = enetc_alloc_rx_resources(priv, extended);
+    if (IS_ERR(rx_res)) {
+        err = PTR_ERR(rx_res);
+        goto err_alloc_rx;
+    }
+
+    enetc_tx_onestep_tstamp_init(priv);
+    enetc_assign_tx_resources(priv, tx_res);
+    enetc_assign_rx_resources(priv, rx_res);
+    enetc_setup_bdrs(priv, extended);
+    ec_enetc_start(ndev);
+
+	enetc_ec_disable_eee(ndev);
+    enetc_set_eee(ndev);
+
+    return 0;
 
 err_alloc_rx:
-	enetc_free_rx_resources(priv);
+    enetc_free_tx_resources(tx_res, priv->num_tx_rings);
 err_alloc_tx:
-	enetc_free_tx_resources(priv);
-	if (priv->phylink)
-		phylink_disconnect_phy(priv->phylink);
+    if (priv->phylink) {
+        phylink_disconnect_phy(priv->phylink);
+    } else {
+        if (si->pdev->is_virtfn && si->vf_free_msg_msix) {
+            if (si->vf_register_link_status_notify)
+                si->vf_register_link_status_notify(si, false);
+
+            si->vf_free_msg_msix(si);
+
+            return 0;
+        }
+    }
 err_phy_connect:
+    //enetc_free_irqs(priv);
+err_setup_irqs:
+    clk_disable_unprepare(priv->ref_clk);
 
-	return err;
+    return err;
 }
 
-void enetc_stop(struct net_device *ndev)
+void ec_enetc_stop(struct net_device *ndev)
 {
-	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-	if (priv->phylink) {
-		rtnl_lock();
-		phylink_stop(priv->phylink);
-		rtnl_unlock();
-	}
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    int i;
+
+    set_bit(ENETC_TX_DOWN, &priv->flags);
+
+    netif_tx_stop_all_queues(ndev);
+
+    enetc_disable_rx_bdrs(priv);
+
+    enetc_wait_bdrs(priv);
+
+    enetc_disable_tx_bdrs(priv);
+
 }
 
-int enetc_close(struct net_device *ndev)
+int ec_enetc_close(struct net_device *ndev)
 {
-	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct enetc_si *si = priv->si;
 
-	enetc_stop(ndev);
-	enetc_clear_bdrs(priv);
+    ec_enetc_stop(ndev);
 
-	if (priv->phylink) {
-		rtnl_lock();
-		phylink_disconnect_phy(priv->phylink);
-		rtnl_unlock();
-	}
-	enetc_free_rxtx_rings(priv);
-	enetc_free_rx_resources(priv);
-	enetc_free_tx_resources(priv);
+    if (priv->phylink) {
+        phylink_stop(priv->phylink);
+        phylink_disconnect_phy(priv->phylink);
+    } else {
+        if (si->pdev->is_virtfn && si->vf_free_msg_msix) {
+            if (si->vf_register_link_status_notify)
+                si->vf_register_link_status_notify(si, false);
 
-	return 0;
+            si->vf_free_msg_msix(si);
+        }
+
+        netif_carrier_off(ndev);
+    }
+
+    enetc_free_rxtx_rings(priv);
+
+    /* Avoids dangling pointers and also frees old resources */
+    enetc_assign_rx_resources(priv, NULL);
+    enetc_assign_tx_resources(priv, NULL);
+
+    clk_disable_unprepare(priv->ref_clk);
+
+    return 0;
+}
+
+static int enetc_reconfigure(struct enetc_ndev_priv *priv, bool extended,
+                 int (*cb)(struct enetc_ndev_priv *priv, void *ctx),
+                 void *ctx)
+{
+    struct enetc_bdr_resource *tx_res, *rx_res;
+    int err;
+
+    ASSERT_RTNL();
+
+    /* If the interface is down, run the callback right away,
+     * without reconfiguration.
+     */
+    if (!netif_running(priv->ndev)) {
+        if (cb) {
+            err = cb(priv, ctx);
+            if (err)
+                return err;
+        }
+
+        return 0;
+    }
+
+    tx_res = enetc_alloc_tx_resources(priv);
+    if (IS_ERR(tx_res)) {
+        err = PTR_ERR(tx_res);
+        goto out;
+    }
+
+    rx_res = enetc_alloc_rx_resources(priv, extended);
+    if (IS_ERR(rx_res)) {
+        err = PTR_ERR(rx_res);
+        goto out_free_tx_res;
+    }
+
+    ec_enetc_stop(priv->ndev);
+    enetc_free_rxtx_rings(priv);
+
+    /* Interface is down, run optional callback now */
+    if (cb) {
+        err = cb(priv, ctx);
+        if (err)
+            goto out_restart;
+    }
+
+    enetc_assign_tx_resources(priv, tx_res);
+    enetc_assign_rx_resources(priv, rx_res);
+    enetc_setup_bdrs(priv, extended);
+    ec_enetc_start(priv->ndev);
+
+    return 0;
+
+out_restart:
+    enetc_setup_bdrs(priv, extended);
+    ec_enetc_start(priv->ndev);
+    enetc_free_rx_resources(rx_res, priv->num_rx_rings);
+out_free_tx_res:
+    enetc_free_tx_resources(tx_res, priv->num_tx_rings);
+out:
+    return err;
+}
+
+int ec_enetc_suspend(struct net_device *ndev, bool wol)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+
+    ec_enetc_stop(ndev);
+
+    enetc_free_rxtx_rings(priv);
+
+    /* Avoids dangling pointers and also frees old resources */
+    enetc_assign_rx_resources(priv, NULL);
+    enetc_assign_tx_resources(priv, NULL);
+
+    if (!wol) {
+        clk_disable_unprepare(priv->ref_clk);
+    }
+
+    return 0;
+}
+
+static int enetc_set_rsc(struct net_device *ndev, bool en)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    bool extended = en;
+    int err;
+
+    /* TODO: Supporting both XDP and RSC at the same time. */
+    if (priv->xdp_prog) {
+        netdev_err(ndev, "XDP and RSC cannot be enabled at the same time\n");
+        return -EOPNOTSUPP;
+    }
+
+    if (en)
+        priv->active_offloads |= ENETC_F_RSC;
+    else
+        priv->active_offloads &= ~ENETC_F_RSC;
+
+    if (priv->active_offloads & ENETC_F_RX_TSTAMP && !en)
+        extended = true;
+    err = enetc_reconfigure(priv, extended, NULL, NULL);
+    if (err) {
+        netdev_err(ndev, " %s RSC enetc reconfigure failed(%d)\n",
+               en ? "Enable" : "Disable", err);
+        return err;
+    }
+
+    return 0;
+}
+
+int ec_enetc_resume(struct net_device *ndev, bool wol)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct enetc_bdr_resource *tx_res, *rx_res;
+    bool extended;
+    int err;
+
+    extended = !!(priv->active_offloads & ENETC_F_RX_TSTAMP ||
+              priv->active_offloads & ENETC_F_RSC);
+
+    if (!wol) {
+        err = clk_prepare_enable(priv->ref_clk);
+            if (err)
+                return err;
+    } else {
+//        enetc_restore_irqs_affinity(priv);
+    }
+
+    tx_res = enetc_alloc_tx_resources(priv);
+    if (IS_ERR(tx_res)) {
+        err = PTR_ERR(tx_res);
+        goto out_free_irqs;
+    }
+
+    rx_res = enetc_alloc_rx_resources(priv, extended);
+    if (IS_ERR(rx_res)) {
+        err = PTR_ERR(rx_res);
+        goto out_free_tx_res;
+    }
+
+    enetc_tx_onestep_tstamp_init(priv);
+    enetc_assign_tx_resources(priv, tx_res);
+    enetc_assign_rx_resources(priv, rx_res);
+    enetc_setup_bdrs(priv, extended);
+    ec_enetc_start(priv->ndev);
+
+	enetc_ec_disable_eee(ndev);
+    enetc_set_eee(ndev);
+
+    return 0;
+
+out_free_tx_res:
+    enetc_free_tx_resources(tx_res, priv->num_tx_rings);
+out_free_irqs:
+    if (!wol)
+        //enetc_free_irqs(priv);
+out_setup_irqs:
+    if (!wol)
+        clk_disable_unprepare(priv->ref_clk);
+
+    return err;
+}
+
+static void enetc_debug_tx_ring_prios(struct enetc_ndev_priv *priv)
+{
+    int i;
+
+    for (i = 0; i < priv->num_tx_rings; i++)
+        netdev_dbg(priv->ndev, "TX ring %d prio %d\n", i,
+               priv->tx_ring[i]->prio);
+}
+
+void ec_enetc_reset_tc_mqprio(struct net_device *ndev)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct enetc_hw *hw = &priv->si->hw;
+    struct enetc_bdr *tx_ring;
+    int num_stack_tx_queues;
+    int i;
+
+    num_stack_tx_queues = enetc_num_stack_tx_queues(priv);
+
+    netdev_reset_tc(ndev);
+    netif_set_real_num_tx_queues(ndev, num_stack_tx_queues);
+
+    if (!priv->shared_tx_rings)
+        priv->min_num_stack_tx_queues = num_possible_cpus();
+
+    /* Reset all ring priorities to 0 */
+    for (i = 0; i < priv->num_tx_rings; i++) {
+        tx_ring = priv->tx_ring[i];
+        tx_ring->prio = 0;
+        enetc_set_bdr_prio(hw, tx_ring->index, tx_ring->prio);
+    }
+
+    enetc_debug_tx_ring_prios(priv);
+
+    ec_enetc_change_preemptible_tcs(priv, 0);
+}
+
+int ec_enetc_setup_tc_mqprio(struct net_device *ndev, void *type_data)
+{
+    struct tc_mqprio_qopt_offload *mqprio = type_data;
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct tc_mqprio_qopt *qopt = &mqprio->qopt;
+    struct enetc_hw *hw = &priv->si->hw;
+    int num_stack_tx_queues = 0;
+    struct enetc_bdr *tx_ring;
+    u8 num_tc = qopt->num_tc;
+    int offset, count;
+    int err, tc, q;
+
+    if (!num_tc) {
+        ec_enetc_reset_tc_mqprio(ndev);
+        return 0;
+    }
+
+    err = netdev_set_num_tc(ndev, num_tc);
+    if (err)
+        return err;
+
+    for (tc = 0; tc < num_tc; tc++) {
+        offset = qopt->offset[tc];
+        count = qopt->count[tc];
+        num_stack_tx_queues += count;
+
+        err = netdev_set_tc_queue(ndev, tc, count, offset);
+        if (err)
+            goto err_reset_tc;
+
+        for (q = offset; q < offset + count; q++) {
+            tx_ring = priv->tx_ring[q];
+            /* The prio_tc_map is skb_tx_hash()'s way of selecting
+             * between TX queues based on skb->priority. As such,
+             * there's nothing to offload based on it.
+             * Make the mqprio "traffic class" be the priority of
+             * this ring group, and leave the Tx IPV to traffic
+             * class mapping as its default mapping value of 1:1.
+             */
+            tx_ring->prio = tc;
+            enetc_set_bdr_prio(hw, tx_ring->index, tx_ring->prio);
+        }
+    }
+
+    err = netif_set_real_num_tx_queues(ndev, num_stack_tx_queues);
+    if (err)
+        goto err_reset_tc;
+
+    if (!priv->shared_tx_rings)
+        priv->min_num_stack_tx_queues = num_stack_tx_queues;
+
+    enetc_debug_tx_ring_prios(priv);
+
+    ec_enetc_change_preemptible_tcs(priv, mqprio->preemptible_tcs);
+
+    return 0;
+
+err_reset_tc:
+    ec_enetc_reset_tc_mqprio(ndev);
+    return err;
+}
+
+
+static int enetc_reconfigure_xdp_cb(struct enetc_ndev_priv *priv, void *ctx)
+{
+    struct bpf_prog *old_prog, *prog = ctx;
+    int num_stack_tx_queues;
+    int err, i;
+
+    old_prog = xchg(&priv->xdp_prog, prog);
+
+    num_stack_tx_queues = enetc_num_stack_tx_queues(priv);
+    err = netif_set_real_num_tx_queues(priv->ndev, num_stack_tx_queues);
+    if (err) {
+        xchg(&priv->xdp_prog, old_prog);
+        return err;
+    }
+
+    if (old_prog)
+        bpf_prog_put(old_prog);
+
+    for (i = 0; i < priv->num_rx_rings; i++) {
+        struct enetc_bdr *rx_ring = priv->rx_ring[i];
+
+        rx_ring->xdp.prog = prog;
+
+        if (prog)
+            rx_ring->buffer_offset = XDP_PACKET_HEADROOM;
+        else
+            rx_ring->buffer_offset = ENETC_RXB_PAD;
+    }
+
+    return 0;
+}
+
+static int enetc_setup_xdp_prog(struct net_device *ndev, struct bpf_prog *prog,
+                struct netlink_ext_ack *extack)
+{
+    int num_xdp_tx_queues = prog ? num_possible_cpus() : 0;
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    bool extended, update_bdrs;
+    struct bpf_prog *old_prog;
+    int i;
+
+    if (priv->active_offloads & ENETC_F_RSC) {
+        netdev_err(ndev, "XDP and RSC cannot be enabled at the same time\n");
+        return -EOPNOTSUPP;
+    }
+
+    update_bdrs = !!priv->xdp_prog != !!prog;
+    if (!update_bdrs) {
+        old_prog = xchg(&priv->xdp_prog, prog);
+
+        for (i = 0; i < priv->num_rx_rings; i++)
+            priv->rx_ring[i]->xdp.prog = prog;
+
+        if (old_prog)
+            bpf_prog_put(old_prog);
+
+        return 0;
+    }
+
+    if (!priv->shared_tx_rings &&
+        priv->min_num_stack_tx_queues + num_xdp_tx_queues >
+        priv->num_tx_rings) {
+        NL_SET_ERR_MSG_FMT_MOD(extack,
+                       "Reserving %d XDP TXQs leaves under %d for stack (total %d)",
+                       num_xdp_tx_queues,
+                       priv->min_num_stack_tx_queues,
+                       priv->num_tx_rings);
+        return -EBUSY;
+    }
+
+    extended = !!(priv->active_offloads & ENETC_F_RX_TSTAMP);
+
+    /* The buffer layout is changing, so we need to drain the old
+     * RX buffers and seed new ones.
+     */
+    return enetc_reconfigure(priv, extended, enetc_reconfigure_xdp_cb, prog);
+}
+
+int ec_enetc_setup_bpf(struct net_device *ndev, struct netdev_bpf *bpf)
+{
+    switch (bpf->command) {
+    case XDP_SETUP_PROG:
+        return enetc_setup_xdp_prog(ndev, bpf->prog, bpf->extack);
+    default:
+        return -EINVAL;
+    }
+
+    return 0;
+}
+
+struct net_device_stats *ec_enetc_get_stats(struct net_device *ndev)
+{
+	printk(KERN_ERR "ec_enetc_get_stats: %s %s %d\n", __FILE__, __FUNCTION__, __LINE__);
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct net_device_stats *stats = &ndev->stats;
+    unsigned long packets = 0, bytes = 0;
+    unsigned long tx_dropped = 0;
+    int i;
+
+	printk(KERN_ERR "priv->num_rx_rings: %s %s %d %d\n", __FILE__, __FUNCTION__, __LINE__, priv->num_rx_rings);
+    for (i = 0; i < priv->num_rx_rings; i++) {
+        packets += priv->rx_ring[i]->stats.packets;
+		printk(KERN_ERR "packets: %s %s %d %d\n", __FILE__, __FUNCTION__, __LINE__, packets);
+        bytes   += priv->rx_ring[i]->stats.bytes;
+		printk(KERN_ERR "bytes: %s %s %d %d\n", __FILE__, __FUNCTION__, __LINE__, bytes);
+    }
+
+	printk(KERN_ERR "ec_enetc_get_stats: %s %s %d\n", __FILE__, __FUNCTION__, __LINE__);
+    stats->rx_packets = packets;
+    stats->rx_bytes = bytes;
+    bytes = 0;
+    packets = 0;
+
+	printk(KERN_ERR "priv->num_tx_rings: %s %s %d %d\n", __FILE__, __FUNCTION__, __LINE__, priv->num_tx_rings);
+    for (i = 0; i < priv->num_tx_rings; i++) {
+        packets += priv->tx_ring[i]->stats.packets;
+		printk(KERN_ERR "packets: %s %s %d %d\n", __FILE__, __FUNCTION__, __LINE__, packets);
+        bytes   += priv->tx_ring[i]->stats.bytes;
+		printk(KERN_ERR "bytes: %s %s %d %d\n", __FILE__, __FUNCTION__, __LINE__, bytes);
+        tx_dropped += priv->tx_ring[i]->stats.win_drop;
+		printk(KERN_ERR "tx_dropped: %s %s %d\n", __FILE__, __FUNCTION__, __LINE__, tx_dropped);
+    }
+
+	printk(KERN_ERR "ec_enetc_get_stats: %s %s %d\n", __FILE__, __FUNCTION__, __LINE__);
+    stats->tx_packets = packets;
+    stats->tx_bytes = bytes;
+    stats->tx_dropped = tx_dropped;
+
+	printk(KERN_ERR "ec_enetc_get_stats: %s %s %d\n", __FILE__, __FUNCTION__, __LINE__);
+    return stats;
+}
+
+static void enetc_enable_rxvlan(struct net_device *ndev, bool en)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct enetc_hw *hw = &priv->si->hw;
+    int i;
+
+    for (i = 0; i < priv->num_rx_rings; i++)
+        enetc_bdr_enable_rxvlan(hw, i, en);
+}
+
+static void enetc_enable_txvlan(struct net_device *ndev, bool en)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct enetc_hw *hw = &priv->si->hw;
+    int i;
+
+    for (i = 0; i < priv->num_tx_rings; i++)
+        enetc_bdr_enable_txvlan(hw, i, en);
+}
+
+void ec_enetc_set_features(struct net_device *ndev, netdev_features_t features)
+{
+    netdev_features_t changed = ndev->features ^ features;
+
+    if (changed & NETIF_F_LRO)
+        enetc_set_rsc(ndev, !!(features & NETIF_F_LRO));
+
+    if (changed & NETIF_F_RXHASH)
+        enetc_set_rss(ndev, !!(features & NETIF_F_RXHASH));
+
+    if (changed & NETIF_F_HW_VLAN_CTAG_RX)
+        enetc_enable_rxvlan(ndev,
+                    !!(features & NETIF_F_HW_VLAN_CTAG_RX));
+
+    if (changed & NETIF_F_HW_VLAN_CTAG_TX)
+        enetc_enable_txvlan(ndev,
+                    !!(features & NETIF_F_HW_VLAN_CTAG_TX));
+}
+
+static int enetc_hwtstamp_set(struct net_device *ndev, struct ifreq *ifr)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    int err, new_offloads = priv->active_offloads;
+    struct hwtstamp_config config;
+
+    if (copy_from_user(&config, ifr->ifr_data, sizeof(config)))
+        return -EFAULT;
+
+    switch (config.tx_type) {
+    case HWTSTAMP_TX_OFF:
+        new_offloads &= ~ENETC_F_TX_TSTAMP_MASK;
+        break;
+    case HWTSTAMP_TX_ON:
+        new_offloads &= ~ENETC_F_TX_TSTAMP_MASK;
+        new_offloads |= ENETC_F_TX_TSTAMP;
+        break;
+    case HWTSTAMP_TX_ONESTEP_SYNC:
+        /* When preemption is enabled on a port, IEEE 1588 PTP
+         * one-step timestamping is not supported.
+         */
+        if (!!(priv->active_offloads & ENETC_F_QBU) &&
+            is_enetc_rev4(priv->si))
+            return -EOPNOTSUPP;
+
+        /* Pseudo MAC does not support one-step timestamp */
+        if (priv->si->hw_features & ENETC_SI_F_PPM)
+            return -EOPNOTSUPP;
+
+        new_offloads &= ~ENETC_F_TX_TSTAMP_MASK;
+        new_offloads |= ENETC_F_TX_ONESTEP_SYNC_TSTAMP;
+        break;
+    default:
+        return -ERANGE;
+    }
+
+    switch (config.rx_filter) {
+    case HWTSTAMP_FILTER_NONE:
+        new_offloads &= ~ENETC_F_RX_TSTAMP;
+        break;
+    default:
+        new_offloads |= ENETC_F_RX_TSTAMP;
+        config.rx_filter = HWTSTAMP_FILTER_ALL;
+    }
+
+    if ((new_offloads ^ priv->active_offloads) & ENETC_F_RX_TSTAMP &&
+        !(priv->active_offloads & ENETC_F_RSC)) {
+        bool extended = !!(new_offloads & ENETC_F_RX_TSTAMP);
+
+        err = enetc_reconfigure(priv, extended, NULL, NULL);
+        if (err)
+            return err;
+    }
+
+    priv->active_offloads = new_offloads;
+
+    return copy_to_user(ifr->ifr_data, &config, sizeof(config)) ?
+           -EFAULT : 0;
+}
+
+static int enetc_hwtstamp_get(struct net_device *ndev, struct ifreq *ifr)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct hwtstamp_config config;
+
+    config.flags = 0;
+
+    if (priv->active_offloads & ENETC_F_TX_ONESTEP_SYNC_TSTAMP)
+        config.tx_type = HWTSTAMP_TX_ONESTEP_SYNC;
+    else if (priv->active_offloads & ENETC_F_TX_TSTAMP)
+        config.tx_type = HWTSTAMP_TX_ON;
+    else
+        config.tx_type = HWTSTAMP_TX_OFF;
+
+    config.rx_filter = (priv->active_offloads & ENETC_F_RX_TSTAMP) ?
+                HWTSTAMP_FILTER_ALL : HWTSTAMP_FILTER_NONE;
+
+    return copy_to_user(ifr->ifr_data, &config, sizeof(config)) ?
+           -EFAULT : 0;
+}
+
+int ec_enetc_ioctl(struct net_device *ndev, struct ifreq *rq, int cmd)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+
+    if (enetc_ptp_clock_is_enabled(priv->si)) {
+        if (cmd == SIOCSHWTSTAMP)
+            return enetc_hwtstamp_set(ndev, rq);
+        if (cmd == SIOCGHWTSTAMP)
+            return enetc_hwtstamp_get(ndev, rq);
+    }
+
+    if (!priv->phylink)
+        return -EOPNOTSUPP;
+
+    return phylink_mii_ioctl(priv->phylink, rq, cmd);
 }
 
 static void enetc_kfree_si(struct enetc_si *si)
@@ -1151,7 +3118,7 @@ static void enetc_detect_errata(struct enetc_si *si)
 			     ENETC_ERR_UCMCSWP;
 }
 
-int enetc_pci_probe(struct pci_dev *pdev, const char *name, int sizeof_priv)
+int ec_enetc_pci_probe(struct pci_dev *pdev, const char *name, int sizeof_priv)
 {
 	struct enetc_si *si, *p;
 	struct enetc_hw *hw;
@@ -1233,7 +3200,7 @@ err_dma:
 	return err;
 }
 
-void enetc_pci_remove(struct pci_dev *pdev)
+void ec_enetc_pci_remove(struct pci_dev *pdev)
 {
 	struct enetc_si *si = pci_get_drvdata(pdev);
 	struct enetc_hw *hw = &si->hw;
@@ -1243,3 +3210,6 @@ void enetc_pci_remove(struct pci_dev *pdev)
 	pci_release_mem_regions(pdev);
 	pci_disable_device(pdev);
 }
+
+MODULE_DESCRIPTION("NXP ENETC Ethernet driver");
+MODULE_LICENSE("Dual BSD/GPL");
diff --git a/devices/enetc/enetc.h b/devices/enetc/enetc.h
index 6d48d3f..a06ac4b 100644
--- a/devices/enetc/enetc.h
+++ b/devices/enetc/enetc.h
@@ -10,32 +10,77 @@
 #include <linux/ethtool.h>
 #include <linux/if_vlan.h>
 #include <linux/phylink.h>
+#include <linux/fsl/netc_lib.h>
 #include <linux/dim.h>
+#include <net/xdp.h>
 #ifdef CONFIG_ENETC_TSN
 #include <net/tsn.h>
 #endif
 
 #include "enetc_hw.h"
+#include "enetc4_hw.h"
 #include "../ecdev.h"
+#include "enetc_msg.h"
+
+#define ENETC_CBD_DATA_MEM_ALIGN 64
+
 #define ENETC_MAC_MAXFRM_SIZE	9600
+#define ENETC4_MAC_MAXFRM_SIZE     2000
 #define ENETC_MAX_MTU		(ENETC_MAC_MAXFRM_SIZE - \
 				(ETH_FCS_LEN + ETH_HLEN + VLAN_HLEN))
+#define ENETC4_MAX_MTU      (ENETC4_MAC_MAXFRM_SIZE - \
+                (ETH_FCS_LEN + ETH_HLEN + VLAN_HLEN))
+
+#define ENETC_CBD_DATA_MEM_ALIGN 64
+
+#define ENETC_INT_NAME_MAX  (IFNAMSIZ + 8)
 
 struct enetc_tx_swbd {
-	struct sk_buff *skb;
-	dma_addr_t dma;
-	u16 len;
-	u8 is_dma_page:1;
-	u8 check_wb:1;
-	u8 do_tstamp:1;
-	u8 qbv_en:1;
+    union {
+        struct sk_buff *skb;
+        struct xdp_frame *xdp_frame;
+    };
+    dma_addr_t dma;
+    struct page *page;  /* valid only if is_xdp_tx */
+    u16 page_offset;    /* valid only if is_xdp_tx */
+    u16 len;
+    enum dma_data_direction dir;
+    u8 is_dma_page:1;
+    u8 check_wb:1;
+    u8 do_twostep_tstamp:1;
+    u8 is_eof:1;
+    u8 is_xdp_tx:1;
+    u8 is_xdp_redirect:1;
+    u8 qbv_en:1;
 };
 
+struct enetc_lso_t {
+    bool    ipv6;
+    bool    tcp;
+    u8  l3_hdr_len;
+    u8  hdr_len; /* LSO header length */
+    u8  l3_start;
+    u16 lso_seg_size;
+    int total_len; /* total data length, not include LSO header */
+};
+
+struct enetc_xdp_data {
+    struct xdp_rxq_info rxq;
+    struct bpf_prog *prog;
+    int xdp_tx_in_flight;
+};
+
+#define ENETC_1KB_SIZE          1024
+#define ENETC_LSO_MAX_DATA_LEN      (256 * ENETC_1KB_SIZE)
+
 #define ENETC_RX_MAXFRM_SIZE	ENETC_MAC_MAXFRM_SIZE
 #define ENETC_RXB_TRUESIZE	2048 /* PAGE_SIZE >> 1 */
 #define ENETC_RXB_PAD		NET_SKB_PAD /* add extra space if needed */
 #define ENETC_RXB_DMA_SIZE	\
 	(SKB_WITH_OVERHEAD(ENETC_RXB_TRUESIZE) - ENETC_RXB_PAD)
+#define ENETC_RXB_DMA_SIZE_XDP  \
+    (SKB_WITH_OVERHEAD(ENETC_RXB_TRUESIZE) - XDP_PACKET_HEADROOM)
+#define ENETC_RS_MAX_BYTES  (ENETC_RXB_DMA_SIZE * (MAX_SKB_FRAGS + 1))
 
 struct enetc_rx_swbd {
 	dma_addr_t dma;
@@ -43,17 +88,50 @@ struct enetc_rx_swbd {
 	u16 page_offset;
 };
 
+/* ENETC overhead: optional extension BD + 1 BD gap */
+#define ENETC_TXBDS_NEEDED(val) ((val) + 2)
+/* max # of chained Tx BDs is 15, including head and extension BD */
+#define ENETC_MAX_SKB_FRAGS 13
+/* For ENETC 4, max # of chained Tx BDs is 63, including head and extension BD */
+#define ENETC4_MAX_SKB_FRAGS    61
+/* 3: 1 BD for head, 1 BD for optional extended BD and 1 BD gap */
+#define ENETC_TX_STOP_THRESHOLD (MAX_SKB_FRAGS + 3)
+
 struct enetc_ring_stats {
-	unsigned int packets;
-	unsigned int bytes;
-	unsigned int rx_alloc_errs;
-	unsigned int win_drop;
+    unsigned int packets;
+    unsigned int bytes;
+    unsigned int rx_alloc_errs;
+    unsigned int xdp_drops;
+    unsigned int xdp_tx;
+    unsigned int xdp_tx_drops;
+    unsigned int xdp_redirect;
+    unsigned int xdp_redirect_failures;
+    unsigned int recycles;
+    unsigned int recycle_failures;
+    unsigned int win_drop;
 };
 
 #define ENETC_RX_RING_DEFAULT_SIZE	512
 #define ENETC_TX_RING_DEFAULT_SIZE	256
 #define ENETC_DEFAULT_TX_WORK		(ENETC_TX_RING_DEFAULT_SIZE / 2)
 
+struct enetc_bdr_resource {
+    /* Input arguments saved for teardown */
+    struct device *dev; /* for DMA mapping */
+    size_t bd_count;
+    size_t bd_size;
+
+    /* Resource proper */
+    void *bd_base; /* points to Rx or Tx BD ring */
+    dma_addr_t bd_dma_base;
+    union {
+        struct enetc_tx_swbd *tx_swbd;
+        struct enetc_rx_swbd *rx_swbd;
+    };
+    char *tso_headers;
+    dma_addr_t tso_headers_dma;
+};
+
 struct enetc_bdr {
 	struct device *dev; /* for DMA mapping */
 	struct net_device *ndev;
@@ -63,6 +141,7 @@ struct enetc_bdr {
 		void __iomem *rcir;
 	};
 	u16 index;
+	u16 prio;
 	int bd_count; /* # of BDs */
 	int next_to_use;
 	int next_to_clean;
@@ -76,11 +155,18 @@ struct enetc_bdr {
 	};
 	void __iomem *idr; /* Interrupt Detect Register pointer */
 
+	int buffer_offset;
+	struct enetc_xdp_data xdp;
+
 	struct enetc_ring_stats stats;
 
 	dma_addr_t bd_dma_base;
 	u8 tsd_enable; /* Time specific departure */
 	bool ext_en; /* enable h/w descriptor extensions */
+
+	/* DMA buffer for TSO headers */
+    char *tso_headers;
+    dma_addr_t tso_headers_dma;
 } ____cacheline_aligned_in_smp;
 
 static inline void enetc_bdr_idx_inc(struct enetc_bdr *bdr, int *i)
@@ -103,12 +189,14 @@ struct enetc_cbdr {
 	void *bd_base; /* points to Rx or Tx BD ring */
 	void __iomem *pir;
 	void __iomem *cir;
+	void __iomem *mr; /* mode register */
 
 	int bd_count; /* # of BDs */
 	int next_to_use;
 	int next_to_clean;
 
 	dma_addr_t bd_dma_base;
+	struct device *dma_dev;
 };
 
 #define ENETC_TXBD(BDR, i) (&(((union enetc_tx_bd *)((BDR).bd_base))[i]))
@@ -139,17 +227,6 @@ static inline union enetc_rx_bd *enetc_rxbd_next(struct enetc_bdr *rx_ring,
 	return rxbd;
 }
 
-static inline union enetc_rx_bd *enetc_rxbd_ext(union enetc_rx_bd *rxbd)
-{
-	return ++rxbd;
-}
-
-struct enetc_msg_swbd {
-	void *vaddr;
-	dma_addr_t dma;
-	int size;
-};
-
 #ifdef CONFIG_ENETC_TSN
 /* Credit-Based Shaper parameters */
 struct cbs {
@@ -173,21 +250,49 @@ struct enetc_cbs {
 #endif
 
 #define ENETC_REV1	0x1
+#define ENETC_REV4  0x4
 enum enetc_errata {
 	ENETC_ERR_TXCSUM	= BIT(0),
 	ENETC_ERR_VLAN_ISOL	= BIT(1),
 	ENETC_ERR_UCMCSWP	= BIT(2),
 };
 
-#define ENETC_SI_F_QBV BIT(0)
-#define ENETC_SI_F_QBU BIT(1)
-#define ENETC_SI_F_PSFP BIT(1)
+#define ENETC_SI_F_PSFP BIT(0)
+#define ENETC_SI_F_QBV  BIT(1)
+#define ENETC_SI_F_QBU  BIT(2)
+#define ENETC_SI_F_LSO  BIT(3)
+#define ENETC_SI_F_RSC  BIT(4)
+#define ENETC_SI_F_PPM  BIT(5) /* Pseduo MAC */
+
+enum enetc_mac_addr_type {UC, MC, MADDR_TYPE};
+
+#define ENETC_MADDR_HASH_TBL_SZ 64
+struct enetc_mac_filter {
+    union {
+        char mac_addr[ETH_ALEN];
+        DECLARE_BITMAP(mac_hash_table, ENETC_MADDR_HASH_TBL_SZ);
+    };
+    int mac_addr_cnt;
+};
+
+#define ENETC_VLAN_HT_SIZE  64
+#define ENETC_INT_NAME_MAX  (IFNAMSIZ + 8)
+struct enetc_debugfs_params {
+    u32 isit_eid;
+    u32 ist_eid;
+    u32 isft_eid;
+    u32 sgit_eid;
+    u32 sgclt_eid;
+    u32 isct_eid;
+    u32 rpt_eid;
+};
 
 /* PCI IEP device data */
 struct enetc_si {
 	struct pci_dev *pdev;
 	struct enetc_hw hw;
 	enum enetc_errata errata;
+	u16 revision;
 
 	struct net_device *ndev; /* back ref. */
 
@@ -199,12 +304,92 @@ struct enetc_si {
 	int num_rss; /* number of RSS buckets */
 	unsigned short pad;
 	int hw_features;
+	int pmac_offset; /* Only valid for PSI that supports 802.1Qbu */
+    struct enetc_cbs *ecbs;
+
+	u64 clk_freq;
+    struct ntmp_priv ntmp;
+    struct dentry *debugfs_root;
+    struct enetc_debugfs_params dbg_params;
+
+	struct workqueue_struct *workqueue;
+    struct work_struct rx_mode_task;
+    struct work_struct msg_task;
+    struct enetc_mac_filter mac_filter[MADDR_TYPE];
+    struct mutex msg_lock; /* mailbox message lock */
+    char msg_int_name[ENETC_INT_NAME_MAX];
+
+    DECLARE_BITMAP(active_vlans, VLAN_N_VID);
+    DECLARE_BITMAP(vlan_ht_filter, ENETC_VLAN_HT_SIZE);
+
+	int (*set_rss_table)(struct enetc_si *si, const u32 *table, int count);
+    int (*get_rss_table)(struct enetc_si *si, u32 *table, int count);
+
+	/* Notice, only for VSI/VF to use */
+    int (*vf_register_msg_msix)(struct enetc_si *si);
+    void (*vf_free_msg_msix)(struct enetc_si *si);
+    int (*vf_register_link_status_notify)(struct enetc_si *si, bool notify);
+
 #ifdef CONFIG_ENETC_TSN
 	struct enetc_cbs *ecbs;
 #endif
 
 };
 
+#define ntmp_to_enetc_si(ntmp_priv) \
+    container_of((ntmp_priv), struct enetc_si, ntmp)
+
+static inline bool is_enetc_rev1(struct enetc_si *si)
+{
+    return si->pdev->revision == ENETC_REV1;
+}
+
+static inline bool is_enetc_rev4(struct enetc_si *si)
+{
+    return si->pdev->revision != ENETC_REV1;
+}
+
+static inline void *enetc_cbd_alloc_data_mem(struct enetc_si *si,
+                         struct enetc_cbd *cbd,
+                         int size, dma_addr_t *dma,
+                         void **data_align)
+{
+    struct enetc_cbdr *ring = &si->cbd_ring;
+    dma_addr_t dma_align;
+    void *data;
+
+    data = dma_alloc_coherent(ring->dma_dev,
+                  size + ENETC_CBD_DATA_MEM_ALIGN,
+                  dma, GFP_KERNEL);
+    if (!data) {
+        dev_err(ring->dma_dev, "CBD alloc data memory failed!\n");
+        return NULL;
+    }
+
+    dma_align = ALIGN(*dma, ENETC_CBD_DATA_MEM_ALIGN);
+    *data_align = PTR_ALIGN(data, ENETC_CBD_DATA_MEM_ALIGN);
+
+    cbd->addr[0] = cpu_to_le32(lower_32_bits(dma_align));
+    cbd->addr[1] = cpu_to_le32(upper_32_bits(dma_align));
+    cbd->length = cpu_to_le16(size);
+
+    return data;
+}
+
+static inline void enetc_cbd_free_data_mem(struct enetc_si *si, int size,
+                       void *data, dma_addr_t *dma)
+{
+    struct enetc_cbdr *ring = &si->cbd_ring;
+
+    dma_free_coherent(ring->dma_dev, size + ENETC_CBD_DATA_MEM_ALIGN,
+              data, *dma);
+}
+
+static inline union enetc_rx_bd *enetc_rxbd_ext(union enetc_rx_bd *rxbd)
+{
+    return ++rxbd;
+}
+
 #define ENETC_SI_ALIGN	32
 
 static inline void *enetc_si_priv(const struct enetc_si *si)
@@ -217,8 +402,37 @@ static inline bool enetc_si_is_pf(struct enetc_si *si)
 	return !!(si->hw.port);
 }
 
+static inline int enetc_pf_to_port(struct pci_dev *pf_pdev)
+{
+    switch (pf_pdev->devfn) {
+    case 0:
+        return 0;
+    case 1:
+        return 1;
+    case 2:
+        return 2;
+    case 6:
+        return 3;
+    default:
+        return -1;
+    }
+}
+
+static inline int enetc4_pf_to_port(struct pci_dev *pf_pdev)
+{
+    switch (pf_pdev->devfn) {
+    case 0:
+        return 0;
+    case 64:
+        return 1;
+    case 128:
+        return 2;
+    default:
+        return -1;
+    }
+}
+
 #define ENETC_MAX_NUM_TXQS	8
-#define ENETC_INT_NAME_MAX	(IFNAMSIZ + 8)
 
 struct enetc_int_vector {
 	void __iomem *rbier;
@@ -251,13 +465,25 @@ struct psfp_cap {
 	u32 max_psfp_meter;
 };
 
-/* TODO: more hardware offloads */
+#define ENETC_F_TX_TSTAMP_MASK  0xff
 enum enetc_active_offloads {
-	ENETC_F_RX_TSTAMP	= BIT(0),
-	ENETC_F_TX_TSTAMP	= BIT(1),
-	ENETC_F_QBV             = BIT(2),
-	ENETC_F_QCI		= BIT(3),
-	ENETC_F_QBU             = BIT(4),
+    /* 8 bits reserved for TX timestamp types (hwtstamp_tx_types) */
+    ENETC_F_TX_TSTAMP       = BIT(0),
+    ENETC_F_TX_ONESTEP_SYNC_TSTAMP  = BIT(1),
+
+    ENETC_F_RX_TSTAMP       = BIT(8),
+    ENETC_F_QBV         = BIT(9),
+    ENETC_F_QCI         = BIT(10),
+    ENETC_F_QBU         = BIT(11),
+
+    ENETC_F_CHECKSUM        = BIT(12),
+    ENETC_F_LSO         = BIT(13),
+    ENETC_F_RSC         = BIT(14),
+};
+
+enum enetc_flags_bit {
+    ENETC_TX_ONESTEP_TSTAMP_IN_PROGRESS = 0,
+    ENETC_TX_DOWN,
 };
 
 /* interrupt coalescing modes */
@@ -273,12 +499,16 @@ enum enetc_ic_mode {
 
 #define ENETC_RXIC_PKTTHR	min_t(u32, 256, ENETC_RX_RING_DEFAULT_SIZE / 2)
 #define ENETC_TXIC_PKTTHR	min_t(u32, 128, ENETC_TX_RING_DEFAULT_SIZE / 2)
-#define ENETC_TXIC_TIMETHR	enetc_usecs_to_cycles(600)
+
+#define ENETC_TXIC_TIMETHR  enetc_usecs_to_cycles(600, ENETC_CLK)
+#define ENETC4_TXIC_TIMETHR enetc_usecs_to_cycles(500, ENETC4_CLK)
 
 struct enetc_ndev_priv {
 	struct net_device *ndev;
 	struct device *dev; /* dma-mapping device */
 	struct enetc_si *si;
+	struct clk *ref_clk; /* RGMII/RMII reference clock */
+    struct pci_dev *rcec;
 
 	int bdr_int_num; /* number of Rx/Tx ring interrupts */
 	struct enetc_int_vector *int_vector[ENETC_MAX_BDR_INT];
@@ -286,22 +516,55 @@ struct enetc_ndev_priv {
 	u16 rx_bd_count, tx_bd_count;
 
 	u16 msg_enable;
-	int active_offloads;
 
-	u32 speed; /* store speed for compare update pspeed */
+	u8 preemptible_tcs;
+    /* Kernel stack and XDP share the tx rings, note that shared_tx_ring
+     * cannot be set to 'true' when enetc_has_err050089 is true, because
+     * this may cause a deadlock.
+     */
+    bool shared_tx_rings;
+
+	enum enetc_active_offloads active_offloads;
 
+	u32 speed; /* store speed for compare update pspeed */
+	struct enetc_bdr **xdp_tx_ring;
 	struct enetc_bdr *tx_ring[16];
 	struct enetc_bdr *rx_ring[16];
+	const struct enetc_bdr_resource *tx_res;
+    const struct enetc_bdr_resource *rx_res;	
 
 	struct enetc_cls_rule *cls_rules;
+	int max_ipf_entries;
+    u32 ipt_wol_eid;
+
 	ec_device_t *ecdev;
 
+	struct ethtool_keee eee;
 	struct psfp_cap psfp_cap;
 
+	/* Minimum number of TX queues required by the network stack */
+    unsigned int min_num_stack_tx_queues;
+
 	struct phylink *phylink;
 	int ic_mode;
 	u32 tx_ictt;
 	struct device *ptp_dev;
+
+	struct bpf_prog *xdp_prog;
+
+    unsigned long flags;
+    int wolopts;
+
+    struct work_struct  tx_onestep_tstamp;
+    struct sk_buff_head tx_skbs;
+
+    /* The maximum number of BDs for fragments */
+    int max_frags_bd;
+
+    /* Serialize access to MAC Merge state between ethtool requests
+     * and link state updates
+     */
+    struct mutex        mm_lock;
 };
 
 /* Messaging */
@@ -320,24 +583,41 @@ struct enetc_msg_cmd_set_primary_mac {
 extern int enetc_phc_index;
 
 /* SI common */
-int enetc_pci_probe(struct pci_dev *pdev, const char *name, int sizeof_priv);
-void enetc_pci_remove(struct pci_dev *pdev);
-int enetc_alloc_msix(struct enetc_ndev_priv *priv);
-void enetc_free_msix(struct enetc_ndev_priv *priv);
-void enetc_get_si_caps(struct enetc_si *si);
-void enetc_init_si_rings_params(struct enetc_ndev_priv *priv);
-int enetc_alloc_si_resources(struct enetc_ndev_priv *priv);
-void enetc_free_si_resources(struct enetc_ndev_priv *priv);
-
-int enetc_open(struct net_device *ndev);
-int enetc_close(struct net_device *ndev);
-void enetc_start(struct net_device *ndev);
-void enetc_stop(struct net_device *ndev);
-netdev_tx_t enetc_xmit(struct sk_buff *skb, struct net_device *ndev);
-struct net_device_stats *enetc_get_stats(struct net_device *ndev);
-int enetc_set_features(struct net_device *ndev,
+u32 ec_enetc_port_mac_rd(struct enetc_si *si, u32 reg);
+void ec_enetc_port_mac_wr(struct enetc_si *si, u32 reg, u32 val);
+int ec_enetc_pci_probe(struct pci_dev *pdev, const char *name, int sizeof_priv);
+void ec_enetc_pci_remove(struct pci_dev *pdev);
+int ec_enetc_alloc_msix(struct enetc_ndev_priv *priv);
+void ec_enetc_free_msix(struct enetc_ndev_priv *priv);
+void ec_enetc_get_si_caps(struct enetc_si *si);
+void ec_enetc_init_si_rings_params(struct enetc_ndev_priv *priv);
+int ec_enetc_alloc_si_resources(struct enetc_ndev_priv *priv);
+void ec_enetc_free_si_resources(struct enetc_ndev_priv *priv);
+int ec_enetc_configure_si(struct enetc_ndev_priv *priv);
+
+int ec_enetc_suspend(struct net_device *ndev, bool wol);
+int ec_enetc_resume(struct net_device *ndev, bool wol);
+int ec_enetc_open(struct net_device *ndev);
+int ec_enetc_close(struct net_device *ndev);
+void ec_enetc_start(struct net_device *ndev);
+void ec_enetc_stop(struct net_device *ndev);
+netdev_tx_t ec_enetc_xmit(struct sk_buff *skb, struct net_device *ndev);
+struct net_device_stats *ec_enetc_get_stats(struct net_device *ndev);
+void ec_enetc_set_features(struct net_device *ndev,
 		       netdev_features_t features);
-int enetc_ioctl(struct net_device *ndev, struct ifreq *rq, int cmd);
+int ec_enetc_ioctl(struct net_device *ndev, struct ifreq *rq, int cmd);
+int ec_enetc_setup_tc_mqprio(struct net_device *ndev, void *type_data);
+void ec_enetc_reset_tc_mqprio(struct net_device *ndev);
+int ec_enetc_setup_bpf(struct net_device *ndev, struct netdev_bpf *bpf);
+int ec_enetc_xdp_xmit(struct net_device *ndev, int num_frames,
+           struct xdp_frame **frames, u32 flags);
+void ec_enetc_change_preemptible_tcs(struct enetc_ndev_priv *priv,
+                  u8 preemptible_tcs);
+void ec_enetc_reset_mac_addr_filter(struct enetc_mac_filter *filter);
+void ec_enetc_add_mac_addr_ht_filter(struct enetc_mac_filter *filter,
+                  const unsigned char *addr);
+int ec_enetc_vid_hash_idx(unsigned int vid);
+void ec_enetc_refresh_vlan_ht_filter(struct enetc_si *si);
 int enetc_setup_tc(struct net_device *ndev, enum tc_setup_type type,
 		   void *type_data);
 void ec_poll(struct net_device *ndev);
@@ -345,29 +625,41 @@ int enetc_alloc_rings(struct enetc_ndev_priv *priv);
 void enetc_free_rings(struct enetc_ndev_priv *priv);
 
 /* ethtool */
-void enetc_set_ethtool_ops(struct net_device *ndev);
+void ec_enetc_set_ethtool_ops(struct net_device *ndev);
+void ec_enetc_mm_link_state_update(struct enetc_ndev_priv *priv, bool link);
+void ec_enetc_mm_commit_preemptible_tcs(struct enetc_ndev_priv *priv);
+void ec_enetc_eee_mode_set(struct net_device *dev, bool enable);
 
 /* control buffer descriptor ring (CBDR) */
-int enetc_set_mac_flt_entry(struct enetc_si *si, int index,
+int ec_enetc_init_cbdr(struct enetc_si *si);
+void ec_enetc_free_cbdr(struct enetc_si *si);
+int ec_enetc_set_mac_flt_entry(struct enetc_si *si, int index,
 			    char *mac_addr, int si_map);
-int enetc_clear_mac_flt_entry(struct enetc_si *si, int index);
-int enetc_set_fs_entry(struct enetc_si *si, struct enetc_cmd_rfse *rfse,
+int ec_enetc_clear_mac_flt_entry(struct enetc_si *si, int index);
+int ec_enetc_set_fs_entry(struct enetc_si *si, struct enetc_cmd_rfse *rfse,
 		       int index);
-void enetc_set_rss_key(struct enetc_hw *hw, const u8 *bytes);
-int enetc_get_rss_table(struct enetc_si *si, u32 *table, int count);
-int enetc_set_rss_table(struct enetc_si *si, const u32 *table, int count);
-int enetc_send_cmd(struct enetc_si *si, struct enetc_cbd *cbd);
+void ec_enetc_set_rss_key(struct enetc_hw *hw, const u8 *bytes);
+int ec_enetc_get_rss_table(struct enetc_si *si, u32 *table, int count);
+int ec_enetc_set_rss_table(struct enetc_si *si, const u32 *table, int count);
+int ec_enetc_send_cmd(struct enetc_si *si, struct enetc_cbd *cbd);
+
+static inline bool enetc_ptp_clock_is_enabled(struct enetc_si *si)
+{
+    return !!((IS_ENABLED(CONFIG_FSL_ENETC_PTP_CLOCK) && is_enetc_rev1(si)) ||
+          (IS_ENABLED(CONFIG_PTP_1588_CLOCK_NETC) && is_enetc_rev4(si)));
+}
 
 #ifdef CONFIG_FSL_ENETC_QOS
+int enetc_qos_query_caps(struct net_device *ndev, void *type_data);
 int enetc_setup_tc_taprio(struct net_device *ndev, void *type_data);
-void enetc_sched_speed_set(struct enetc_ndev_priv *priv, int speed);
+//void enetc_sched_speed_set(struct enetc_ndev_priv *priv, int speed);
 int enetc_setup_tc_cbs(struct net_device *ndev, void *type_data);
 int enetc_setup_tc_txtime(struct net_device *ndev, void *type_data);
-int enetc_setup_tc_block_cb(enum tc_setup_type type, void *type_data,
-			    void *cb_priv);
 int enetc_setup_tc_psfp(struct net_device *ndev, void *type_data);
 int enetc_psfp_init(struct enetc_ndev_priv *priv);
 int enetc_psfp_clean(struct enetc_ndev_priv *priv);
+int enetc_set_tc_flower(struct net_device *ndev, bool en);
+void enetc4_clear_flower_list(struct enetc_si *si);
 
 static inline void enetc_get_max_cap(struct enetc_ndev_priv *priv)
 {
@@ -424,12 +716,12 @@ static inline int enetc_psfp_disable(struct enetc_ndev_priv *priv)
 }
 
 #else
+#define enetc_qos_query_caps(ndev, type_data) -EOPNOTSUPP
 #define enetc_setup_tc_taprio(ndev, type_data) -EOPNOTSUPP
 #define enetc_sched_speed_set(priv, speed) (void)0
 #define enetc_setup_tc_cbs(ndev, type_data) -EOPNOTSUPP
 #define enetc_setup_tc_txtime(ndev, type_data) -EOPNOTSUPP
 #define enetc_setup_tc_psfp(ndev, type_data) -EOPNOTSUPP
-#define enetc_setup_tc_block_cb NULL
 
 #define enetc_get_max_cap(p)		\
 	memset(&((p)->psfp_cap), 0, sizeof(struct psfp_cap))
@@ -443,6 +735,16 @@ static inline int enetc_psfp_disable(struct enetc_ndev_priv *priv)
 {
 	return 0;
 }
+
+static inline int enetc_set_tc_flower(struct net_device *ndev, bool en)
+{
+    return -EOPNOTSUPP;
+}
+
+static inline void enetc4_clear_flower_list(struct enetc_si *si)
+{
+}
+
 #endif
 #ifdef CONFIG_ENETC_TSN
 void enetc_tsn_pf_init(struct net_device *netdev, struct pci_dev *pdev);
@@ -455,4 +757,18 @@ void enetc_pspeed_set(struct enetc_ndev_priv *priv, int speed);
 #else
 #define enetc_tsn_pf_init(netdev, pdev) (void)0
 #define enetc_tsn_pf_deinit(netdev) (void)0
+
+#endif
+
+#if IS_ENABLED(CONFIG_DEBUG_FS)
+void enetc_create_debugfs(struct enetc_si *si);
+void enetc_remove_debugfs(struct enetc_si *si);
+#else
+static inline void enetc_create_debugfs(struct enetc_si *si)
+{
+}
+
+static inline void enetc_remove_debugfs(struct enetc_si *si)
+{
+}
 #endif
diff --git a/devices/enetc/enetc4_hw.h b/devices/enetc/enetc4_hw.h
new file mode 100755
index 0000000..f15303d
--- /dev/null
+++ b/devices/enetc/enetc4_hw.h
@@ -0,0 +1,586 @@
+/* SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause) */
+/*
+ * This header file defines the register offsets and bit fields
+ * of ENETC4 PF and VFs. Note that the same registers as ENETC
+ * version 1.0 are defined in the enetc_hw.h file.
+ *
+ * Copyright 2023 NXP
+ */
+#include <linux/bitops.h>
+
+#define NXP_ENETC_PPM_DEV_ID		0xe110
+
+/**********************Station interface registers************************/
+#define ENETC4_SIBCAR		0x40
+#define ENETC4_SIMCAR		0x44
+#define ENETC4_SICCAR		0x48
+
+/* Station interface transmit discard frame counter */
+#define ENETC4_SITDFCR		0x340
+
+/* Station interface LSO segmentation flag mask register 0/1 */
+#define ENETC4_SILSOSFMR0	0x1300
+#define  SILSOSFMR0_TCP_MID_SEG	GENMASK(27, 16)
+#define  SILSOSFMR0_TCP_1ST_SEG	GENMASK(11, 0)
+#define  SILSOSFMR0_VAL_SET(first, mid)	((((mid) << 16) & SILSOSFMR0_TCP_MID_SEG) | \
+					 ((first) & SILSOSFMR0_TCP_1ST_SEG))
+
+#define ENETC4_SILSOSFMR1		0x1304
+#define  SILSOSFMR1_TCP_LAST_SEG	GENMASK(11, 0)
+#define   TCP_FLAGS_FIN			BIT(0)
+#define   TCP_FLAGS_SYN			BIT(1)
+#define   TCP_FLAGS_RST			BIT(2)
+#define   TCP_FLAGS_PSH			BIT(3)
+#define   TCP_FLAGS_ACK			BIT(4)
+#define   TCP_FLAGS_URG			BIT(5)
+#define   TCP_FLAGS_ECE			BIT(6)
+#define   TCP_FLAGS_CWR			BIT(7)
+#define   TCP_FLAGS_NS			BIT(8)
+/* According to tso_build_hdr(), clear all special flags for not last packet. */
+#define TCP_NL_SEG_FLAGS_DMASK	(TCP_FLAGS_FIN | TCP_FLAGS_RST | TCP_FLAGS_PSH)
+
+/***************************ENETC port registers**************************/
+#define ENETC4_ECAPR0		0x0
+#define  ECAPR0_RFS		BIT(2)
+#define  ECAPR0_TSD		BIT(5)
+#define  ECAPR0_RSS		BIT(8)
+#define  ECAPR0_RSC		BIT(9)
+#define  ECAPR0_LSO		BIT(10)
+#define  ECAPR0_WO		BIT(13)
+
+#define ENETC4_ECAPR1		0x4
+#define  ECAPR1_NUM_TCS		GENMASK(6, 4)
+#define  ECAPR1_NUM_MCH		GENMASK(9, 8)
+#define  ECAPR1_NUM_UCH		GENMASK(11, 10)
+#define  ECAPR1_NUM_MSIX	GENMASK(22, 12)
+#define  ECAPR1_NUM_VSI		GENMASK(27, 24)
+#define  ECAPR1_NUM_IPV		BIT(31)
+
+#define ENETC4_ECAPR2		0x8
+#define  ECAPR2_NUM_TX_BDR	GENMASK(9, 0)
+#define  ECAPR2_NUM_RX_BDR	GENMASK(25, 16)
+
+#define ENETC4_PMR		0x10
+#define  PMR_SI_EN(a)		BIT((16 + (a)))
+
+/* Port Pause ON/OFF threshold register */
+#define ENETC4_PPAUONTR		0x108
+#define ENETC4_PPAUOFFTR	0x10c
+
+/* Port ingress congestion DRa (a=0,1,2,3) discard count register */
+#define ENETC4_PICDRDCR(a)	((a) * 0x10 + 0x140)
+
+/* Port Station interface promiscuous MAC mode register */
+#define ENETC4_PSIPMMR		0x200
+#define  PSIPMMR_SI0_MAC_UP	BIT(0)
+#define  PSIPMMR_SI_MAC_UP	GENMASK(2, 0)
+#define  PSIPMMR_SI0_MAC_MP	BIT(16)
+#define  PSIPMMR_SI_MAC_MP	GENMASK(18, 16)
+
+/* Port Station interface promiscuous VLAN mode register */
+#define ENETC4_PSIPVMR		0x204
+
+/* Port broadcast frames dropped due to MAC filtering register */
+#define ENETC4_PBFDSIR		0x208
+
+/* Port frame drop MAC source address pruning register */
+#define ENETC4_PFDMSAPR		0x20c
+
+/* Port RSS key register n. n = 0,1,2,...,9 */
+#define ENETC4_PRSSKR(n)	((n) * 0x4 + 0x250)
+
+/* Port station interface MAC address filtering capability register */
+#define ENETC4_PSIMAFCAPR		0x280
+#define  PSIMAFCAPR_NUM_MAC_AFTE	GENMASK(11, 0)
+
+/* Port unicast frames dropped due to MAC filtering register */
+#define ENETC4_PUFDMFR		0x284
+
+/* Port multicast frames dropped due to MAC filtering register */
+#define ENETC4_PMFDMFR		0x288
+
+/* Port station interface VLAN filtering capability register */
+#define ENETC4_PSIVLANFCAPR		0x2c0
+#define  PSIVLANFCAPR_NUM_VLAN_FTE	GENMASK(11, 0)
+
+/* Port station interface VLAN filtering mode register */
+#define ENETC4_PSIVLANFMR	0x2c4
+#define  PSIVLANFMR_VS		BIT(0)
+
+/* Port unicast frames dropped VLAN filtering register */
+#define ENETC4_PUFDVFR		0x2d0
+
+/* Port multicast frames dropped VLAN filtering register */
+#define ENETC4_PMFDVFR		0x2d4
+
+/* Port broadcast frames dropped VLAN filtering register */
+#define ENETC4_PBFDVFR		0x2d8
+
+/* Port low power mode register */
+#define ENETC4_PLPMR		0x340
+#define  PLPMR_WME		BIT(0)
+
+/* Port wake-on status register */
+#define ENETC4_PWOSR		0x344
+#define  PWOSR_WOLA		BIT(0)
+#define  PWOSR_ICMB		BIT(1)
+
+/* Port traffic class a time specific departure register */
+#define ENETC4_PTCTSDR(a)	((a) * 0x4 + 0x390)
+#define  PTCTSDR_TSDE		BIT(31)
+
+/* Ingress port capability register */
+#define ENETC4_IPCAPR		0x1000
+#define  IPCAPR_ISID		BIT(2)
+
+/* Ingress port filter table capability register */
+#define ENETC4_IPFTCAPR		0x1644
+#define  IPFTCAPR_NUM_WORDS	GENMASK(15, 0)
+
+/* Ingress port filter table memory operational register */
+#define ENETC4_IPFTMOR		0x1648
+#define  IPFTMOR_NUM_WORDS	GENMASK(15, 0)
+
+/* Rate policer index table capability register */
+#define ENETC4_RPITCAPR		0x1814
+#define  RPITCAPR_NUM_ENTRIES	GENMASK(13, 0)
+
+/* Ingress stream counter index table capability register */
+#define ENETC4_ISCICAPR		0x1824
+#define  ISCICAPR_NUM_ENTRIES	GENMASK(15, 0)
+
+/* Ingress stream index table capability register  */
+#define ENETC4_ISITCAPR		0x1834
+#define  ISITCAPR_NUM_ENTRIES	GENMASK(15, 0)
+
+/* Stream gate capability register */
+#define ENETC4_SGCAPR		0x1860
+
+/* Stream gate instance index table capability register */
+#define	ENETC4_SGIITCAPR	0x1864
+#define  SGITCAPR_NUM_ENTRIES	GENMASK(15, 0)
+
+/* Stream gate control list index table capability register */
+#define ENETC4_SGCLITCAPR	0x1874
+#define  SGCLITCAPR_NUM_WORDS	GENMASK(15, 0)
+
+/* Time gate scheduling table capability register */
+#define ENETC4_TGSTCAPR		0x18d4
+#define  TGSTCAPR_NUM_WORDS	GENMASK(15, 0)
+
+/* Time gate scheduling table memory operation register */
+#define ENETC4_TGSTMOR		0x18dc
+#define  TGSTMOR_NUM_WORDS	GENMASK(15, 0)
+
+/* Hash table memory capability register */
+#define ENETC4_HTMCAPR		0X1900
+#define  HTMCAPR_NUM_WORDS	GENMASK(15, 0)
+
+/* Ingress stream identification key construction a configuration register 0 */
+#define ENETC4_ISIDKC0CR0	0x1924
+#define ENETC4_ISIDKC1CR0	0x1944
+#define  ISIDKCCR0_VALID	BIT(0)
+#define  ISIDKCCR0_DMACP	BIT(3)
+#define  ISIDKCCR0_SMACP	BIT(4)
+#define  ISIDKCCR0_OVIDP	BIT(5)
+#define  ISIDKCCR0_OPCPP	BIT(6)
+
+/* Port Station interface a primary MAC address registers */
+#define ENETC4_PSIPMAR0(a)	((a) * 0x80 + 0x2000)
+#define ENETC4_PSIPMAR1(a)	((a) * 0x80 + 0x2004)
+
+/* Port station interface a VLAN register */
+#define ENETC4_PSIVLANR(a)	((a) * 0x80 + 0x2008)
+#define  PSIVLANR_VID		GENMASK(11, 0)
+#define  PSIVLANR_DEI		BIT(12)
+#define  PSIVLANR_PCP_OFF	13
+#define  PSIVLANR_PCP		GENMASK(15, PSIVLANR_PCP_OFF)
+#define  PSIVLANR_TPID		GENMASK(17, 16)
+#define  PSIVLANR_TXTAGR	GENMASK(23, 20)
+#define  PSIVLANR_VTEA		BIT(30)
+#define  PSIVLANR_E		BIT(31)
+
+/* Port station interface a configuration register 0/2 */
+#define ENETC4_PSICFGR0(a)	((a) * 0x80 + 0x2010)
+#define  PSICFGR0_VASE		BIT(13)
+#define  PSICFGR0_ASE		BIT(15)
+#define  PSICFGR0_ANTI_SPOOFING	(PSICFGR0_VASE | PSICFGR0_ASE)
+
+#define ENETC4_PSICFGR2(a)	((a) * 0x80 + 0x2018)
+
+/* Port station interface a unicast MAC hash filter register 0/1 */
+#define ENETC4_PSIUMHFR0(a)	((a) * 0x80 + 0x2050)
+#define ENETC4_PSIUMHFR1(a)	((a) * 0x80 + 0x2054)
+
+/* Port station interface a multicast MAC hash filter register 0/1 */
+#define ENETC4_PSIMMHFR0(a)	((a) * 0x80 + 0x2058)
+#define ENETC4_PSIMMHFR1(a)	((a) * 0x80 + 0x205c)
+
+/* Port station interface a VLAN hash filter register 0/1 */
+#define ENETC4_PSIVHFR0(a)	((a) * 0x80 + 0x2060)
+#define ENETC4_PSIVHFR1(a)	((a) * 0x80 + 0x2064)
+
+/* Define Ethernet MAC port resiters. Notice that the offset
+ * adds 0x4000 which compared to RM.
+ */
+#define ENETC4_PCAPR		0x4000
+#define  PCAPR_TGS		BIT(28)
+#define  PCAPR_CBS		BIT(29)
+#define  PCAPR_NUM_TC		GENMASK(15, 12)
+#define  PCAPR_LINK_TYPE	BIT(4)
+
+#define ENETC4_PMCAPR		0x4004
+#define  PMCAPR_HD		BIT(8)
+#define  PMCAPR_FP		GENMASK(10, 9)
+#define   PMCAPR_FP_SUPP	2
+#define   PMCAPR_GET_FP(val)	(((val) & PMCAPR_FP) >> 9)
+
+#define ENETC4_PIOCAPR		0x4008
+
+/* Port configuration register */
+#define ENETC4_PCR		0x4010
+#define  PCR_HDR_FMT		BIT(0)
+#define  PCR_L2DOSE		BIT(4)
+#define  PCR_TIMER_CS		BIT(8)
+#define  PCR_PSPEED		GENMASK(29, 16)
+#define  PCR_PSPEED_VAL(speed)	(((speed) / 10 - 1) << 16)
+
+/* Port MAC address register 0/1 */
+#define ENETC4_PMAR0		0x4020
+#define ENETC4_PMAR1		0x4024
+
+/* Port ingress port filter configuration register */
+#define ENETC4_PIPFCR		0x4084
+#define  PIPFCR_EN		BIT(0)
+
+/* Port operational register */
+#define ENETC4_POR		0x4100
+
+/* Port status register */
+#define ENETC4_PSR		0x4104
+
+/* Port time gate scheduling control register */
+#define ENETC4_PTGSCR		0x4110
+#define  PTGSCR_TGE		BIT(31)
+
+/* Port time gate scheduling admin gate list status register */
+#define ENETC4_PTGAGLSR		0x4114
+#define  PTGAGLSR_TG		BIT(0)
+#define  PTGAGLSR_CFG_PEND	BIT(1)
+
+/* Port time gate scheduling admin gate list length register */
+#define ENETC4_PTGAGLLR		0x4118
+#define  PTGAGLLR_LIST_LEN	GENMASK(15, 0)
+
+/* Port time gating operational gate list length register */
+#define ENETC4_PTGOGLLR		0x411c
+#define  PTGOGLLR_LIST_LEN	GENMASK(15, 0)
+
+/* Port frame preemption configuration register */
+#define ENETC4_PFPCR		0x4134
+#define  PFPCR_TC_PMAC_EN(a)	BIT(a)
+
+/* Port Rx discard count register */
+#define ENETC4_PRXDCR		0x41c0
+
+/* Port traffic class a transmit maximum SDU register */
+#define ENETC4_PTCTMSDUR(a)	((a) * 0x20 + 0x4208)
+#define  PTCTMSDUR_MAXSDU	GENMASK(15, 0)
+#define  PTCTMSDUR_SDU_TYPE	GENMASK(17, 16)
+#define   SDU_TYPE_PPDU		0
+#define   SDU_TYPE_MPDU		1
+#define   SDU_TYPE_MSDU		2
+
+/* Port transmit traffic class a credit based shaper register 0 */
+#define ENETC4_PTCCBSR0(a)	((a) * 0x20 + 0x4210)
+#define  PTCCBSR0_BW		GENMASK(6, 0)
+#define  PTCCBSR0_FRACT		GENMASK(19, 16)
+#define  PTCCBSR0_GET_FRACT(x)  (((x) & PTCCBSR0_FRACT) >> 16)
+#define  PTCCBSR0_CBSE		BIT(31)
+
+/* Port traffic class a credit based shaper register 1 */
+#define ENETC4_PTCCBSR1(a)	((a) * 0x20 + 0x4214)
+
+/* Port ingress stream identification configuration register */
+#define ENETC4_PISIDCR		0x4460
+#define  PISIDCR_KC0EN		BIT(1)
+#define  PISIDCR_KC1EN		BIT(2)
+
+#define ENETC4_PMAC_OFFSET		0x400
+#define ENETC4_PM_CMD_CFG(mac)		(0x5008 + (mac) * 0x400)
+#define  PM_CMD_CFG_TX_EN		BIT(0)
+#define  PM_CMD_CFG_RX_EN		BIT(1)
+#define  PM_CMD_CFG_PAUSE_FWD		BIT(7)
+#define  PM_CMD_CFG_PAUSE_IGN		BIT(8)
+#define  PM_CMD_CFG_TX_ADDR_INS		BIT(9)
+#define  PM_CMD_CFG_LOOP_EN		BIT(10)
+#define  PM_CMD_CFG_LPBK_MODE		GENMASK(12, 11)
+#define   LPBCK_MODE_EXT_TX_CLK		0
+#define   LPBCK_MODE_MAC_LEVEL		1
+#define   LPBCK_MODE_INT_TX_CLK		2
+#define  PM_CMD_CFG_CNT_FRM_EN		BIT(13)
+#define  PM_CMD_CFG_TXP			BIT(15)
+#define  PM_CMD_CFG_SEND_IDLE		BIT(16)
+#define  PM_CMD_CFG_HD_FCEN		BIT(18)
+#define  PM_CMD_CFG_SFD			BIT(21)
+#define  PM_CMD_CFG_TX_FLUSH		BIT(22)
+#define  PM_CMD_CFG_TX_LOWP_EN		BIT(23)
+#define  PM_CMD_CFG_RX_LOWP_EMPTY	BIT(24)
+#define  PM_CMD_CFG_SWR			BIT(26)
+#define  PM_CMD_CFG_TS_MODE		BIT(30)
+#define  PM_CMD_CFG_MG			BIT(31)
+
+/* Port MAC 0/1 Maximum Frame Length Register */
+#define ENETC4_PM_MAXFRM(mac)		(0x5014 + (mac) * 0x400)
+
+/* Port MAC 0/1 Pause Quanta Register */
+#define ENETC4_PM_PAUSE_QUANTA(mac)	(0x5054 + (mac) * 0x400)
+
+/* Port MAC 0/1 Pause Quanta Threshold Register */
+#define ENETC4_PM_PAUSE_THRESH(mac)	(0x5064 + (mac) * 0x400)
+
+#define ENETC4_PM_LPWAKE_TIMER(mac)	(0x50B8 + (mac) * 0x400)
+#define ENETC4_PM_SLEEP_TIMER(mac)	(0x50BC + (mac) * 0x400)
+#define  PM_EEE_TIMER			GENMASK(23, 0)
+
+#define ENETC4_PM_SINGLE_STEP(mac)	(0x50C0 + (mac) * 0x400)
+#define  PM_SINGLE_STEP_CH		BIT(6)
+#define  PM_SINGLE_STEP_OFFSET_MASK	GENMASK(15, 7)
+#define   PM_SINGLE_STEP_OFFSET(v)	(((v) << 7) & PM_SINGLE_STEP_OFFSET_MASK)
+#define  PM_SINGLE_STEP_EN		BIT(31)
+
+/* Port MAC 0/1 Receive Ethernet Octets Counter */
+#define ENETC4_PM_REOCT(mac)		(0x5100 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive Octets Counter */
+#define ENETC4_PM_ROCT(mac)		(0x5108 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive Alignment Error Counter Register */
+#define ENETC4_PM_RALN(mac)		(0x5110 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive Valid Pause Frame Counter */
+#define ENETC4_PM_RXPF(mac)		(0x5118 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive Frame Counter */
+#define ENETC4_PM_RFRM(mac)		(0x5120 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive Frame Check Sequence Error Counter */
+#define ENETC4_PM_RFCS(mac)		(0x5128 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive VLAN Frame Counter */
+#define ENETC4_PM_RVLAN(mac)		(0x5130 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive Frame Error Counter */
+#define ENETC4_PM_RERR(mac)		(0x5138 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive Unicast Frame Counter */
+#define ENETC4_PM_RUCA(mac)		(0x5140 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive Multicast Frame Counter */
+#define ENETC4_PM_RMCA(mac)		(0x5148 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive Broadcast Frame Counter */
+#define ENETC4_PM_RBCA(mac)		(0x5150 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive Dropped Packets Counter */
+#define ENETC4_PM_RDRP(mac)		(0x5158 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive Packets Counter */
+#define ENETC4_PM_RPKT(mac)		(0x5160 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive Undersized Packet Counter */
+#define ENETC4_PM_RUND(mac)		(0x5168 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive 64-Octet Packet Counter */
+#define ENETC4_PM_R64(mac)		(0x5170 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive 65 to 127-Octet Packet Counter */
+#define ENETC4_PM_R127(mac)		(0x5178 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive 128 to 255-Octet Packet Counter */
+#define ENETC4_PM_R255(mac)		(0x5180 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive 256 to 511-Octet Packet Counter */
+#define ENETC4_PM_R511(mac)		(0x5188 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive 512 to 1023-Octet Packet Counter */
+#define ENETC4_PM_R1023(mac)		(0x5190 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive 1024 to 1522-Octet Packet Counter */
+#define ENETC4_PM_R1522(mac)		(0x5198 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive 1523 to Max-Octet Packet Counter */
+#define ENETC4_PM_R1523X(mac)		(0x51a0 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive Oversized Packet Counter */
+#define ENETC4_PM_ROVR(mac)		(0x51a8 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive Jabber Packet Counter */
+#define ENETC4_PM_RJBR(mac)		(0x51b0 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive Fragment Packet Counter */
+#define ENETC4_PM_RFRG(mac)		(0x51b8 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive Control Packet Counter */
+#define ENETC4_PM_RCNP(mac)		(0x51c0 + (mac) * 0x400)
+
+/* Port MAC 0/1 Receive Dropped Not Truncated Packets Counter */
+#define ENETC4_PM_RDRNTP(mac)		(0x51c8 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit Ethernet Octets Counter */
+#define ENETC4_PM_TEOCT(mac)		(0x5200 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit Octets Counter */
+#define ENETC4_PM_TOCT(mac)		(0x5208 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit Valid Pause Frame Counter */
+#define ENETC4_PM_TXPF(mac)		(0x5218 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit Frame Counter */
+#define ENETC4_PM_TFRM(mac)		(0x5220 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit Frame Check Sequence Error Counter */
+#define ENETC4_PM_TFCS(mac)		(0x5228 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit VLAN Frame Counter */
+#define ENETC4_PM_TVLAN(mac)		(0x5230 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit Frame Error Counter */
+#define ENETC4_PM_TERR(mac)		(0x5238 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit Unicast Frame Counter */
+#define ENETC4_PM_TUCA(mac)		(0x5240 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit Multicast Frame Counter */
+#define ENETC4_PM_TMCA(mac)		(0x5248 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit Broadcast Frame Counter */
+#define ENETC4_PM_TBCA(mac)		(0x5250 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit Packets Counter */
+#define ENETC4_PM_TPKT(mac)		(0x5260 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit Undersized Packet Counter */
+#define ENETC4_PM_TUND(mac)		(0x5268 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit 64-Octet Packet Counter */
+#define ENETC4_PM_T64(mac)		(0x5270 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit 65 to 127-Octet Packet Counter */
+#define ENETC4_PM_T127(mac)		(0x5278 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit 128 to 255-Octet Packet Counter */
+#define ENETC4_PM_T255(mac)		(0x5280 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit 256 to 511-Octet Packet Counter */
+#define ENETC4_PM_T511(mac)		(0x5288 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit 512 to 1023-Octet Packet Counter */
+#define ENETC4_PM_T1023(mac)		(0x5290 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit 1024 to 1522-Octet Packet Counter */
+#define ENETC4_PM_T1522(mac)		(0x5298 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit 1523 to TX_MTU-Octet Packet Counter */
+#define ENETC4_PM_T1523X(mac)		(0x52a0 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit Control Packet Counter */
+#define ENETC4_PM_TCNP(mac)		(0x52c0 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit Deferred Packet Counter */
+#define ENETC4_PM_TDFR(mac)		(0x52d0 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit Multiple Collisions Counter */
+#define ENETC4_PM_TMCOL(mac)		(0x52d8 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit Single Collision */
+#define ENETC4_PM_TSCOL(mac)		(0x52e0 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit Late Collision Counter */
+#define ENETC4_PM_TLCOL(mac)		(0x52e8 + (mac) * 0x400)
+
+/* Port MAC 0/1 Transmit Excessive Collisions Counter */
+#define ENETC4_PM_TECOL(mac)		(0x52f0 + (mac) * 0x400)
+
+/* Port MAC 0 Interface Mode Control Register */
+#define ENETC4_PM_IF_MODE(mac)		(0x5300 + (mac) * 0x400)
+#define  PM_IF_MODE_IFMODE		GENMASK(2, 0)
+#define   IFMODE_XGMII			0
+#define   IFMODE_RMII			3
+#define   IFMODE_RGMII			4
+#define   IFMODE_SGMII			5
+#define  PM_IF_MODE_REVMII		BIT(3)
+#define  PM_IF_MODE_M10			BIT(4)
+#define  PM_IF_MODE_HD			BIT(6)
+#define  PM_IF_MODE_SSP			GENMASK(14, 13)
+#define   SSP_100M			0
+#define   SSP_10M			1
+#define   SSP_1G			2
+#define  PM_IF_MODE_ENA			BIT(15)
+
+/* Port MAC Merge Control and Status Register */
+#define ENETC4_MMCSR			0x5800
+#define  MMCSR_RAFS			GENMASK(9, 8)
+#define  MMCSR_ME			GENMASK(16, 15)
+#define   MMCSR_ME_DISABLE		0
+#define   MMCSR_ME_ANY_BOUNDARY		1
+#define   MMCSR_ME_4B_BOUNDARY		2
+#define  MMCSR_VDIS			BIT(17)
+#define  MMCSR_VSTS			GENMASK(20, 18)
+#define   MMCSR_VSTS_DISABLED		0
+#define   MMCSR_VSTS_IN_PROGRESS	2
+#define   MMCSR_VSTS_SUCCESSFUL		3
+#define   MMCSR_VSTS_FAILED		4
+#define   MMCSR_GET_VSTS(x)		(((x) & MMCSR_VSTS) >> 18)
+#define  MMCSR_VT			GENMASK(29, 23)
+#define   MMCSR_GET_VT(x)		(((x) & MMCSR_VT) >> 23)
+#define  MMCSR_LINK_FAIL		BIT(31)
+
+/* Port MAC Merge Control and Status Register */
+#define ENETC4_MMFAECR			0x5808
+
+/* Port MAC Merge Frame SMD Error Count Register */
+#define ENETC4_MMFSECR			0x580c
+
+/* Port MAC Merge Frame Assembly OK Count Register */
+#define ENETC4_MMFAOCR			0x5810
+
+/* Port MAC Merge Fragment Count RX Register */
+#define ENETC4_MMFCRXR			0x5814
+
+/* Port MAC Merge Fragment Count TX Register */
+#define ENETC4_MMFCTXR			0x5818
+
+/* Port MAC Merge Hold Count Register */
+#define ENETC4_MMHCR			0x581c
+
+/* Port internal MDIO base address, use to access PCS */
+#define ENETC4_PM_IMDIO_BASE		0x5030
+
+/* Port external MDIO Base address, use to access off-chip PHY */
+#define ENETC4_EMDIO_BASE		0x5c00
+
+/**********************ENETC Pseudo MAC port registers************************/
+/* Port pseudo MAC receive octets counter (64-bit) */
+#define ENETC4_PPMROCR			0x5080
+
+/* Port pseudo MAC receive unicast frame counter register (64-bit) */
+#define ENETC4_PPMRUFCR			0x5088
+
+/* Port pseudo MAC receive multicast frame counter register (64-bit) */
+#define ENETC4_PPMRMFCR			0x5090
+
+/* Port pseudo MAC receive broadcast frame counter register (64-bit) */
+#define ENETC4_PPMRBFCR			0x5098
+
+/* Port pseudo MAC transmit octets counter (64-bit) */
+#define ENETC4_PPMTOCR			0x50c0
+
+/* Port pseudo MAC transmit unicast frame counter register (64-bit) */
+#define ENETC4_PPMTUFCR			0x50c8
+
+/* Port pseudo MAC transmit multicast frame counter register (64-bit) */
+#define ENETC4_PPMTMFCR			0x50d0
+
+/* Port pseudo MAC transmit broadcast frame counter register (64-bit) */
+#define ENETC4_PPMTBFCR			0x50d8
diff --git a/devices/enetc/enetc4_pf.c b/devices/enetc/enetc4_pf.c
new file mode 100755
index 0000000..fb87d13
--- /dev/null
+++ b/devices/enetc/enetc4_pf.c
@@ -0,0 +1,1788 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+/* Copyright 2023 NXP */
+#include <linux/module.h>
+#include <linux/of_mdio.h>
+#include <linux/of_net.h>
+#include <linux/of_platform.h>
+#include <linux/clk.h>
+#include <linux/fsl/enetc_mdio.h>
+#include <linux/pinctrl/consumer.h>
+#include <linux/regulator/consumer.h>
+#include <linux/unaligned.h>
+#include <linux/fsl/netc_global.h>
+
+#include "enetc_pf.h"
+#include "../ecdev.h"
+
+#define ENETC_SI_MAX_RING_NUM	8
+
+static void enetc4_get_port_caps(struct enetc_pf *pf)
+{
+	struct enetc_hw *hw = &pf->si->hw;
+	u32 val;
+
+	val = enetc_port_rd(hw, ENETC4_ECAPR0);
+	pf->caps.wol = (val & ECAPR0_WO) ? 1 : 0;
+
+	val = enetc_port_rd(hw, ENETC4_ECAPR1);
+	pf->caps.num_vsi = (val & ECAPR1_NUM_VSI) >> 24;
+	pf->caps.num_msix = ((val & ECAPR1_NUM_MSIX) >> 12) + 1;
+
+	val = enetc_port_rd(hw, ENETC4_ECAPR2);
+	pf->caps.num_rx_bdr = (val & ECAPR2_NUM_RX_BDR) >> 16;
+	pf->caps.num_tx_bdr = val & ECAPR2_NUM_TX_BDR;
+
+	val = enetc_port_rd(hw, ENETC4_PMCAPR);
+	pf->caps.half_duplex = (val & PMCAPR_HD) ? 1 : 0;
+
+	val = enetc_port_rd(hw, ENETC4_PSIMAFCAPR);
+	pf->caps.mac_filter_num = val & PSIMAFCAPR_NUM_MAC_AFTE;
+
+	val = enetc_port_rd(hw, ENETC4_PSIVLANFCAPR);
+	pf->caps.vlan_filter_num = val & PSIVLANFCAPR_NUM_VLAN_FTE;
+
+	val = enetc_port_rd(hw, ENETC4_IPFTCAPR);
+	pf->caps.ipf_words_num = val & IPFTCAPR_NUM_WORDS;
+}
+
+static void enetc4_pf_set_tc_msdu(struct enetc_hw *hw, u32 *max_sdu)
+{
+	int tc;
+
+	for (tc = 0; tc < 8; tc++) {
+		u32 val = ENETC4_MAC_MAXFRM_SIZE;
+
+		if (max_sdu[tc])
+			val = max_sdu[tc] + VLAN_ETH_HLEN;
+
+		val = u32_replace_bits(val, SDU_TYPE_MPDU, PTCTMSDUR_SDU_TYPE);
+		enetc_port_wr(hw, ENETC4_PTCTMSDUR(tc), val);
+	}
+}
+
+static void enetc4_pf_reset_tc_msdu(struct enetc_hw *hw)
+{
+	u32 val = ENETC4_MAC_MAXFRM_SIZE;
+	int tc;
+
+	val = u32_replace_bits(val, SDU_TYPE_MPDU, PTCTMSDUR_SDU_TYPE);
+
+	for (tc = 0; tc < 8; tc++)
+		enetc_port_wr(hw, ENETC4_PTCTMSDUR(tc), val);
+}
+
+static void enetc4_set_trx_frame_size(struct enetc_pf *pf)
+{
+	struct enetc_si *si = pf->si;
+
+	ec_enetc_port_mac_wr(si, ENETC4_PM_MAXFRM(0),
+			  ENETC_SET_MAXFRM(ENETC4_MAC_MAXFRM_SIZE));
+
+	enetc4_pf_reset_tc_msdu(&si->hw);
+}
+
+/* Allocate the number of MSI-X vectors for per SI. */
+static void enetc4_set_si_msix_num(struct enetc_pf *pf)
+{
+	struct enetc_hw *hw = &pf->si->hw;
+	int i, num_msix, total_si;
+	u32 val;
+
+	total_si = pf->caps.num_vsi + 1;
+
+	num_msix = pf->caps.num_msix / total_si +
+		   pf->caps.num_msix % total_si - 1;
+	val = num_msix & 0x3f;
+	enetc_port_wr(hw, ENETC4_PSICFGR2(0), val);
+
+	num_msix = pf->caps.num_msix / total_si - 1;
+	val = num_msix & 0x3f;
+	for (i = 0; i < pf->caps.num_vsi; i++)
+		enetc_port_wr(hw, ENETC4_PSICFGR2(i + 1), val);
+}
+
+static u32 enetc4_psicfgr0_val_construct(bool is_vf, u32 num_tx_bdr, u32 num_rx_bdr)
+{
+	u32 val;
+
+	val = ENETC_PSICFGR0_SET_TXBDR(num_tx_bdr);
+	val |= ENETC_PSICFGR0_SET_RXBDR(num_rx_bdr);
+	val |= ENETC_PSICFGR0_SIVC(ENETC_VLAN_TYPE_C | ENETC_VLAN_TYPE_S);
+
+	if (is_vf)
+		val |= ENETC_PSICFGR0_VTE | ENETC_PSICFGR0_SIVIE;
+
+	return val;
+}
+
+static void enetc4_devlink_allocate_rings(struct enetc_pf *pf)
+{
+	struct enetc_devlink_priv *devl_priv = pf->devl_priv;
+	u32 num_si =  pf->caps.num_vsi + 1;
+	struct enetc_hw *hw = &pf->si->hw;
+	u32 num_rings, val;
+	int i;
+
+	for (i = 0; i < num_si && i < ENETC_MAX_SI_NUM; i++) {
+		num_rings = devl_priv->si_num_rings[i];
+		val = enetc4_psicfgr0_val_construct(i > 0, num_rings, num_rings);
+		enetc_port_wr(hw, ENETC4_PSICFGR0(i), val);
+	}
+}
+
+static void enetc4_default_rings_allocation(struct enetc_pf *pf)
+{
+	struct enetc_hw *hw = &pf->si->hw;
+	u32 num_rx_bdr, num_tx_bdr, val;
+	u32 vf_tx_bdr, vf_rx_bdr;
+	int i, rx_rem, tx_rem;
+
+	if (pf->caps.num_rx_bdr < ENETC_SI_MAX_RING_NUM + pf->caps.num_vsi)
+		num_rx_bdr = pf->caps.num_rx_bdr - pf->caps.num_vsi;
+	else
+		num_rx_bdr = ENETC_SI_MAX_RING_NUM;
+
+	if (pf->caps.num_tx_bdr < ENETC_SI_MAX_RING_NUM + pf->caps.num_vsi)
+		num_tx_bdr = pf->caps.num_tx_bdr - pf->caps.num_vsi;
+	else
+		num_tx_bdr = ENETC_SI_MAX_RING_NUM;
+
+	val = enetc4_psicfgr0_val_construct(false, num_tx_bdr, num_rx_bdr);
+	enetc_port_wr(hw, ENETC4_PSICFGR0(0), val);
+
+	num_rx_bdr = pf->caps.num_rx_bdr - num_rx_bdr;
+	rx_rem = num_rx_bdr % pf->caps.num_vsi;
+	num_rx_bdr = num_rx_bdr / pf->caps.num_vsi;
+
+	num_tx_bdr = pf->caps.num_tx_bdr - num_tx_bdr;
+	tx_rem = num_tx_bdr % pf->caps.num_vsi;
+	num_tx_bdr = num_tx_bdr / pf->caps.num_vsi;
+
+	for (i = 0; i < pf->caps.num_vsi; i++) {
+		vf_tx_bdr = (i < tx_rem) ? num_tx_bdr + 1 : num_tx_bdr;
+		vf_rx_bdr = (i < rx_rem) ? num_rx_bdr + 1 : num_rx_bdr;
+		val = enetc4_psicfgr0_val_construct(true, vf_tx_bdr, vf_rx_bdr);
+		enetc_port_wr(hw, ENETC4_PSICFGR0(i + 1), val);
+	}
+}
+
+static void enetc4_allocate_si_rings(struct enetc_pf *pf)
+{
+	if (!pf->devl_priv->si_num_rings[0]) {
+		enetc4_default_rings_allocation(pf);
+	} else {
+		enetc4_devlink_allocate_rings(pf);
+	}
+}
+
+static void enetc4_pf_set_si_vlan_promisc(struct enetc_hw *hw, int si, bool en)
+{
+	u32 val = enetc_port_rd(hw, ENETC4_PSIPVMR);
+
+	if (en)
+		val |= BIT(si);
+	else
+		val &= ~BIT(si);
+
+	enetc_port_wr(hw, ENETC4_PSIPVMR, val);
+}
+
+static void enetc4_set_default_si_vlan_promisc(struct enetc_pf *pf)
+{
+	struct enetc_hw *hw = &pf->si->hw;
+	int num_si = pf->caps.num_vsi + 1;
+	int i;
+
+	/* enforce VLAN promiscuous mode for all SIs */
+	for (i = 0; i < num_si; i++)
+		enetc4_pf_set_si_vlan_promisc(hw, i, true);
+}
+
+static void enetc4_port_si_configure(struct enetc_pf *pf)
+{
+	struct enetc_hw *hw = &pf->si->hw;
+
+	enetc4_allocate_si_rings(pf);
+
+	/* Outer VLAN tag will be used for VLAN filtering */
+	enetc_port_wr(hw, ENETC4_PSIVLANFMR, PSIVLANFMR_VS);
+
+	/* enforce VLAN promisc mode for all SIs */
+	enetc4_set_default_si_vlan_promisc(pf);
+
+	/* Disable SI MAC multicast & unicast promiscuous */
+	enetc_port_wr(hw, ENETC4_PSIPMMR, 0);
+
+	enetc4_set_si_msix_num(pf);
+}
+
+static void enetc4_set_default_rss_key(struct enetc_hw *hw)
+{
+	u8 hash_key[ENETC_RSSHASH_KEY_SIZE];
+
+	/* set up hash key */
+	get_random_bytes(hash_key, ENETC_RSSHASH_KEY_SIZE);
+	ec_enetc_set_rss_key(hw, hash_key);
+}
+
+static void enetc4_set_isit_key_construct_rule(struct enetc_hw *hw)
+{
+	u32 val;
+
+	/* Key construction rule 0: SMAC + VID */
+	val = ISIDKCCR0_VALID | ISIDKCCR0_SMACP | ISIDKCCR0_OVIDP;
+	enetc_port_wr(hw, ENETC4_ISIDKC0CR0, val);
+
+	/* Key construction rule 1: DMAC + VID */
+	val = ISIDKCCR0_VALID | ISIDKCCR0_DMACP | ISIDKCCR0_OVIDP;
+	enetc_port_wr(hw, ENETC4_ISIDKC1CR0, val);
+
+	/* Enable key construction rule 0 and 1 */
+	val = enetc_port_rd(hw, ENETC4_PISIDCR);
+	val |= PISIDCR_KC0EN | PISIDCR_KC1EN;
+	enetc_port_wr(hw, ENETC4_PISIDCR, val);
+}
+
+static void enetc4_enable_all_si(struct enetc_pf *pf)
+{
+	struct enetc_hw *hw = &pf->si->hw;
+	int num_si = pf->caps.num_vsi + 1;
+	u32 si_bitmap = 0;
+	int i;
+
+	/* Master enable for all SIs */
+	for (i = 0; i < num_si; i++)
+		si_bitmap |= PMR_SI_EN(i);
+
+	enetc_port_wr(hw, ENETC4_PMR, si_bitmap);
+}
+
+static void enetc4_configure_port(struct enetc_pf *pf)
+{
+	struct enetc_hw *hw = &pf->si->hw;
+
+	enetc4_port_si_configure(pf);
+
+	enetc4_set_trx_frame_size(pf);
+
+	enetc4_set_default_rss_key(hw);
+
+	enetc4_set_isit_key_construct_rule(hw);
+
+	/* Master enable for all SIs */
+	enetc4_enable_all_si(pf);
+
+	/* Enable port transmit/receive */
+	enetc_port_wr(hw, ENETC4_POR, 0);
+}
+
+static int enetc4_pf_set_mac_exact_filter(struct enetc_pf *pf, int type)
+{
+	struct enetc_mac_entry *mac_tbl __free(kfree);
+	int mf_max_num = pf->caps.mac_filter_num;
+	struct net_device *ndev = pf->si->ndev;
+	struct netdev_hw_addr *ha;
+	u8 si_mac[ETH_ALEN];
+	int mac_cnt = 0;
+
+	mac_tbl = kcalloc(mf_max_num, sizeof(*mac_tbl), GFP_KERNEL);
+	if (!mac_tbl)
+		return -ENOMEM;
+
+	enetc_get_si_primary_mac(&pf->si->hw, si_mac);
+
+	netif_addr_lock_bh(ndev);
+	if (type & ENETC_MAC_FILTER_TYPE_UC) {
+		netdev_for_each_uc_addr(ha, ndev) {
+			if (!is_valid_ether_addr(ha->addr) ||
+			    ether_addr_equal(ha->addr, si_mac))
+				continue;
+
+			if (mac_cnt >= mf_max_num)
+				goto err_nospace_out;
+
+			ether_addr_copy(mac_tbl[mac_cnt++].addr, ha->addr);
+		}
+	}
+
+	if (type & ENETC_MAC_FILTER_TYPE_MC) {
+		netdev_for_each_mc_addr(ha, ndev) {
+			if (!is_multicast_ether_addr(ha->addr))
+				continue;
+
+			if (mac_cnt >= mf_max_num)
+				goto err_nospace_out;
+
+			ether_addr_copy(mac_tbl[mac_cnt++].addr, ha->addr);
+		}
+	}
+	netif_addr_unlock_bh(ndev);
+
+	return enetc_pf_set_mac_exact_filter(pf, 0, mac_tbl, mac_cnt);
+
+err_nospace_out:
+	netif_addr_unlock_bh(ndev);
+
+	return -ENOSPC;
+}
+
+static void enetc4_pf_set_mac_hash_filter(struct enetc_pf *pf, int type)
+{
+	struct net_device *ndev = pf->si->ndev;
+	struct enetc_mac_filter *mac_filter;
+	struct enetc_hw *hw = &pf->si->hw;
+	struct enetc_si *si = pf->si;
+	struct netdev_hw_addr *ha;
+
+	netif_addr_lock_bh(ndev);
+	if (type & ENETC_MAC_FILTER_TYPE_UC) {
+		mac_filter = &si->mac_filter[UC];
+		ec_enetc_reset_mac_addr_filter(mac_filter);
+		netdev_for_each_uc_addr(ha, ndev)
+			ec_enetc_add_mac_addr_ht_filter(mac_filter, ha->addr);
+
+		pf->hw_ops->set_si_mac_hash_filter(hw, 0, UC,
+						   *mac_filter->mac_hash_table);
+	}
+
+	if (type & ENETC_MAC_FILTER_TYPE_MC) {
+		mac_filter = &si->mac_filter[MC];
+		ec_enetc_reset_mac_addr_filter(mac_filter);
+		netdev_for_each_mc_addr(ha, ndev)
+			ec_enetc_add_mac_addr_ht_filter(mac_filter, ha->addr);
+
+		pf->hw_ops->set_si_mac_hash_filter(hw, 0, MC,
+						   *mac_filter->mac_hash_table);
+	}
+	netif_addr_unlock_bh(ndev);
+}
+
+static void enetc4_pf_set_mac_filter(struct enetc_pf *pf, int type)
+{
+	if (!(type & ENETC_MAC_FILTER_TYPE_ALL))
+		return;
+
+	if (enetc4_pf_set_mac_exact_filter(pf, type))
+		/* Fallback to use MAC hash filter */
+		enetc4_pf_set_mac_hash_filter(pf, type);
+}
+
+static void enetc4_pf_do_set_rx_mode(struct work_struct *work)
+{
+	struct enetc_si *si = container_of(work, struct enetc_si,
+					   rx_mode_task);
+	struct enetc_ndev_priv *priv = netdev_priv(si->ndev);
+	struct enetc_pf *pf = enetc_si_priv(priv->si);
+	struct net_device *ndev = si->ndev;
+	struct enetc_hw *hw = &si->hw;
+	bool uc_promisc = false;
+	bool mc_promisc = false;
+	int type = 0;
+
+	if (!pf->hw_ops->set_si_mac_hash_filter ||
+	    !pf->hw_ops->set_si_mac_promisc)
+		return;
+
+	if (ndev->flags & IFF_PROMISC) {
+		uc_promisc = true;
+		mc_promisc = true;
+	} else if (ndev->flags & IFF_ALLMULTI) {
+		mc_promisc = true;
+		type = ENETC_MAC_FILTER_TYPE_UC;
+	} else {
+		type = ENETC_MAC_FILTER_TYPE_ALL;
+	}
+
+	pf->hw_ops->set_si_mac_promisc(hw, 0, UC, uc_promisc);
+	pf->hw_ops->set_si_mac_promisc(hw, 0, MC, mc_promisc);
+
+	/* Clear MAC filter */
+	enetc_pf_flush_mac_exact_filter(pf, 0, ENETC_MAC_FILTER_TYPE_ALL);
+	pf->hw_ops->set_si_mac_hash_filter(hw, 0, UC, 0);
+	pf->hw_ops->set_si_mac_hash_filter(hw, 0, MC, 0);
+
+	enetc4_pf_set_mac_filter(pf, type);
+}
+
+static void enetc4_pf_set_rx_mode(struct net_device *ndev)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	struct enetc_si *si = priv->si;
+
+	queue_work(si->workqueue, &si->rx_mode_task);
+}
+
+static const struct net_device_ops enetc4_ndev_ops = {
+	.ndo_open		= ec_enetc_open,
+	.ndo_stop		= ec_enetc_close,
+	.ndo_start_xmit		= ec_enetc_xmit,
+	.ndo_vlan_rx_add_vid	= ec_enetc_vlan_rx_add_vid,
+	.ndo_vlan_rx_kill_vid	= ec_enetc_vlan_rx_del_vid,
+};
+
+static void enetc4_mac_config(struct enetc_pf *pf, unsigned int mode,
+			      phy_interface_t phy_mode)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(pf->si->ndev);
+	struct enetc_si *si = pf->si;
+	u32 val;
+
+	if (si->hw_features & ENETC_SI_F_PPM)
+		return;
+
+	val = ec_enetc_port_mac_rd(si, ENETC4_PM_IF_MODE(0));
+	val &= ~(PM_IF_MODE_IFMODE | PM_IF_MODE_ENA);
+
+	switch (phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_RGMII_ID:
+	case PHY_INTERFACE_MODE_RGMII_RXID:
+	case PHY_INTERFACE_MODE_RGMII_TXID:
+		val |= IFMODE_RGMII;
+		/* We need to enable auto-negotiation for the MAC
+		 * if its RGMII interface support In-Band status.
+		 */
+		if (phylink_autoneg_inband(mode))
+			val |= PM_IF_MODE_ENA;
+		break;
+	case PHY_INTERFACE_MODE_RMII:
+		val |= IFMODE_RMII;
+		break;
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_2500BASEX:
+		val |= IFMODE_SGMII;
+		break;
+	case PHY_INTERFACE_MODE_10GBASER:
+	case PHY_INTERFACE_MODE_XGMII:
+	case PHY_INTERFACE_MODE_USXGMII:
+		val |= IFMODE_XGMII;
+		break;
+	default:
+		dev_err(priv->dev,
+			"Unsupported PHY mode:%d\n", phy_mode);
+		return;
+	}
+
+	ec_enetc_port_mac_wr(si, ENETC4_PM_IF_MODE(0), val);
+}
+
+static struct phylink_pcs *
+enetc4_pl_mac_select_pcs(struct phylink_config *config, phy_interface_t iface)
+{
+	struct enetc_pf *pf = phylink_to_enetc_pf(config);
+
+	return pf->pcs;
+}
+
+static void enetc4_pl_mac_config(struct phylink_config *config,
+				 unsigned int mode,
+				 const struct phylink_link_state *state)
+{
+	struct enetc_pf *pf = phylink_to_enetc_pf(config);
+
+	enetc4_mac_config(pf, mode, state->interface);
+}
+
+static void enetc4_set_port_speed(struct enetc_ndev_priv *priv, int speed)
+{
+	u32 old_speed = priv->speed;
+	u32 val;
+
+	if (speed == old_speed)
+		return;
+
+	val = enetc_port_rd(&priv->si->hw, ENETC4_PCR);
+	val &= ~PCR_PSPEED;
+
+	switch (speed) {
+	case SPEED_10:
+	case SPEED_100:
+	case SPEED_1000:
+	case SPEED_2500:
+	case SPEED_10000:
+		val |= (PCR_PSPEED & PCR_PSPEED_VAL(speed));
+		break;
+	default:
+		val |= (PCR_PSPEED & PCR_PSPEED_VAL(SPEED_10));
+	}
+
+	priv->speed = speed;
+	enetc_port_wr(&priv->si->hw, ENETC4_PCR, val);
+}
+
+static void enetc4_set_rgmii_mac(struct enetc_pf *pf, int speed, int duplex)
+{
+	struct enetc_si *si = pf->si;
+	u32 old_val, val;
+
+	old_val = ec_enetc_port_mac_rd(si, ENETC4_PM_IF_MODE(0));
+	val = old_val & ~(PM_IF_MODE_ENA | PM_IF_MODE_M10 | PM_IF_MODE_REVMII);
+
+	switch (speed) {
+	case SPEED_1000:
+		val = u32_replace_bits(val, SSP_1G, PM_IF_MODE_SSP);
+		break;
+	case SPEED_100:
+		val = u32_replace_bits(val, SSP_100M, PM_IF_MODE_SSP);
+		break;
+	case SPEED_10:
+		val = u32_replace_bits(val, SSP_10M, PM_IF_MODE_SSP);
+	}
+
+	val = u32_replace_bits(val, duplex == DUPLEX_FULL ? 0 : 1,
+			       PM_IF_MODE_HD);
+
+	if (val == old_val)
+		return;
+
+	ec_enetc_port_mac_wr(si, ENETC4_PM_IF_MODE(0), val);
+}
+
+static void enetc4_set_rmii_mac(struct enetc_pf *pf, int speed, int duplex)
+{
+	struct enetc_si *si = pf->si;
+	u32 old_val, val;
+
+	old_val = ec_enetc_port_mac_rd(si, ENETC4_PM_IF_MODE(0));
+	val = old_val & ~(PM_IF_MODE_ENA | PM_IF_MODE_SSP);
+
+	switch (speed) {
+	case SPEED_100:
+		val &= ~PM_IF_MODE_M10;
+		break;
+	case SPEED_10:
+		val |= PM_IF_MODE_M10;
+	}
+
+	val = u32_replace_bits(val, duplex == DUPLEX_FULL ? 0 : 1,
+			       PM_IF_MODE_HD);
+
+	if (val == old_val)
+		return;
+
+	ec_enetc_port_mac_wr(si, ENETC4_PM_IF_MODE(0), val);
+}
+
+static void enetc4_set_hd_flow_control(struct enetc_pf *pf, bool enable)
+{
+	struct enetc_si *si = pf->si;
+	u32 old_val, val;
+
+	if (!pf->caps.half_duplex)
+		return;
+
+	old_val = ec_enetc_port_mac_rd(si, ENETC4_PM_CMD_CFG(0));
+	val = u32_replace_bits(old_val, enable ? 1 : 0, PM_CMD_CFG_HD_FCEN);
+	if (val == old_val)
+		return;
+
+	ec_enetc_port_mac_wr(si, ENETC4_PM_CMD_CFG(0), val);
+}
+
+static void enetc4_set_rx_pause(struct enetc_pf *pf, bool rx_pause)
+{
+	struct enetc_si *si = pf->si;
+	u32 old_val, val;
+
+	old_val = ec_enetc_port_mac_rd(si, ENETC4_PM_CMD_CFG(0));
+	val = u32_replace_bits(old_val, rx_pause ? 0 : 1, PM_CMD_CFG_PAUSE_IGN);
+	if (val == old_val)
+		return;
+
+	ec_enetc_port_mac_wr(si, ENETC4_PM_CMD_CFG(0), val);
+}
+
+static void enetc4_set_tx_pause(struct enetc_pf *pf, int num_rxbdr, bool tx_pause)
+{
+	u32 pause_off_thresh = 0, pause_on_thresh = 0;
+	u32 init_quanta = 0, refresh_quanta = 0;
+	struct enetc_hw *hw = &pf->si->hw;
+	u32 rbmr, old_rbmr;
+	int i;
+
+	for (i = 0; i < num_rxbdr; i++) {
+		old_rbmr = enetc_rxbdr_rd(hw, i, ENETC_RBMR);
+		rbmr = u32_replace_bits(old_rbmr, tx_pause ? 1 : 0, ENETC_RBMR_CM);
+		if (rbmr == old_rbmr)
+			continue;
+
+		enetc_rxbdr_wr(hw, i, ENETC_RBMR, rbmr);
+	}
+
+	if (tx_pause) {
+		/* When the port first enters congestion, send a PAUSE request
+		 * with the maximum number of quanta. When the port exits
+		 * congestion, it will automatically send a PAUSE frame with
+		 * zero quanta.
+		 */
+		init_quanta = 0xffff;
+
+		/* Also, set up the refresh timer to send follow-up PAUSE
+		 * frames at half the quanta value, in case the congestion
+		 * condition persists.
+		 */
+		refresh_quanta = 0xffff / 2;
+
+		/* Start emitting PAUSE frames when 3 large frames (or more
+		 * smaller frames) have accumulated in the FIFO waiting to be
+		 * DMAed to the RX ring.
+		 */
+		pause_on_thresh = 3 * ENETC4_MAC_MAXFRM_SIZE;
+		pause_off_thresh = 1 * ENETC4_MAC_MAXFRM_SIZE;
+	}
+
+	ec_enetc_port_mac_wr(pf->si, ENETC4_PM_PAUSE_QUANTA(0), init_quanta);
+	ec_enetc_port_mac_wr(pf->si, ENETC4_PM_PAUSE_THRESH(0), refresh_quanta);
+	enetc_port_wr(hw, ENETC4_PPAUONTR, pause_on_thresh);
+	enetc_port_wr(hw, ENETC4_PPAUOFFTR, pause_off_thresh);
+}
+
+static void enetc4_enable_mac(struct enetc_pf *pf, bool en)
+{
+	struct enetc_si *si = pf->si;
+	u32 val;
+
+	val = ec_enetc_port_mac_rd(si, ENETC4_PM_CMD_CFG(0));
+	val &= ~(PM_CMD_CFG_TX_EN | PM_CMD_CFG_RX_EN);
+	val |= en ? (PM_CMD_CFG_TX_EN | PM_CMD_CFG_RX_EN) : 0;
+
+	ec_enetc_port_mac_wr(si, ENETC4_PM_CMD_CFG(0), val);
+}
+
+static void enetc4_pf_send_link_status_msg(struct enetc_pf *pf, bool up)
+{
+	struct device *dev = &pf->si->pdev->dev;
+	union enetc_pf_msg pf_msg;
+	u16 ms_mask = 0;
+	int i, err;
+
+	for (i = 0; i < pf->num_vfs; i++)
+		if (pf->vf_link_status_notify[i])
+			ms_mask |= PSIMSGSR_MS(i);
+
+	if (!ms_mask)
+		return;
+
+	pf_msg.class_id = ENETC_MSG_CLASS_ID_LINK_STATUS;
+	pf_msg.class_code = up ? ENETC_PF_NC_LINK_STATUS_UP :
+				 ENETC_PF_NC_LINK_STATUS_DOWN;
+
+	err = enetc_pf_send_msg(pf, pf_msg.code, ms_mask);
+	if (err)
+		dev_err(dev, "PF notifies link status failed\n");
+}
+
+static void enetc4_pl_mac_link_up(struct phylink_config *config,
+				  struct phy_device *phy, unsigned int mode,
+				  phy_interface_t interface, int speed,
+				  int duplex, bool tx_pause, bool rx_pause)
+{
+	struct enetc_pf *pf = phylink_to_enetc_pf(config);
+	struct enetc_si *si = pf->si;
+	struct enetc_ndev_priv *priv;
+	bool hd_fc = false;
+
+	priv = netdev_priv(si->ndev);
+	enetc4_set_port_speed(priv, speed);
+
+	if (!phylink_autoneg_inband(mode) &&
+	    phy_interface_mode_is_rgmii(interface))
+		enetc4_set_rgmii_mac(pf, speed, duplex);
+
+	if (interface == PHY_INTERFACE_MODE_RMII)
+		enetc4_set_rmii_mac(pf, speed, duplex);
+
+	if (duplex == DUPLEX_FULL) {
+		/* When preemption is enabled, generation of PAUSE frames
+		 * must be disabled, as stated in the IEEE 802.3 standard.
+		 */
+		if (priv->active_offloads & ENETC_F_QBU)
+			tx_pause = false;
+	} else { /* DUPLEX_HALF */
+		if (tx_pause || rx_pause)
+			hd_fc = true;
+
+		/* As per 802.3 annex 31B, PAUSE frames are only supported
+		 * when the link is configured for full duplex operation.
+		 */
+		tx_pause = false;
+		rx_pause = false;
+	}
+
+	enetc4_set_hd_flow_control(pf, hd_fc);
+	enetc4_set_tx_pause(pf, priv->num_rx_rings, tx_pause);
+	enetc4_set_rx_pause(pf, rx_pause);
+	enetc4_enable_mac(pf, true);
+	ecdev_set_link(priv->ecdev, 1);
+
+	//priv->eee.eee_active = phylink_init_eee(priv->phylink, true) >= 0;
+	priv->eee.eee_active = false;
+	ec_enetc_eee_mode_set(si->ndev, priv->eee.eee_active);
+
+	if (si->hw_features & ENETC_SI_F_QBU)
+		ec_enetc_mm_link_state_update(priv, true);
+
+	enetc4_pf_send_link_status_msg(pf, true);
+}
+
+static void enetc4_pl_mac_link_down(struct phylink_config *config,
+				    unsigned int mode,
+				    phy_interface_t interface)
+{
+	struct enetc_pf *pf = phylink_to_enetc_pf(config);
+	struct enetc_si *si = pf->si;
+	struct enetc_ndev_priv *priv;
+
+	priv = netdev_priv(si->ndev);
+
+	priv->eee.eee_active = false;
+	ec_enetc_eee_mode_set(si->ndev, priv->eee.eee_active);
+
+	if (si->hw_features & ENETC_SI_F_QBU)
+		ec_enetc_mm_link_state_update(priv, false);
+
+	enetc4_pf_send_link_status_msg(pf, false);
+	enetc4_enable_mac(pf, false);
+	ecdev_set_link(priv->ecdev, 0);
+}
+
+static const struct phylink_mac_ops enetc_pl_mac_ops = {
+	.mac_select_pcs = enetc4_pl_mac_select_pcs,
+	.mac_config = enetc4_pl_mac_config,
+	.mac_link_up = enetc4_pl_mac_link_up,
+	.mac_link_down = enetc4_pl_mac_link_down,
+};
+
+static int enetc4_alloc_cls_rules(struct enetc_ndev_priv *priv)
+{
+	struct enetc_pf *pf = enetc_si_priv(priv->si);
+
+	/* Each ingress port filter entry occupies 2 words at least. */
+	priv->max_ipf_entries = pf->caps.ipf_words_num / 2;
+	priv->cls_rules = kcalloc(priv->max_ipf_entries, sizeof(*priv->cls_rules),
+				  GFP_KERNEL);
+	if (!priv->cls_rules)
+		return -ENOMEM;
+
+	return 0;
+}
+
+static void enetc4_free_cls_rules(struct enetc_ndev_priv *priv)
+{
+	kfree(priv->cls_rules);
+}
+
+static void enetc4_pf_set_si_primary_mac(struct enetc_hw *hw, int si, const u8 *addr)
+{
+	u16 lower = get_unaligned_le16(addr + 4);
+	u32 upper = get_unaligned_le32(addr);
+
+	if (si != 0) {
+		printk(KERN_ERR "enetc4_pf_set_si_primary_mac si !=0 mac: %s %s %d %d %d %d\n", __FILE__, __FUNCTION__, __LINE__, addr[0], addr[1], addr[2]);
+		__raw_writel(upper, hw->port + ENETC4_PSIPMAR0(si));
+		__raw_writew(lower, hw->port + ENETC4_PSIPMAR1(si));
+	} else {
+		printk(KERN_ERR "enetc4_pf_set_si_primary_mac si ==0 mac: %s %s %d %d %d %d\n", __FILE__, __FUNCTION__, __LINE__, addr[0], addr[1], addr[2]);
+		__raw_writel(upper, hw->port + ENETC4_PMAR0);
+		__raw_writew(lower, hw->port + ENETC4_PMAR1);
+	}
+}
+
+static void enetc4_pf_get_si_primary_mac(struct enetc_hw *hw, int si, u8 *addr)
+{
+	u32 upper;
+	u16 lower;
+
+	upper = __raw_readl(hw->port + ENETC4_PSIPMAR0(si));
+	lower = __raw_readw(hw->port + ENETC4_PSIPMAR1(si));
+	printk(KERN_ERR "DBG upper: %s %s %d %d\n", __FILE__, __FUNCTION__, __LINE__, upper);
+	printk(KERN_ERR "DBG lower: %s %s %d %d\n", __FILE__, __FUNCTION__, __LINE__, lower);
+	put_unaligned_le32(upper, addr);
+	put_unaligned_le16(lower, addr + 4);
+}
+
+static void enetc4_pf_set_si_based_vlan(struct enetc_hw *hw, int si,
+					u16 vlan, u8 qos)
+{
+	u32 val = 0;
+
+	if (vlan) {
+		val = PSIVLANR_E | (vlan & PSIVLANR_VID);
+		val = u32_replace_bits(val, qos, PSIVLANR_PCP);
+	}
+
+	enetc_port_wr(hw, ENETC4_PSIVLANR(si), val);
+}
+
+static void enetc4_pf_get_si_based_vlan(struct enetc_hw *hw, int si,
+					u32 *vid, u32 *pcp)
+{
+	u32 val = enetc_port_rd(hw, ENETC4_PSIVLANR(si));
+
+	*vid = val & PSIVLANR_VID;
+	*pcp = (val & PSIVLANR_PCP) >> PSIVLANR_PCP_OFF;
+}
+
+static void enetc4_pf_set_si_anti_spoofing(struct enetc_hw *hw, int si, bool en)
+{
+	u32 val = enetc_port_rd(hw, ENETC4_PSICFGR0(si));
+
+	val = (val & ~PSICFGR0_ANTI_SPOOFING) | (en ? PSICFGR0_ANTI_SPOOFING : 0);
+	enetc_port_wr(hw, ENETC4_PSICFGR0(si), val);
+}
+
+static void enetc4_pf_set_si_mac_promisc(struct enetc_hw *hw, int si, int type, bool en)
+{
+	u32 val = enetc_port_rd(hw, ENETC4_PSIPMMR);
+
+	if (type == UC) {
+		if (en)
+			val |= ENETC_PSIPMR_SET_UP(si);
+		else
+			val &= ~ENETC_PSIPMR_SET_UP(si);
+	} else { /* Multicast promiscuous mode. */
+		if (en)
+			val |= ENETC_PSIPMR_SET_MP(si);
+		else
+			val &= ~ENETC_PSIPMR_SET_MP(si);
+	}
+
+	enetc_port_wr(hw, ENETC4_PSIPMMR, val);
+}
+
+static void enetc4_pf_set_si_mac_hash_filter(struct enetc_hw *hw, int si,
+					     int type, u64 hash)
+{
+	if (type == UC) {
+		enetc_port_wr(hw, ENETC4_PSIUMHFR0(si), lower_32_bits(hash));
+		enetc_port_wr(hw, ENETC4_PSIUMHFR1(si), upper_32_bits(hash));
+	} else { /* MC */
+		enetc_port_wr(hw, ENETC4_PSIMMHFR0(si), lower_32_bits(hash));
+		enetc_port_wr(hw, ENETC4_PSIMMHFR1(si), upper_32_bits(hash));
+	}
+}
+
+static void enetc4_pf_set_si_vlan_hash_filter(struct enetc_hw *hw, int si, u64 hash)
+{
+	enetc_port_wr(hw, ENETC4_PSIVHFR0(si), lower_32_bits(hash));
+	enetc_port_wr(hw, ENETC4_PSIVHFR1(si), upper_32_bits(hash));
+}
+
+static void enetc4_pf_set_loopback(struct net_device *ndev, bool en)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	struct enetc_si *si = priv->si;
+	u32 val;
+
+	val = ec_enetc_port_mac_rd(si, ENETC4_PM_CMD_CFG(0));
+	/* Enable or disable loopback. */
+	val = u32_replace_bits(val, en ? 1 : 0, PM_CMD_CFG_LOOP_EN);
+	/* Default to select MAC level loopback mode if loopback is enabled. */
+	val = u32_replace_bits(val, en ? LPBCK_MODE_MAC_LEVEL : 0,
+			       PM_CMD_CFG_LPBK_MODE);
+
+	ec_enetc_port_mac_wr(si, ENETC4_PM_CMD_CFG(0), val);
+}
+
+static void enetc4_pf_set_tc_tsd(struct enetc_hw *hw, int tc, bool en)
+{
+	enetc_port_wr(hw, ENETC4_PTCTSDR(tc), en ? PTCTSDR_TSDE : 0);
+}
+
+static bool enetc4_pf_get_time_gating(struct enetc_hw *hw)
+{
+	return !!(enetc_port_rd(hw, ENETC4_PTGSCR) & PTGSCR_TGE);
+}
+
+static void enetc4_pf_set_time_gating(struct enetc_hw *hw, bool en)
+{
+	u32 old_val, val;
+
+	old_val = enetc_port_rd(hw, ENETC4_PTGSCR);
+	val = u32_replace_bits(old_val, en ? 1 : 0, PTGSCR_TGE);
+	if (val != old_val)
+		enetc_port_wr(hw, ENETC4_PTGSCR, val);
+}
+
+static const struct enetc_pf_hw_ops enetc4_pf_hw_ops = {
+	.set_si_primary_mac = enetc4_pf_set_si_primary_mac,
+	.get_si_primary_mac = enetc4_pf_get_si_primary_mac,
+	.set_si_based_vlan = enetc4_pf_set_si_based_vlan,
+	.get_si_based_vlan = enetc4_pf_get_si_based_vlan,
+	.set_si_anti_spoofing = enetc4_pf_set_si_anti_spoofing,
+	.set_si_vlan_promisc = enetc4_pf_set_si_vlan_promisc,
+	.set_si_mac_promisc = enetc4_pf_set_si_mac_promisc,
+	.set_si_mac_hash_filter = enetc4_pf_set_si_mac_hash_filter,
+	.set_si_vlan_hash_filter = enetc4_pf_set_si_vlan_hash_filter,
+	.set_loopback = enetc4_pf_set_loopback,
+	.set_tc_tsd = enetc4_pf_set_tc_tsd,
+	.set_tc_msdu = enetc4_pf_set_tc_msdu,
+	.reset_tc_msdu = enetc4_pf_reset_tc_msdu,
+	.get_time_gating = enetc4_pf_get_time_gating,
+	.set_time_gating = enetc4_pf_set_time_gating,
+};
+
+static void enetc4_get_ntmp_caps(struct enetc_si *si)
+{
+	struct ntmp_caps *caps = &si->ntmp.caps;
+	struct enetc_hw *hw = &si->hw;
+	u32 reg;
+
+	/* Get the max number of entris of RP table */
+	reg = enetc_port_rd(hw, ENETC4_RPITCAPR);
+	caps->rpt_num_entries = reg & RPITCAPR_NUM_ENTRIES;
+
+	/* Get the max number of entris of IS table */
+	reg = enetc_port_rd(hw, ENETC4_ISITCAPR);
+	caps->ist_num_entries = reg & ISITCAPR_NUM_ENTRIES;
+
+	/* Get the max number of entris of SGI table */
+	reg = enetc_port_rd(hw, ENETC4_SGIITCAPR);
+	caps->sgit_num_entries = reg & SGITCAPR_NUM_ENTRIES;
+
+	/* Get the max number of entris of ISC table */
+	reg = enetc_port_rd(hw, ENETC4_ISCICAPR);
+	caps->isct_num_entries = reg & ISCICAPR_NUM_ENTRIES;
+
+	/* Get the max number of words of SGCL table */
+	reg = enetc_port_rd(hw, ENETC4_SGCLITCAPR);
+	caps->sgclt_num_words = reg & SGCLITCAPR_NUM_WORDS;
+}
+
+static u64 enetc4_get_current_time(struct enetc_si *si)
+{
+	u32 time_l, time_h;
+	u64 current_time;
+
+	time_l = enetc_rd_hot(&si->hw, ENETC_SICTR0);
+	time_h = enetc_rd_hot(&si->hw, ENETC_SICTR1);
+	current_time = (u64)time_h << 32 | time_l;
+
+	return current_time;
+}
+
+static u64 enetc4_adjust_base_time(struct ntmp_priv *ntmp, u64 base_time,
+				   u32 cycle_time)
+{
+	struct enetc_si *si = ntmp_to_enetc_si(ntmp);
+	u64 current_time, delta, n;
+
+	current_time = enetc4_get_current_time(si);
+	if (base_time >= current_time)
+		return base_time;
+
+	delta = current_time - base_time;
+	n = DIV_ROUND_UP_ULL(delta, cycle_time);
+	base_time += (n * (u64)cycle_time);
+
+	return base_time;
+}
+
+static u32 enetc4_get_tgst_free_words(struct ntmp_priv *ntmp)
+{
+	struct enetc_si *si = ntmp_to_enetc_si(ntmp);
+	struct enetc_hw *hw = &si->hw;
+	u32 words_in_use;
+	u32 total_words;
+
+	/* Notice that the admin gate list should be delete first before call
+	 * this function, so the ENETC4_PTGAGLLR[ADMIN_GATE_LIST_LENGTH] equal
+	 * to zero. That is, the ENETC4_TGSTMOR only contains the words of the
+	 * operational gate control list.
+	 */
+	words_in_use = enetc_port_rd(hw, ENETC4_TGSTMOR) & TGSTMOR_NUM_WORDS;
+	total_words = enetc_port_rd(hw, ENETC4_TGSTCAPR) & TGSTCAPR_NUM_WORDS;
+
+	return total_words - words_in_use;
+}
+
+static int enetc4_ntmp_bitmap_init(struct ntmp_priv *ntmp)
+{
+	ntmp->ist_eid_bitmap = bitmap_zalloc(ntmp->caps.ist_num_entries,
+					     GFP_KERNEL);
+	if (!ntmp->ist_eid_bitmap)
+		return -ENOMEM;
+
+	ntmp->sgit_eid_bitmap = bitmap_zalloc(ntmp->caps.sgit_num_entries,
+					      GFP_KERNEL);
+	if (!ntmp->sgit_eid_bitmap)
+		goto free_ist_bitmap;
+
+	ntmp->sgclt_word_bitmap = bitmap_zalloc(ntmp->caps.sgclt_num_words,
+						GFP_KERNEL);
+	if (!ntmp->sgclt_word_bitmap)
+		goto free_sgit_bitmap;
+
+	ntmp->isct_eid_bitmap = bitmap_zalloc(ntmp->caps.isct_num_entries,
+					      GFP_KERNEL);
+	if (!ntmp->isct_eid_bitmap)
+		goto free_sgclt_bitmap;
+
+	ntmp->rpt_eid_bitmap = bitmap_zalloc(ntmp->caps.rpt_num_entries,
+					     GFP_KERNEL);
+	if (!ntmp->rpt_eid_bitmap)
+		goto free_isct_bitmap;
+
+	return 0;
+
+free_isct_bitmap:
+	bitmap_free(ntmp->isct_eid_bitmap);
+	ntmp->isct_eid_bitmap = NULL;
+
+free_sgclt_bitmap:
+	bitmap_free(ntmp->sgclt_word_bitmap);
+	ntmp->sgclt_word_bitmap = NULL;
+
+free_sgit_bitmap:
+	bitmap_free(ntmp->sgit_eid_bitmap);
+	ntmp->sgit_eid_bitmap = NULL;
+
+free_ist_bitmap:
+	bitmap_free(ntmp->ist_eid_bitmap);
+	ntmp->ist_eid_bitmap = NULL;
+
+	return -ENOMEM;
+}
+
+static void enetc4_ntmp_bitmap_free(struct ntmp_priv *ntmp)
+{
+	bitmap_free(ntmp->rpt_eid_bitmap);
+	ntmp->rpt_eid_bitmap = NULL;
+
+	bitmap_free(ntmp->isct_eid_bitmap);
+	ntmp->isct_eid_bitmap = NULL;
+
+	bitmap_free(ntmp->sgclt_word_bitmap);
+	ntmp->sgclt_word_bitmap = NULL;
+
+	bitmap_free(ntmp->sgit_eid_bitmap);
+	ntmp->sgit_eid_bitmap = NULL;
+
+	bitmap_free(ntmp->ist_eid_bitmap);
+	ntmp->ist_eid_bitmap = NULL;
+}
+
+static int enetc4_init_ntmp_priv(struct enetc_si *si)
+{
+	struct ntmp_priv *ntmp = &si->ntmp;
+	int err;
+
+	ntmp->dev_type = NETC_DEV_ENETC;
+
+	if (si->revision == NETC_REVISION_4_1)
+		ntmp->errata = NTMP_ERR052134;
+
+	err = ec_enetc_init_cbdr(si);
+	if (err)
+		return err;
+
+	enetc4_get_ntmp_caps(si);
+	err = enetc4_ntmp_bitmap_init(ntmp);
+	if (err)
+		goto free_cbdr;
+
+	ntmp->adjust_base_time = enetc4_adjust_base_time;
+	ntmp->get_tgst_free_words = enetc4_get_tgst_free_words;
+
+	INIT_HLIST_HEAD(&ntmp->flower_list);
+	mutex_init(&ntmp->flower_lock);
+
+	return 0;
+
+free_cbdr:
+	ec_enetc_free_cbdr(si);
+
+	return err;
+}
+
+static void enetc4_deinit_ntmp_priv(struct enetc_si *si)
+{
+	enetc4_clear_flower_list(si);
+	mutex_destroy(&si->ntmp.flower_lock);
+	enetc4_ntmp_bitmap_free(&si->ntmp);
+	ec_enetc_free_cbdr(si);
+}
+
+static int enetc4_pf_init(struct enetc_pf *pf)
+{
+	struct device *dev = &pf->si->pdev->dev;
+	int err;
+
+	enetc_get_ip_revision(pf->si);
+
+	dev_err(dev, "enetc4_pf_init\n");
+	/* Initialize the MAC address for PF and VFs */
+	err = enetc_setup_mac_addresses(dev->of_node, pf);
+	if (err) {
+		dev_err(dev, "Failed to set MAC addresses\n");
+		return err;
+	}
+
+	err =  enetc4_init_ntmp_priv(pf->si);
+	if (err) {
+		dev_err(dev, "Failed to init CBDR\n");
+		return err;
+	}
+
+	enetc4_configure_port(pf);
+
+	return 0;
+}
+
+static void enetc4_pf_deinit(struct enetc_pf *pf)
+{
+	enetc4_deinit_ntmp_priv(pf->si);
+}
+
+static int enetc4_link_init(struct enetc_ndev_priv *priv,
+			    struct device_node *node)
+{
+	struct enetc_pf *pf = enetc_si_priv(priv->si);
+	struct device *dev = priv->dev;
+	int err;
+
+	err = of_get_phy_mode(node, &pf->if_mode);
+	if (err) {
+		dev_err(dev, "Failed to get PHY mode\n");
+		return err;
+	}
+
+	err = enetc_mdiobus_create(pf, node);
+	if (err) {
+		dev_err(dev, "Failed to create MDIO bus\n");
+		return err;
+	}
+
+	err = enetc_phylink_create(priv, node, &enetc_pl_mac_ops);
+	if (err) {
+		dev_err(dev, "Failed to create phylink\n");
+		goto err_phylink_create;
+	}
+
+	return 0;
+
+err_phylink_create:
+	enetc_mdiobus_destroy(pf);
+
+	return err;
+}
+
+static void enetc4_link_deinit(struct enetc_ndev_priv *priv)
+{
+	struct enetc_pf *pf = enetc_si_priv(priv->si);
+
+	enetc_phylink_destroy(priv);
+	enetc_mdiobus_destroy(pf);
+}
+
+static int enetc4_pf_netdev_create(struct enetc_si *si)
+{
+	struct device *dev = &si->pdev->dev;
+	struct enetc_ndev_priv *priv;
+	struct net_device *ndev;
+	int err;
+
+	dev_err(dev, "enetc4_pf_netdev_create\n");
+	ndev = alloc_etherdev_mqs(sizeof(struct enetc_ndev_priv),
+				  si->num_tx_rings, si->num_rx_rings);
+	if (!ndev)
+		return  -ENOMEM;
+
+	priv = netdev_priv(ndev);
+	mutex_init(&priv->mm_lock);
+
+	if (si->pdev->rcec)
+		priv->rcec = si->pdev->rcec;
+
+	priv->ref_clk = devm_clk_get_optional(dev, "enet_ref_clk");
+	if (IS_ERR(priv->ref_clk)) {
+		dev_err(dev, "Get enet_ref_clk failed\n");
+		err = PTR_ERR(priv->ref_clk);
+		goto err_clk_get;
+	}
+
+	enetc_pf_netdev_setup(si, ndev, &enetc4_ndev_ops);
+
+	ec_enetc_init_si_rings_params(priv);
+	err = ec_enetc_configure_si(priv);
+	if (err) {
+		dev_err(dev, "Failed to configure SI\n");
+		goto err_config_si;
+	}
+
+	err = enetc4_alloc_cls_rules(priv);
+	if (err) {
+		dev_err(dev, "Failed to alloc cls rules memory\n");
+		goto err_alloc_cls_rules;
+	}
+
+	err = enetc4_link_init(priv, dev->of_node);
+	if (err)
+		goto err_link_init;
+
+	priv->ecdev = ecdev_offer(ndev, ec_poll, THIS_MODULE);
+        //if (ecdev_open(priv->ecdev)) {
+        //        ecdev_withdraw(priv->ecdev);
+        //        goto err_reg_ec_net;
+        //}
+
+	err = register_netdev(ndev);
+	if (err) {
+		printk(KERN_ERR "register_netdev err: %s %s %d %d\n", __FILE__, __FUNCTION__, __LINE__, err);
+		dev_err(dev, "Failed to register netdev\n");
+		goto err_reg_netdev;
+	}
+
+	return 0;
+
+err_reg_netdev:
+	enetc4_link_deinit(priv);
+err_reg_ec_net:
+	enetc_phylink_destroy(priv);
+err_link_init:
+	//ec_enetc_free_msix(priv);
+err_alloc_msix:
+	enetc4_free_cls_rules(priv);
+err_alloc_cls_rules:
+err_config_si:
+err_clk_get:
+	mutex_destroy(&priv->mm_lock);
+	free_netdev(ndev);
+
+	return err;
+}
+
+static void enetc4_pf_netdev_destroy(struct enetc_si *si)
+{
+	struct net_device *ndev = si->ndev;
+	struct enetc_ndev_priv *priv;
+
+	priv = netdev_priv(ndev);
+	unregister_netdev(ndev);
+	enetc4_link_deinit(priv);
+	//ec_enetc_free_msix(priv);
+	enetc4_free_cls_rules(priv);
+	mutex_destroy(&priv->mm_lock);
+	free_netdev(ndev);
+}
+
+static void enetc4_pf_destroy_vlan_list(struct enetc_pf *pf)
+{
+	struct enetc_vlan_list_entry *entry;
+	struct hlist_node *tmp;
+
+	guard(mutex)(&pf->vlan_list_lock);
+	hlist_for_each_entry_safe(entry, tmp, &pf->vlan_list, node) {
+		hlist_del(&entry->node);
+		kfree(entry);
+	}
+
+	pf->num_vlan_fe = 0;
+}
+
+static void enetc4_pf_destroy_mac_list(struct enetc_pf *pf)
+{
+	struct enetc_mac_list_entry *entry;
+	struct hlist_node *tmp;
+
+	guard(mutex)(&pf->mac_list_lock);
+	hlist_for_each_entry_safe(entry, tmp, &pf->mac_list, node) {
+		hlist_del(&entry->node);
+		kfree(entry);
+	}
+
+	pf->num_mac_fe = 0;
+}
+
+static int enetc4_pf_unload(struct enetc_pf *pf)
+{
+	struct enetc_si *si = pf->si;
+
+	drain_workqueue(si->workqueue);
+	enetc4_pf_netdev_destroy(si);
+	enetc4_pf_deinit(pf);
+	enetc4_pf_destroy_vlan_list(pf);
+	enetc4_pf_destroy_mac_list(pf);
+	pci_disable_device(si->pdev);
+
+	return 0;
+}
+
+static int enetc4_pf_load(struct enetc_pf *pf)
+{
+	struct pci_dev *pdev = pf->si->pdev;
+	struct enetc_si *si = pf->si;
+	int err;
+
+	dev_err(&pdev->dev, "enetc4_pf_load!!!!\n");
+	pcie_flr(pdev);
+	err = pci_enable_device_mem(pdev);
+	if (err) {
+		dev_err(&pdev->dev, "Failed to enable ENETC\n");
+		return err;
+	}
+
+	pci_set_master(pdev);
+
+	err = enetc4_pf_init(pf);
+	if (err)
+		goto err_pf_init;
+
+	ec_enetc_get_si_caps(si);
+	err = enetc4_pf_netdev_create(si);
+	if (err)
+		goto err_netdev_create;
+
+	return 0;
+
+err_netdev_create:
+	enetc4_pf_deinit(pf);
+err_pf_init:
+	pci_disable_device(pdev);
+
+	return err;
+}
+
+static int enetc4_init_devlink(struct enetc_pf *pf)
+{
+	struct enetc_devlink_priv *devl_priv = pf->devl_priv;
+	struct devlink *devlink = priv_to_devlink(devl_priv);
+	int err;
+
+	devl_priv->pf_load = enetc4_pf_load;
+	devl_priv->pf_unload = enetc4_pf_unload;
+
+	err = enetc_devlink_params_register(devlink);
+	if (err)
+		return err;
+
+	devlink_register(devlink);
+
+	enetc_devlink_init_params(devlink);
+
+	return 0;
+}
+
+static void enetc4_deinit_devlink(struct enetc_pf *pf)
+{
+	struct devlink *devlink = priv_to_devlink(pf->devl_priv);
+
+	devlink_unregister(devlink);
+	enetc_devlink_params_unregister(devlink);
+}
+
+static void enetc4_get_psi_hw_features(struct enetc_si *si)
+{
+	struct enetc_hw *hw = &si->hw;
+	u32 val;
+
+	val = enetc_port_rd(hw, ENETC4_PCAPR);
+	if (val & PCAPR_TGS)
+		si->hw_features |= ENETC_SI_F_QBV;
+
+	val = enetc_port_rd(hw, ENETC4_PMCAPR);
+	if (PMCAPR_GET_FP(val) == PMCAPR_FP_SUPP) {
+		si->hw_features |= ENETC_SI_F_QBU;
+		si->pmac_offset = ENETC4_PMAC_OFFSET;
+	}
+
+	val = enetc_port_rd(hw, ENETC4_IPCAPR);
+	if (val & IPCAPR_ISID)
+		si->hw_features |= ENETC_SI_F_PSFP;
+
+	val = enetc_port_rd(hw, ENETC4_PCAPR);
+	if (val & PCAPR_LINK_TYPE)
+		si->hw_features |= ENETC_SI_F_PPM;
+}
+
+static int enetc4_pf_struct_init(struct enetc_si *si)
+{
+	struct enetc_pf *pf = enetc_si_priv(si);
+	struct device *dev = &si->pdev->dev;
+	int err;
+
+	pf->si = si;
+	pf->total_vfs = pci_sriov_get_totalvfs(si->pdev);
+	if (pf->total_vfs) {
+		pf->vf_state = kcalloc(pf->total_vfs, sizeof(struct enetc_vf_state),
+				       GFP_KERNEL);
+		if (!pf->vf_state)
+			return -ENOMEM;
+	}
+
+	enetc4_get_psi_hw_features(si);
+	enetc4_get_port_caps(pf);
+	enetc_pf_register_hw_ops(pf, &enetc4_pf_hw_ops);
+
+	printk(KERN_ERR "DBG hw_ops done: %s %s %d\n", __FILE__, __FUNCTION__, __LINE__);
+	err = enetc_devlink_alloc(pf);
+	if (err) {
+		dev_err(dev, "Failed to alloc devlink\n");
+		goto free_vf_state;
+	}
+
+	err = enetc4_init_devlink(pf);
+	if (err) {
+		dev_err(dev, "Failed to init devlink\n");
+		goto free_vf_state;
+	}
+
+	INIT_HLIST_HEAD(&pf->mac_list);
+	mutex_init(&pf->mac_list_lock);
+	INIT_HLIST_HEAD(&pf->vlan_list);
+	mutex_init(&pf->vlan_list_lock);
+
+	printk(KERN_ERR "DBG enetc4_pf_struct_init done: %s %s %d\n", __FILE__, __FUNCTION__, __LINE__);
+	return 0;
+
+free_vf_state:
+	kfree(pf->vf_state);
+
+	return err;
+}
+
+static void enetc4_pf_struct_deinit(struct enetc_pf *pf)
+{
+	enetc4_pf_destroy_vlan_list(pf);
+	mutex_destroy(&pf->vlan_list_lock);
+	enetc4_pf_destroy_mac_list(pf);
+	mutex_destroy(&pf->mac_list_lock);
+	enetc4_deinit_devlink(pf);
+	kfree(pf->vf_state);
+}
+
+static bool enetc_is_emdio_consumer(const struct device_node *np)
+{
+	struct device_node *phy_node, *mdio_node;
+
+	/* If the node does not have phy-handle property, then the PF
+	 * does not connect to a PHY, so it is not the EMDIO consumer.
+	 */
+	phy_node = of_parse_phandle(np, "phy-handle", 0);
+	if (!phy_node)
+		return false;
+
+	of_node_put(phy_node);
+
+	/* If the node has phy-handle property and it contains a mdio
+	 * child node, then the PF is not the EMDIO consumer.
+	 */
+	mdio_node = of_get_child_by_name(np, "mdio");
+	if (mdio_node) {
+		of_node_put(mdio_node);
+		return false;
+	}
+
+	return true;
+}
+
+static int enetc_add_emdio_consumer(struct pci_dev *pdev)
+{
+	struct device_node *node = pdev->dev.of_node;
+	struct device *dev = &pdev->dev;
+	struct device_node *phy_node;
+	struct phy_device *phydev;
+	struct device_link *link;
+
+	if (!node || !enetc_is_emdio_consumer(node))
+		return 0;
+
+	phy_node = of_parse_phandle(node, "phy-handle", 0);
+	phydev = of_phy_find_device(phy_node);
+	of_node_put(phy_node);
+	if (!phydev)
+		return -EPROBE_DEFER;
+
+	link = device_link_add(dev, phydev->mdio.bus->parent,
+			       DL_FLAG_PM_RUNTIME |
+			       DL_FLAG_AUTOREMOVE_SUPPLIER);
+	put_device(&phydev->mdio.dev);
+	if (!link)
+		return -EINVAL;
+
+	return 0;
+}
+
+static int enetc4_pf_probe(struct pci_dev *pdev,
+			   const struct pci_device_id *ent)
+{
+	struct device *dev = &pdev->dev;
+	struct enetc_si *si;
+	struct enetc_pf *pf;
+	char wq_name[24];
+	int err;
+
+	if (enetc_pf_is_owned_by_mcore(pdev))
+		return 0;
+
+	err = enetc_add_emdio_consumer(pdev);
+	if (err)
+		return err;
+
+	err = ec_enetc_pci_probe(pdev, KBUILD_MODNAME, sizeof(*pf));
+	if (err) {
+		dev_err(dev, "PCIe probing failed\n");
+		return err;
+	}
+
+	/* si is the private data. */
+	si = pci_get_drvdata(pdev);
+	if (!si->hw.port || !si->hw.global) {
+		err = -ENODEV;
+		dev_err(dev, "Couldn't map PF only space!\n");
+		goto err_ec_enetc_pci_probe;
+	}
+
+	err = enetc4_pf_struct_init(si);
+	if (err)
+		goto err_pf_struct_init;
+
+	pf = enetc_si_priv(si);
+	INIT_WORK(&si->rx_mode_task, enetc4_pf_do_set_rx_mode);
+	snprintf(wq_name, sizeof(wq_name), "enetc-%s", pci_name(pdev));
+	si->workqueue = create_singlethread_workqueue(wq_name);
+	if (!si->workqueue) {
+		err = -ENOMEM;
+		goto err_create_wq;
+	}
+
+	err = enetc4_pf_init(pf);
+	if (err)
+		goto err_pf_init;
+
+	ec_enetc_get_si_caps(si);
+
+	err = enetc4_pf_netdev_create(si);
+	if (err)
+		goto err_netdev_create;
+
+	return 0;
+
+err_netdev_create:
+	enetc4_pf_deinit(pf);
+err_pf_init:
+	destroy_workqueue(si->workqueue);
+err_create_wq:
+	enetc4_pf_struct_deinit(pf);
+err_pf_struct_init:
+err_ec_enetc_pci_probe:
+	ec_enetc_pci_remove(pdev);
+
+	return err;
+}
+
+static void enetc4_pf_remove(struct pci_dev *pdev)
+{
+	struct enetc_si *si;
+	struct enetc_pf *pf;
+	struct enetc_ndev_priv *priv;
+
+	if (enetc_pf_is_owned_by_mcore(pdev)) {
+		pci_sriov_configure_simple(pdev, 0);
+		return;
+	}
+
+	si = pci_get_drvdata(pdev);
+
+	pf = enetc_si_priv(si);
+	if (pf->num_vfs)
+		enetc_sriov_configure(pdev, 0);
+
+	priv = netdev_priv(si->ndev);
+	ecdev_close(priv->ecdev);
+	ecdev_withdraw(priv->ecdev);
+
+	enetc4_pf_netdev_destroy(si);
+	enetc4_pf_deinit(pf);
+	destroy_workqueue(si->workqueue);
+	enetc4_pf_struct_deinit(pf);
+	ec_enetc_pci_remove(pdev);
+}
+
+/* Only ENETC PF Function can be probed. */
+static const struct pci_device_id enetc4_pf_id_table[] = {
+	{ PCI_DEVICE(PCI_VENDOR_ID_NXP2, PCI_DEVICE_ID_NXP2_ENETC_PF) },
+	{ PCI_DEVICE(PCI_VENDOR_ID_NXP2, ENETC_PF_VIRTUAL_DEVID) },
+	{ PCI_DEVICE(PCI_VENDOR_ID_NXP2, NXP_ENETC_PPM_DEV_ID) },
+	{ 0, } /* End of table. */
+};
+MODULE_DEVICE_TABLE(pci, enetc4_pf_id_table);
+
+#ifdef CONFIG_PCI_IOV
+static int enetc4_sriov_suspend_resume_configure(struct pci_dev *pdev, bool suspend)
+{
+	struct enetc_si *si = pci_get_drvdata(pdev);
+	struct enetc_pf *pf = enetc_si_priv(si);
+	int err;
+
+	if (pf->num_vfs == 0)
+		return 0;
+
+	if (suspend) {
+		pci_disable_sriov(pdev);
+		enetc_msg_psi_free(pf);
+	} else {
+		err = enetc_msg_psi_init(pf);
+		if (err) {
+			dev_err(&pdev->dev, "enetc_msg_psi_init (%d)\n", err);
+			return err;
+		}
+
+		err = pci_enable_sriov(pdev, pf->num_vfs);
+		if (err) {
+			dev_err(&pdev->dev, "pci_enable_sriov err %d\n", err);
+			goto err_en_sriov;
+		}
+	}
+
+	return 0;
+
+err_en_sriov:
+	enetc_msg_psi_free(pf);
+
+	return err;
+}
+#else
+static int enetc4_sriov_suspend_resume_configure(struct pci_dev *pdev, bool suspend)
+{
+	return 0;
+}
+#endif
+
+static int enetc4_pf_imdio_regulator_enable(struct enetc_pf *pf)
+{
+	struct enetc_mdio_priv *mdio_priv;
+	int err = 0;
+
+	if (!pf->imdio)
+		return -EINVAL;
+	mdio_priv = pf->imdio->priv;
+
+	if (mdio_priv && mdio_priv->regulator)
+		err = regulator_enable(mdio_priv->regulator);
+
+	return err;
+}
+
+static void enetc4_pf_imdio_regulator_disable(struct enetc_pf *pf)
+{
+	struct enetc_mdio_priv *mdio_priv;
+
+	if (!pf->imdio)
+		return;
+	mdio_priv = pf->imdio->priv;
+
+	if (mdio_priv && mdio_priv->regulator)
+		regulator_disable(mdio_priv->regulator);
+}
+
+static void enetc4_pf_power_down(struct enetc_si *si)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(si->ndev);
+	struct enetc_pf *pf = enetc_si_priv(si);
+	struct pci_dev *pdev = si->pdev;
+
+	if (pf->pcs)
+		enetc4_pf_imdio_regulator_disable(pf);
+	//ec_enetc_free_msix(priv);
+	ec_enetc_free_cbdr(si);
+	pci_disable_device(pdev);
+	pcie_flr(pdev);
+}
+
+static int enetc4_pf_power_up(struct pci_dev *pdev, struct device_node *node)
+{
+	struct enetc_ndev_priv *priv;
+	struct enetc_si *si;
+	struct enetc_pf *pf;
+	int err;
+
+	dev_err(&pdev->dev, "enetc4_pf_power_up\n");
+	si = pci_get_drvdata(pdev);
+	pf = enetc_si_priv(si);
+	priv = netdev_priv(si->ndev);
+
+	err = pci_enable_device_mem(pdev);
+	if (err) {
+		dev_err(&pdev->dev, "device enable failed\n");
+		return err;
+	}
+
+	pci_set_master(pdev);
+	pci_restore_state(pdev);
+
+	err = ec_enetc_init_cbdr(si);
+	if (err)
+		goto err_init_cbdr;
+
+	err = enetc_setup_mac_addresses(node, pf);
+	if (err)
+		goto err_init_address;
+
+	enetc_load_primary_mac_addr(&si->hw, priv->ndev);
+
+	enetc4_configure_port(pf);
+
+	err = ec_enetc_configure_si(priv);
+	if (err) {
+		dev_err(&pdev->dev, "Failed to configure SI\n");
+		goto err_config_si;
+	}
+
+	//err = ec_enetc_alloc_msix(priv);
+	//if (err) {
+	//	dev_err(&pdev->dev, "MSIX alloc failed\n");
+	//	goto err_alloc_msix;
+	//}
+
+	if (pf->pcs) {
+		err = enetc4_pf_imdio_regulator_enable(pf);
+		if (err) {
+			dev_err(&pdev->dev, "imdio regulator enable failed\n");
+			goto err_imdio_reg_enable;
+		}
+	}
+
+	return 0;
+
+err_imdio_reg_enable:
+	//ec_enetc_free_msix(priv);
+err_alloc_msix:
+err_config_si:
+err_init_address:
+	ec_enetc_free_cbdr(si);
+err_init_cbdr:
+	pci_disable_device(pdev);
+
+	return err;
+}
+
+static void enetc4_pf_set_wol(struct enetc_si *si, bool en)
+{
+	u32 val = ec_enetc_port_mac_rd(si, ENETC4_PM_CMD_CFG(0));
+
+	if (en)
+		val |= PM_CMD_CFG_MG;
+	else
+		val &= ~PM_CMD_CFG_MG;
+	ec_enetc_port_mac_wr(si, ENETC4_PM_CMD_CFG(0), val);
+
+	ec_enetc_port_mac_wr(si, ENETC4_PLPMR, en ? PLPMR_WME : 0);
+}
+
+//static SIMPLE_DEV_PM_OPS(enetc4_pf_pm_ops, enetc4_pf_suspend, enetc4_pf_resume);
+
+static struct pci_driver enetc4_pf_driver = {
+	.name = KBUILD_MODNAME,
+	.id_table = enetc4_pf_id_table,
+	.probe = enetc4_pf_probe,
+	.remove = enetc4_pf_remove,
+//	.driver.pm = &enetc4_pf_pm_ops,
+#ifdef CONFIG_PCI_IOV
+	.sriov_configure = enetc_sriov_configure,
+#endif
+};
+module_pci_driver(enetc4_pf_driver);
+
+MODULE_DESCRIPTION("ENETC4 PF Driver");
+MODULE_LICENSE("Dual BSD/GPL");
diff --git a/devices/enetc/enetc_cbdr.c b/devices/enetc/enetc_cbdr.c
index 201cbc3..8d0443b 100644
--- a/devices/enetc/enetc_cbdr.c
+++ b/devices/enetc/enetc_cbdr.c
@@ -3,9 +3,117 @@
 
 #include "enetc.h"
 
-static void enetc_clean_cbdr(struct enetc_si *si)
+static int enetc_setup_cbdr(struct device *dev, struct enetc_hw *hw,
+			    int bd_count, struct enetc_cbdr *cbdr)
+{
+	int size = bd_count * sizeof(struct enetc_cbd);
+
+	cbdr->bd_base = dma_alloc_coherent(dev, size, &cbdr->bd_dma_base,
+					   GFP_KERNEL);
+	if (!cbdr->bd_base)
+		return -ENOMEM;
+
+	/* h/w requires 128B alignment */
+	if (!IS_ALIGNED(cbdr->bd_dma_base, 128)) {
+		dma_free_coherent(dev, size, cbdr->bd_base,
+				  cbdr->bd_dma_base);
+		return -EINVAL;
+	}
+
+	cbdr->next_to_clean = 0;
+	cbdr->next_to_use = 0;
+	cbdr->dma_dev = dev;
+	cbdr->bd_count = bd_count;
+
+	cbdr->pir = hw->reg + ENETC_SICBDRPIR;
+	cbdr->cir = hw->reg + ENETC_SICBDRCIR;
+	cbdr->mr = hw->reg + ENETC_SICBDRMR;
+
+	/* set CBDR cache attributes */
+	enetc_wr(hw, ENETC_SICAR2,
+		 ENETC_SICAR_RD_COHERENT | ENETC_SICAR_WR_COHERENT);
+
+	enetc_wr(hw, ENETC_SICBDRBAR0, lower_32_bits(cbdr->bd_dma_base));
+	enetc_wr(hw, ENETC_SICBDRBAR1, upper_32_bits(cbdr->bd_dma_base));
+	enetc_wr(hw, ENETC_SICBDRLENR, ENETC_RTBLENR_LEN(cbdr->bd_count));
+
+	enetc_wr_reg(cbdr->pir, cbdr->next_to_clean);
+	enetc_wr_reg(cbdr->cir, cbdr->next_to_use);
+	/* enable ring */
+	enetc_wr_reg(cbdr->mr, BIT(31));
+
+	return 0;
+}
+
+static int enetc4_setup_cbdr(struct enetc_si *si)
+{
+	struct netc_cbdrs *cbdrs = &si->ntmp.cbdrs;
+	struct device *dev = &si->pdev->dev;
+	struct enetc_hw *hw = &si->hw;
+	struct netc_cbdr_regs regs;
+
+	cbdrs->cbdr_num = 1;
+	cbdrs->cbdr_size = NETC_CBDR_BD_NUM;
+	cbdrs->dma_dev = dev;
+	cbdrs->ring = devm_kcalloc(dev, cbdrs->cbdr_num,
+				   sizeof(struct netc_cbdr), GFP_KERNEL);
+	if (!cbdrs->ring)
+		return -ENOMEM;
+
+	enetc_wr(hw, ENETC4_SICCAR,
+		 ENETC_SICAR_RD_COHERENT | ENETC_SICAR_WR_COHERENT);
+
+	regs.pir = hw->reg + ENETC_SICBDRPIR;
+	regs.cir = hw->reg + ENETC_SICBDRCIR;
+	regs.mr = hw->reg + ENETC_SICBDRMR;
+	regs.bar0 = hw->reg + ENETC_SICBDRBAR0;
+	regs.bar1 = hw->reg + ENETC_SICBDRBAR1;
+	regs.lenr = hw->reg + ENETC_SICBDRLENR;
+
+	return netc_setup_cbdr(dev, cbdrs->cbdr_size, &regs, cbdrs->ring);
+}
+
+int ec_enetc_init_cbdr(struct enetc_si *si)
+{
+	if (is_enetc_rev1(si))
+		return enetc_setup_cbdr(&si->pdev->dev, &si->hw,
+					ENETC_CBDR_DEFAULT_SIZE,
+					&si->cbd_ring);
+	else
+		return enetc4_setup_cbdr(si);
+}
+//EXPORT_SYMBOL_GPL(ec_enetc_init_cbdr);
+
+static void enetc_teardown_cbdr(struct enetc_cbdr *cbdr)
+{
+	int size = cbdr->bd_count * sizeof(struct enetc_cbd);
+
+	/* disable ring */
+	enetc_wr_reg(cbdr->mr, 0);
+
+	dma_free_coherent(cbdr->dma_dev, size, cbdr->bd_base,
+			  cbdr->bd_dma_base);
+	cbdr->bd_base = NULL;
+	cbdr->dma_dev = NULL;
+}
+
+static void enetc4_teardown_cbdr(struct netc_cbdrs *cbdrs)
+{
+	netc_teardown_cbdr(cbdrs->dma_dev, cbdrs->ring);
+	cbdrs->dma_dev = NULL;
+}
+
+void ec_enetc_free_cbdr(struct enetc_si *si)
+{
+	if (is_enetc_rev1(si))
+		enetc_teardown_cbdr(&si->cbd_ring);
+	else
+		enetc4_teardown_cbdr(&si->ntmp.cbdrs);
+}
+//EXPORT_SYMBOL_GPL(ec_enetc_free_cbdr);
+
+static void enetc_clean_cbdr(struct enetc_cbdr *ring)
 {
-	struct enetc_cbdr *ring = &si->cbd_ring;
 	struct enetc_cbd *dest_cbd;
 	int i, status;
 
@@ -15,7 +123,7 @@ static void enetc_clean_cbdr(struct enetc_si *si)
 		dest_cbd = ENETC_CBD(*ring, i);
 		status = dest_cbd->status_flags & ENETC_CBD_STATUS_MASK;
 		if (status)
-			dev_warn(&si->pdev->dev, "CMD err %04x for cmd %04x\n",
+			dev_warn(ring->dma_dev, "CMD err %04x for cmd %04x\n",
 				 status, dest_cbd->cmd);
 
 		memset(dest_cbd, 0, sizeof(*dest_cbd));
@@ -32,7 +140,7 @@ static int enetc_cbd_unused(struct enetc_cbdr *r)
 		r->bd_count;
 }
 
-int enetc_send_cmd(struct enetc_si *si, struct enetc_cbd *cbd)
+int ec_enetc_send_cmd(struct enetc_si *si, struct enetc_cbd *cbd)
 {
 	struct enetc_cbdr *ring = &si->cbd_ring;
 	int timeout = ENETC_CBDR_TIMEOUT;
@@ -43,7 +151,7 @@ int enetc_send_cmd(struct enetc_si *si, struct enetc_cbd *cbd)
 		return -EIO;
 
 	if (unlikely(!enetc_cbd_unused(ring)))
-		enetc_clean_cbdr(si);
+		enetc_clean_cbdr(ring);
 
 	i = ring->next_to_use;
 	dest_cbd = ENETC_CBD(*ring, i);
@@ -69,12 +177,13 @@ int enetc_send_cmd(struct enetc_si *si, struct enetc_cbd *cbd)
 	/* CBD may writeback data, feedback up level */
 	*cbd = *dest_cbd;
 
-	enetc_clean_cbdr(si);
+	enetc_clean_cbdr(ring);
 
 	return 0;
 }
+//EXPORT_SYMBOL_GPL(ec_enetc_send_cmd);
 
-int enetc_clear_mac_flt_entry(struct enetc_si *si, int index)
+int ec_enetc_clear_mac_flt_entry(struct enetc_si *si, int index)
 {
 	struct enetc_cbd cbd;
 
@@ -84,10 +193,11 @@ int enetc_clear_mac_flt_entry(struct enetc_si *si, int index)
 	cbd.status_flags = ENETC_CBD_FLAGS_SF;
 	cbd.index = cpu_to_le16(index);
 
-	return enetc_send_cmd(si, &cbd);
+	return ec_enetc_send_cmd(si, &cbd);
 }
+//EXPORT_SYMBOL_GPL(ec_enetc_clear_mac_flt_entry);
 
-int enetc_set_mac_flt_entry(struct enetc_si *si, int index,
+int ec_enetc_set_mac_flt_entry(struct enetc_si *si, int index,
 			    char *mac_addr, int si_map)
 {
 	struct enetc_cbd cbd;
@@ -109,71 +219,60 @@ int enetc_set_mac_flt_entry(struct enetc_si *si, int index,
 	cbd.addr[0] = cpu_to_le32(upper);
 	cbd.addr[1] = cpu_to_le32(lower);
 
-	return enetc_send_cmd(si, &cbd);
+	return ec_enetc_send_cmd(si, &cbd);
 }
+//EXPORT_SYMBOL_GPL(ec_enetc_set_mac_flt_entry);
 
-#define RFSE_ALIGN	64
 /* Set entry in RFS table */
-int enetc_set_fs_entry(struct enetc_si *si, struct enetc_cmd_rfse *rfse,
+int ec_enetc_set_fs_entry(struct enetc_si *si, struct enetc_cmd_rfse *rfse,
 		       int index)
 {
+	struct enetc_cbdr *ring = &si->cbd_ring;
 	struct enetc_cbd cbd = {.cmd = 0};
-	dma_addr_t dma, dma_align;
 	void *tmp, *tmp_align;
+	dma_addr_t dma;
 	int err;
 
 	/* fill up the "set" descriptor */
 	cbd.cmd = 0;
 	cbd.cls = 4;
 	cbd.index = cpu_to_le16(index);
-	cbd.length = cpu_to_le16(sizeof(*rfse));
 	cbd.opt[3] = cpu_to_le32(0); /* SI */
 
-	tmp = dma_alloc_coherent(&si->pdev->dev, sizeof(*rfse) + RFSE_ALIGN,
-				 &dma, GFP_KERNEL);
-	if (!tmp) {
-		dev_err(&si->pdev->dev, "DMA mapping of RFS entry failed!\n");
+	tmp = enetc_cbd_alloc_data_mem(si, &cbd, sizeof(*rfse),
+				       &dma, &tmp_align);
+	if (!tmp)
 		return -ENOMEM;
-	}
 
-	dma_align = ALIGN(dma, RFSE_ALIGN);
-	tmp_align = PTR_ALIGN(tmp, RFSE_ALIGN);
 	memcpy(tmp_align, rfse, sizeof(*rfse));
 
-	cbd.addr[0] = cpu_to_le32(lower_32_bits(dma_align));
-	cbd.addr[1] = cpu_to_le32(upper_32_bits(dma_align));
-
-	err = enetc_send_cmd(si, &cbd);
+	err = ec_enetc_send_cmd(si, &cbd);
 	if (err)
-		dev_err(&si->pdev->dev, "FS entry add failed (%d)!", err);
+		dev_err(ring->dma_dev, "FS entry add failed (%d)!", err);
 
-	dma_free_coherent(&si->pdev->dev, sizeof(*rfse) + RFSE_ALIGN,
-			  tmp, dma);
+	enetc_cbd_free_data_mem(si, sizeof(*rfse), tmp, &dma);
 
 	return err;
 }
+//EXPORT_SYMBOL_GPL(ec_enetc_set_fs_entry);
 
-#define RSSE_ALIGN	64
 static int enetc_cmd_rss_table(struct enetc_si *si, u32 *table, int count,
 			       bool read)
 {
+	struct enetc_cbdr *ring = &si->cbd_ring;
 	struct enetc_cbd cbd = {.cmd = 0};
-	dma_addr_t dma, dma_align;
 	u8 *tmp, *tmp_align;
+	dma_addr_t dma;
 	int err, i;
 
-	if (count < RSSE_ALIGN)
+	if (count < ENETC_CBD_DATA_MEM_ALIGN)
 		/* HW only takes in a full 64 entry table */
 		return -EINVAL;
 
-	tmp = dma_alloc_coherent(&si->pdev->dev, count + RSSE_ALIGN,
-				 &dma, GFP_KERNEL);
-	if (!tmp) {
-		dev_err(&si->pdev->dev, "DMA mapping of RSS table failed!\n");
+	tmp = enetc_cbd_alloc_data_mem(si, &cbd, count,
+				       &dma, (void *)&tmp_align);
+	if (!tmp)
 		return -ENOMEM;
-	}
-	dma_align = ALIGN(dma, RSSE_ALIGN);
-	tmp_align = PTR_ALIGN(tmp, RSSE_ALIGN);
 
 	if (!read)
 		for (i = 0; i < count; i++)
@@ -182,32 +281,30 @@ static int enetc_cmd_rss_table(struct enetc_si *si, u32 *table, int count,
 	/* fill up the descriptor */
 	cbd.cmd = read ? 2 : 1;
 	cbd.cls = 3;
-	cbd.length = cpu_to_le16(count);
-
-	cbd.addr[0] = cpu_to_le32(lower_32_bits(dma_align));
-	cbd.addr[1] = cpu_to_le32(upper_32_bits(dma_align));
 
-	err = enetc_send_cmd(si, &cbd);
+	err = ec_enetc_send_cmd(si, &cbd);
 	if (err)
-		dev_err(&si->pdev->dev, "RSS cmd failed (%d)!", err);
+		dev_err(ring->dma_dev, "RSS cmd failed (%d)!", err);
 
 	if (read)
 		for (i = 0; i < count; i++)
 			table[i] = tmp_align[i];
 
-	dma_free_coherent(&si->pdev->dev, count + RSSE_ALIGN, tmp, dma);
+	enetc_cbd_free_data_mem(si, count, tmp, &dma);
 
 	return err;
 }
 
 /* Get RSS table */
-int enetc_get_rss_table(struct enetc_si *si, u32 *table, int count)
+int ec_enetc_get_rss_table(struct enetc_si *si, u32 *table, int count)
 {
 	return enetc_cmd_rss_table(si, table, count, true);
 }
+//EXPORT_SYMBOL_GPL(ec_enetc_get_rss_table);
 
 /* Set RSS table */
-int enetc_set_rss_table(struct enetc_si *si, const u32 *table, int count)
+int ec_enetc_set_rss_table(struct enetc_si *si, const u32 *table, int count)
 {
 	return enetc_cmd_rss_table(si, (u32 *)table, count, false);
 }
+//EXPORT_SYMBOL_GPL(ec_enetc_set_rss_table);
diff --git a/devices/enetc/enetc_debugfs.c b/devices/enetc/enetc_debugfs.c
new file mode 100755
index 0000000..9191673
--- /dev/null
+++ b/devices/enetc/enetc_debugfs.c
@@ -0,0 +1,463 @@
+// SPDX-License-Identifier: GPL-2.0+
+/* Copyright 2023 NXP
+ */
+#include <linux/device.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+
+#include "enetc_pf.h"
+
+#define DEFINE_ENETC_DEBUGFS(name, write_op)			\
+static int name##_open(struct inode *inode, struct file *file)		\
+{									\
+	return single_open(file, name##_show, inode->i_private);	\
+}									\
+									\
+static const struct file_operations name##_fops = {			\
+	.owner		= THIS_MODULE,					\
+	.open		= name##_open,					\
+	.read		= seq_read,					\
+	.write		= enetc_##write_op##_write,			\
+	.llseek		= seq_lseek,					\
+	.release	= single_release,				\
+}
+
+static int enetc_tgst_show(struct seq_file *s, void *data)
+{
+	struct enetc_si *si = s->private;
+	int port;
+	u32 val;
+
+	val = enetc_port_rd(&si->hw, ENETC4_PTGSCR);
+	if (!(val & PTGSCR_TGE)) {
+		seq_puts(s, "Time Gating Disable\n");
+		return 0;
+	}
+
+	port = enetc4_pf_to_port(si->pdev);
+	if (port < 0)
+		return -EINVAL;
+
+	return netc_show_tgst_entry(&si->ntmp, s, port);
+}
+DEFINE_SHOW_ATTRIBUTE(enetc_tgst);
+
+static void enetc_get_txq_config(struct seq_file *s, struct enetc_si *si, int index)
+{
+	u32 wrr, prio, val;
+	bool en, fwb, vih;
+
+	val = enetc_txbdr_rd(&si->hw, index, ENETC_TBMR);
+	en = !!(val & ENETC_TBMR_EN);
+	fwb = !!(val & ENETC_TBMR_FWB);
+	vih = !!(val & ENETC_TBMR_VIH);
+	wrr = (val & ENETC_TBMR_WRR) >> 4;
+	prio = val & ENETC_TBMR_PRIO_MASK;
+
+	seq_printf(s, "txq %d:%s Force Writeback:%s VLAN Insert:%s WRR:%u Priority:%u\n",
+		   index, is_en(en), is_en(fwb), is_en(vih), wrr, prio);
+}
+
+static int enetc_txq_show(struct seq_file *s, void *data)
+{
+	struct enetc_si *si = s->private;
+	struct net_device *ndev;
+	u32 txq_num;
+	u8 tc_num;
+	int i, j;
+
+	ndev = si->ndev;
+	txq_num = ndev->real_num_tx_queues;
+	tc_num = ndev->num_tc;
+
+	seq_printf(s, "%s txq number:%u, tc num:%u\n", netdev_name(ndev), txq_num, tc_num);
+	/* TC MQPRIO HW OFFLOAD */
+	if (tc_num) {
+		for (i = 0; i < tc_num; i++) {
+			int offset, count, end;
+
+			offset = ndev->tc_to_txq[i].offset;
+			count = ndev->tc_to_txq[i].count;
+			end = offset + count - 1;
+			seq_printf(s, "txq %d to %d map to traffic class %d\n", offset, end, i);
+
+			for (j = 0; j < count; j++)
+				enetc_get_txq_config(s, si, offset + j);
+		}
+	} else {
+		for (i = 0; i < txq_num; i++)
+			enetc_get_txq_config(s, si, i);
+	}
+
+	return 0;
+}
+DEFINE_SHOW_ATTRIBUTE(enetc_txq);
+
+static void enetc_show_si_mac_hash_filter(struct seq_file *s, struct enetc_hw *hw, int si)
+{
+	u32 hash_h, hash_l;
+
+	hash_l = enetc_port_rd(hw, ENETC4_PSIUMHFR0(si));
+	hash_h = enetc_port_rd(hw, ENETC4_PSIUMHFR1(si));
+	seq_printf(s, "SI%d unicast MAC hash filter:0x%08x%08x\n", si, hash_h, hash_l);
+
+	hash_l = enetc_port_rd(hw, ENETC4_PSIMMHFR0(si));
+	hash_h = enetc_port_rd(hw, ENETC4_PSIMMHFR1(si));
+	seq_printf(s, "SI%d multicast MAC hash filter:0x%08x%08x\n", si, hash_h, hash_l);
+}
+
+static int enetc_rx_mode_show(struct seq_file *s, void *data)
+{
+	struct maft_entry_data maft_data;
+	struct enetc_si *si = s->private;
+	struct enetc_hw *hw = &si->hw;
+	struct maft_keye_data *keye;
+	struct enetc_pf *pf;
+	int i, err;
+	u32 val;
+
+	pf = enetc_si_priv(si);
+
+	val = enetc_port_rd(hw, ENETC4_PSIPMMR);
+	seq_printf(s, "Unicast Promisc:0x%lx. Multicast Promisc:0x%lx\n",
+		   val & PSIPMMR_SI_MAC_UP, (val & PSIPMMR_SI_MAC_MP) >> 16);
+
+	/* Use MAC hash filter */
+	if (!pf->num_mac_fe) {
+		for (i = 0; i < pf->num_vfs + 1; i++)
+			enetc_show_si_mac_hash_filter(s, hw, i);
+
+		return 0;
+	}
+
+	seq_printf(s, "The total number of entries in MAC filter table is %d\n",
+		   pf->num_mac_fe);
+	/* Use MAC exact match table */
+	for (i = 0; i < pf->num_mac_fe; i++) {
+		memset(&maft_data, 0, sizeof(maft_data));
+		err = ntmp_maft_query_entry(&si->ntmp.cbdrs, i, &maft_data);
+		if (err)
+			return err;
+
+		keye = &maft_data.keye;
+		seq_printf(s, "Entry %d, MAC %pM, SI bitmap:0x%04x\n", i,
+			   keye->mac_addr, le16_to_cpu(maft_data.cfge.si_bitmap));
+	}
+
+	for (i = 0; i < pf->num_vfs; i++)
+		enetc_show_si_mac_hash_filter(s, hw, i + 1);
+
+	return 0;
+}
+DEFINE_SHOW_ATTRIBUTE(enetc_rx_mode);
+
+static int enetc_flower_list_show(struct seq_file *s, void *data)
+{
+	struct enetc_si *si = s->private;
+	struct netc_flower_rule *rule;
+
+	guard(mutex)(&si->ntmp.flower_lock);
+	hlist_for_each_entry(rule, &si->ntmp.flower_list, node) {
+		seq_printf(s, "Cookie:0x%lx\n", rule->cookie);
+		seq_printf(s, "Flower type:%d\n", rule->flower_type);
+
+		if (rule->flower_type == FLOWER_TYPE_PSFP)
+			netc_show_psfp_flower(s, rule);
+
+		seq_puts(s, "\n");
+	}
+
+	return 0;
+}
+DEFINE_SHOW_ATTRIBUTE(enetc_flower_list);
+
+static ssize_t enetc_isit_eid_write(struct file *filp, const char __user *buffer,
+				    size_t count, loff_t *ppos)
+{
+	struct seq_file *s = filp->private_data;
+	struct enetc_si *si = s->private;
+
+	return netc_kstrtouint(buffer, count, ppos, &si->dbg_params.isit_eid);
+}
+
+static int enetc_isit_entry_show(struct seq_file *s, void *data)
+{
+	struct enetc_si *si = s->private;
+
+	return netc_show_isit_entry(&si->ntmp, s, si->dbg_params.isit_eid);
+}
+
+DEFINE_ENETC_DEBUGFS(enetc_isit_entry, isit_eid);
+
+static ssize_t enetc_ist_eid_write(struct file *filp, const char __user *buffer,
+				   size_t count, loff_t *ppos)
+{
+	struct seq_file *s = filp->private_data;
+	struct enetc_si *si = s->private;
+
+	return netc_kstrtouint(buffer, count, ppos, &si->dbg_params.ist_eid);
+}
+
+static int enetc_ist_entry_show(struct seq_file *s, void *data)
+{
+	struct enetc_si *si = s->private;
+
+	return netc_show_ist_entry(&si->ntmp, s, si->dbg_params.ist_eid);
+}
+
+DEFINE_ENETC_DEBUGFS(enetc_ist_entry, ist_eid);
+
+static ssize_t enetc_isft_eid_write(struct file *filp, const char __user *buffer,
+				    size_t count, loff_t *ppos)
+{
+	struct seq_file *s = filp->private_data;
+	struct enetc_si *si = s->private;
+
+	return netc_kstrtouint(buffer, count, ppos, &si->dbg_params.isft_eid);
+}
+
+static int enetc_isft_entry_show(struct seq_file *s, void *data)
+{
+	struct enetc_si *si = s->private;
+
+	return netc_show_isft_entry(&si->ntmp, s, si->dbg_params.isft_eid);
+}
+
+DEFINE_ENETC_DEBUGFS(enetc_isft_entry, isft_eid);
+
+static ssize_t enetc_sgit_eid_write(struct file *filp, const char __user *buffer,
+				    size_t count, loff_t *ppos)
+{
+	struct seq_file *s = filp->private_data;
+	struct enetc_si *si = s->private;
+
+	return netc_kstrtouint(buffer, count, ppos, &si->dbg_params.sgit_eid);
+}
+
+static int enetc_sgit_entry_show(struct seq_file *s, void *data)
+{
+	struct enetc_si *si = s->private;
+
+	return netc_show_sgit_entry(&si->ntmp, s, si->dbg_params.sgit_eid);
+}
+
+DEFINE_ENETC_DEBUGFS(enetc_sgit_entry, sgit_eid);
+
+static ssize_t enetc_sgclt_eid_write(struct file *filp, const char __user *buffer,
+				     size_t count, loff_t *ppos)
+{
+	struct seq_file *s = filp->private_data;
+	struct enetc_si *si = s->private;
+
+	return netc_kstrtouint(buffer, count, ppos, &si->dbg_params.sgclt_eid);
+}
+
+static int enetc_sgclt_entry_show(struct seq_file *s, void *data)
+{
+	struct enetc_si *si = s->private;
+
+	return netc_show_sgclt_entry(&si->ntmp, s, si->dbg_params.sgclt_eid);
+}
+
+DEFINE_ENETC_DEBUGFS(enetc_sgclt_entry, sgclt_eid);
+
+static ssize_t enetc_isct_eid_write(struct file *filp, const char __user *buffer,
+				    size_t count, loff_t *ppos)
+{
+	struct seq_file *s = filp->private_data;
+	struct enetc_si *si = s->private;
+
+	return netc_kstrtouint(buffer, count, ppos, &si->dbg_params.isct_eid);
+}
+
+static int enetc_isct_entry_show(struct seq_file *s, void *data)
+{
+	struct enetc_si *si = s->private;
+
+	return netc_show_isct_entry(&si->ntmp, s, si->dbg_params.isct_eid);
+}
+
+DEFINE_ENETC_DEBUGFS(enetc_isct_entry, isct_eid);
+
+static ssize_t enetc_rpt_eid_write(struct file *filp, const char __user *buffer,
+				   size_t count, loff_t *ppos)
+{
+	struct seq_file *s = filp->private_data;
+	struct enetc_si *si = s->private;
+
+	return netc_kstrtouint(buffer, count, ppos, &si->dbg_params.rpt_eid);
+}
+
+static int enetc_rpt_entry_show(struct seq_file *s, void *data)
+{
+	struct enetc_si *si = s->private;
+
+	return netc_show_rpt_entry(&si->ntmp, s, si->dbg_params.rpt_eid);
+}
+
+DEFINE_ENETC_DEBUGFS(enetc_rpt_entry, rpt_eid);
+
+static int enetc_ipft_show(struct seq_file *s, void *data)
+{
+	struct enetc_si *si = s->private;
+	struct enetc_hw *hw = &si->hw;
+	struct enetc_ndev_priv *priv;
+	int i, err;
+	u32 val;
+
+	val = enetc_port_rd(hw, ENETC4_PIPFCR);
+	seq_printf(s, "Ingress port filter is %s\n", is_en(val & PIPFCR_EN));
+	val = enetc_port_rd(hw, ENETC4_IPFTCAPR) & IPFTCAPR_NUM_WORDS;
+	seq_printf(s, "Number of ternary words supported: %u\n", val);
+	val = enetc_port_rd(hw, ENETC4_IPFTMOR) & IPFTMOR_NUM_WORDS;
+	seq_printf(s, "Number of words in-use: %u\n", val);
+
+	priv = netdev_priv(si->ndev);
+	for (i = 0; i < priv->max_ipf_entries; i++) {
+		if (priv->cls_rules[i].used) {
+			err = netc_show_ipft_entry(&si->ntmp, s,
+						   priv->cls_rules[i].entry_id);
+			if (err)
+				return err;
+		}
+	}
+
+	return 0;
+}
+DEFINE_SHOW_ATTRIBUTE(enetc_ipft);
+
+static void enetc_txr_bd_show(struct seq_file *s, union enetc_tx_bd *txbd, int index)
+{
+	__le32 *p = (__le32 *)txbd;
+
+	seq_printf(s, "TX BD%d: %08x %08x %08x %08x\n",
+		   index, le32_to_cpu(*p),
+		   le32_to_cpu(*(p + 1)),
+		   le32_to_cpu(*(p + 2)),
+		   le32_to_cpu(*(p + 3)));
+}
+
+static int enetc_txr_show(struct seq_file *s, void *data)
+{
+	struct enetc_si *si = s->private;
+	struct enetc_hw *hw = &si->hw;
+	struct enetc_ndev_priv *priv;
+	int i, j;
+	u32 val;
+
+	priv = netdev_priv(si->ndev);
+
+	for (i = 0; i < priv->num_tx_rings; i++) {
+		struct enetc_bdr *txr = priv->tx_ring[i];
+		union enetc_tx_bd *txbd;
+
+		seq_printf(s, "TX Ring%d Info:\n", i);
+		seq_printf(s, "SW next_to_clean:%d\n", txr->next_to_clean);
+		seq_printf(s, "SW next_to_use:%d\n", txr->next_to_use);
+
+		val = enetc_txbdr_rd(hw, i, ENETC_TBCIR);
+		seq_printf(s, "HW CIR:%u\n", val);
+		val = enetc_txbdr_rd(hw, i, ENETC_TBPIR);
+		seq_printf(s, "HW PIR:%u\n", val);
+		val = enetc_txbdr_rd(hw, i, ENETC_TBMR);
+		seq_printf(s, "TX BDR mode:0x%x\n", val);
+
+		if (!netif_running(si->ndev))
+			continue;
+
+		for (j = 0; j < txr->bd_count; j++) {
+			txbd = ENETC_TXBD(*txr, j);
+			enetc_txr_bd_show(s, txbd, j);
+		}
+	}
+
+	return 0;
+}
+DEFINE_SHOW_ATTRIBUTE(enetc_txr);
+
+static void enetc_rxr_bd_show(struct seq_file *s, union enetc_rx_bd *rxbd, int index)
+{
+	__le32 *p = (__le32 *)rxbd;
+
+	seq_printf(s, "RX BD%d: %08x %08x %08x %08x\n",
+		   index, le32_to_cpu(*p),
+		   le32_to_cpu(*(p + 1)),
+		   le32_to_cpu(*(p + 2)),
+		   le32_to_cpu(*(p + 3)));
+}
+
+static int enetc_rxr_show(struct seq_file *s, void *data)
+{
+	struct enetc_si *si = s->private;
+	struct enetc_hw *hw = &si->hw;
+	struct enetc_ndev_priv *priv;
+	int i, j;
+	u32 val;
+
+	priv = netdev_priv(si->ndev);
+
+	for (i = 0; i < priv->num_rx_rings; i++) {
+		struct enetc_bdr *rxr = priv->rx_ring[i];
+		union enetc_rx_bd *rxbd;
+
+		seq_printf(s, "RX Ring%d Info:\n", i);
+		seq_printf(s, "Extended RX BD: %s\n", is_en(rxr->ext_en));
+		seq_printf(s, "SW next_to_clean:%d\n", rxr->next_to_clean);
+		seq_printf(s, "SW next_to_use:%d\n", rxr->next_to_use);
+		seq_printf(s, "SW next_to_alloc:%d\n", rxr->next_to_alloc);
+
+		val = enetc_rxbdr_rd(hw, i, ENETC_RBCIR);
+		seq_printf(s, "HW CIR:%u\n", val);
+		val = enetc_rxbdr_rd(hw, i, ENETC_RBPIR);
+		seq_printf(s, "HW PIR:%u\n", val);
+		val = enetc_rxbdr_rd(hw, i, ENETC_RBMR);
+		seq_printf(s, "RX BDR mode:0x%x\n", val);
+
+		if (!netif_running(si->ndev))
+			continue;
+
+		for (j = 0; j < rxr->bd_count; j++) {
+			rxbd = &((union enetc_rx_bd *)rxr->bd_base)[j];
+			enetc_rxr_bd_show(s, rxbd, j);
+		}
+	}
+
+	return 0;
+}
+DEFINE_SHOW_ATTRIBUTE(enetc_rxr);
+
+void enetc_create_debugfs(struct enetc_si *si)
+{
+	struct net_device *ndev = si->ndev;
+	struct dentry *root;
+
+	root = debugfs_create_dir(netdev_name(ndev), NULL);
+	if (IS_ERR(root))
+		return;
+
+	si->debugfs_root = root;
+
+	if (si->hw.port) {
+		debugfs_create_file("enetc_tgst", 0444, root, si, &enetc_tgst_fops);
+		debugfs_create_file("rx_mode", 0444, root, si, &enetc_rx_mode_fops);
+		debugfs_create_file("isit_entry", 0600, root, si, &enetc_isit_entry_fops);
+		debugfs_create_file("ist_entry", 0600, root, si, &enetc_ist_entry_fops);
+		debugfs_create_file("isft_entry", 0600, root, si, &enetc_isft_entry_fops);
+		debugfs_create_file("sgit_entry", 0600, root, si, &enetc_sgit_entry_fops);
+		debugfs_create_file("sgclt_entry", 0600, root, si, &enetc_sgclt_entry_fops);
+		debugfs_create_file("isct_entry", 0600, root, si, &enetc_isct_entry_fops);
+		debugfs_create_file("rpt_entry", 0600, root, si, &enetc_rpt_entry_fops);
+		debugfs_create_file("flower_list", 0444, root, si, &enetc_flower_list_fops);
+		debugfs_create_file("enetc_ipft", 0444, root, si, &enetc_ipft_fops);
+	}
+
+	debugfs_create_file("enetc_txq", 0444, root, si, &enetc_txq_fops);
+	debugfs_create_file("tx_bdr", 0444, root, si, &enetc_txr_fops);
+	debugfs_create_file("rx_bdr", 0444, root, si, &enetc_rxr_fops);
+}
+
+void enetc_remove_debugfs(struct enetc_si *si)
+{
+	debugfs_remove_recursive(si->debugfs_root);
+	si->debugfs_root = NULL;
+}
diff --git a/devices/enetc/enetc_devlink.c b/devices/enetc/enetc_devlink.c
new file mode 100755
index 0000000..3fcd0ef
--- /dev/null
+++ b/devices/enetc/enetc_devlink.c
@@ -0,0 +1,256 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+/* Copyright 2024 NXP */
+#include "enetc_pf.h"
+
+#define ENETC_RINGS_STR_LEN		16
+#define ENETC_RINGS_STR_BUFF_LEN	(ENETC_RINGS_STR_LEN + 1)
+#define ENETC_SI_MAX_RINGS_NUM		32
+
+static int enetc_devlink_reload_down(struct devlink *devlink, bool netns_change,
+				     enum devlink_reload_action action,
+				     enum devlink_reload_limit limit,
+				     struct netlink_ext_ack *extack)
+{
+	struct enetc_devlink_priv *devl_priv = devlink_priv(devlink);
+	struct enetc_pf *pf = devl_priv->pf;
+
+	switch (action) {
+	case DEVLINK_RELOAD_ACTION_DRIVER_REINIT:
+		if (pf->num_vfs) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "Remove all VFs before doing reinit\n");
+			return -EOPNOTSUPP;
+		}
+
+		if (devl_priv->pf_unload)
+			return devl_priv->pf_unload(pf);
+		else
+			return -EOPNOTSUPP;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static int enetc_devlink_parse_rings_param(union devlink_param_value *val,
+					   u32 *si_num_rings, u32 *ring_cnt)
+{
+	char str_buff[ENETC_RINGS_STR_BUFF_LEN];
+	int str_len, num_step;
+	int i, j, k, err;
+	u64 param_val;
+	u32 ring_num;
+	int pos, len;
+
+	str_len = strnlen(val->vstr, sizeof(val->vstr));
+	num_step = str_len / ENETC_RINGS_STR_LEN;
+	if (str_len % ENETC_RINGS_STR_LEN)
+		num_step += 1;
+
+	for (i = num_step - 1, k = 0; i >= 0; i--) {
+		pos = i ? str_len - ENETC_RINGS_STR_LEN : 0;
+		len = i ? ENETC_RINGS_STR_BUFF_LEN : str_len + 1;
+		strscpy(str_buff, &val->vstr[pos], len);
+
+		err = kstrtoull(str_buff, 16, &param_val);
+		if (err)
+			return err;
+
+		for (j = 0; j < 8 && k < ENETC_MAX_VF_NUM; j++) {
+			ring_num = param_val & 0xff;
+			si_num_rings[k++] = ring_num;
+			*ring_cnt += ring_num;
+			param_val >>= 8;
+		}
+
+		str_len -= ENETC_RINGS_STR_LEN;
+	}
+
+	return 0;
+}
+
+static void enetc_devlink_get_rings_param(struct devlink *devlink)
+{
+	struct enetc_devlink_priv *devl_priv = devlink_priv(devlink);
+	struct enetc_pf *pf = devl_priv->pf;
+	u32 rings_cap = pf->caps.num_rx_bdr;
+	union devlink_param_value val;
+	u32 ring_num, ring_cnt = 0;
+	int vf_base_id = 1;
+	int str_len, err;
+
+	err = devl_param_driverinit_value_get(devlink,
+					      ENETC_DEVLINK_PARAM_ID_RING_NUM_ASSIGN,
+					      &val);
+	if (err)
+		goto clear_si_num_rings;
+
+	str_len = strnlen(val.vstr, sizeof(val.vstr));
+	if (!str_len)
+		goto clear_si_num_rings;
+
+	err = enetc_devlink_parse_rings_param(&val, &devl_priv->si_num_rings[vf_base_id],
+					      &ring_cnt);
+	if (err)
+		goto clear_si_num_rings;
+
+	/* Calculate the number of rings of PF */
+	ring_num = rings_cap - ring_cnt;
+	if (ring_num > ENETC_SI_MAX_RINGS_NUM)
+		ring_num = ENETC_SI_MAX_RINGS_NUM;
+
+	devl_priv->si_num_rings[0] = ring_num;
+
+	return;
+
+clear_si_num_rings:
+	memset(devl_priv->si_num_rings, 0, sizeof(devl_priv->si_num_rings));
+}
+
+static void enetc_devlink_get_params(struct devlink *devlink)
+{
+	devl_assert_locked(devlink);
+
+	enetc_devlink_get_rings_param(devlink);
+}
+
+static int enetc_devlink_reload_up(struct devlink *devlink,
+				   enum devlink_reload_action action,
+				   enum devlink_reload_limit limit,
+				   u32 *actions_performed,
+				   struct netlink_ext_ack *extack)
+{
+	struct enetc_devlink_priv *devl_priv = devlink_priv(devlink);
+	struct enetc_pf *pf = devl_priv->pf;
+
+	switch (action) {
+	case DEVLINK_RELOAD_ACTION_DRIVER_REINIT:
+		*actions_performed = BIT(DEVLINK_RELOAD_ACTION_DRIVER_REINIT);
+		enetc_devlink_get_params(devlink);
+
+		if (devl_priv->pf_load)
+			return devl_priv->pf_load(pf);
+		else
+			return -EOPNOTSUPP;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static const struct devlink_ops enetc_devlink_ops = {
+	.reload_actions = BIT(DEVLINK_RELOAD_ACTION_DRIVER_REINIT),
+	.reload_down = enetc_devlink_reload_down,
+	.reload_up = enetc_devlink_reload_up,
+};
+
+static void enetc_devlink_free(void *devlink)
+{
+	devlink_free((struct devlink *)devlink);
+}
+
+int enetc_devlink_alloc(struct enetc_pf *pf)
+{
+	struct device *dev = &pf->si->pdev->dev;
+	struct enetc_devlink_priv *devl_priv;
+	struct devlink *devlink;
+	int err;
+
+	devlink = devlink_alloc(&enetc_devlink_ops,
+				sizeof(struct enetc_devlink_priv), dev);
+	if (!devlink)
+		return -ENOMEM;
+
+	/* Add an action to teardown the devlink when unwinding the driver */
+	err = devm_add_action_or_reset(dev, enetc_devlink_free, devlink);
+	if (err)
+		return err;
+
+	devl_priv = devlink_priv(devlink);
+	devl_priv->pf = pf;
+	pf->devl_priv = devl_priv;
+
+	return 0;
+}
+
+void enetc_devlink_init_params(struct devlink *devlink)
+{
+	struct enetc_devlink_priv *devl_priv = devlink_priv(devlink);
+	union devlink_param_value val;
+
+	devl_lock(devlink);
+	memset(&val, 0, sizeof(val));
+	memset(devl_priv->si_num_rings, 0, sizeof(devl_priv->si_num_rings));
+	devl_param_driverinit_value_set(devlink,
+					ENETC_DEVLINK_PARAM_ID_RING_NUM_ASSIGN,
+					val);
+	devl_unlock(devlink);
+}
+
+static int enetc_devlink_ring_num_validate(struct devlink *devlink, u32 id,
+					   union devlink_param_value val,
+					   struct netlink_ext_ack *extack)
+{
+	struct enetc_devlink_priv *devl_priv = devlink_priv(devlink);
+	u32 vf_num_rings[ENETC_MAX_VF_NUM] = {0};
+	struct enetc_pf *pf = devl_priv->pf;
+	u32 rings_cap = pf->caps.num_rx_bdr;
+	u32 num_vf = pf->caps.num_vsi;
+	u32 ring_cnt = 0;
+	int i, err;
+
+	/* Parse string parameter */
+	err = enetc_devlink_parse_rings_param(&val, vf_num_rings, &ring_cnt);
+	if (err) {
+		NL_SET_ERR_MSG_MOD(extack, "Parameter format is error");
+		return -EINVAL;
+	}
+
+	for (i = 0; i < ENETC_MAX_VF_NUM; i++) {
+		if (i < num_vf) {
+			if (!vf_num_rings[i]) {
+				NL_SET_ERR_MSG_MOD(extack, "Rings number is at least 1");
+				return -EINVAL;
+			} else if (vf_num_rings[i] > ENETC_SI_MAX_RINGS_NUM) {
+				NL_SET_ERR_MSG_FMT_MOD(extack, "Maximum number is %d",
+						       ENETC_SI_MAX_RINGS_NUM);
+				return -EINVAL;
+			}
+		} else {
+			if (vf_num_rings[i]) {
+				NL_SET_ERR_MSG_MOD(extack,
+						   "Cannot allocate rings to inexistent VF");
+				return -EINVAL;
+			}
+		}
+	}
+
+	if (rings_cap < ring_cnt) {
+		NL_SET_ERR_MSG_MOD(extack, "Rings number exceeds hardware capability");
+		return -EINVAL;
+	}
+
+	if (rings_cap == ring_cnt) {
+		NL_SET_ERR_MSG_MOD(extack, "No rings left for PF");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static const struct devlink_param enetc_devlink_params[] = {
+	DEVLINK_PARAM_DRIVER(ENETC_DEVLINK_PARAM_ID_RING_NUM_ASSIGN,
+			     "si_num_rings", DEVLINK_PARAM_TYPE_STRING,
+			     BIT(DEVLINK_PARAM_CMODE_DRIVERINIT),
+			     NULL, NULL, enetc_devlink_ring_num_validate),
+};
+
+int enetc_devlink_params_register(struct devlink *devlink)
+{
+	return devlink_params_register(devlink, enetc_devlink_params,
+				       ARRAY_SIZE(enetc_devlink_params));
+}
+
+void enetc_devlink_params_unregister(struct devlink *devlink)
+{
+	devlink_params_unregister(devlink, enetc_devlink_params,
+				  ARRAY_SIZE(enetc_devlink_params));
+}
diff --git a/devices/enetc/enetc_devlink.h b/devices/enetc/enetc_devlink.h
new file mode 100755
index 0000000..7dce77a
--- /dev/null
+++ b/devices/enetc/enetc_devlink.h
@@ -0,0 +1,30 @@
+/* SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause) */
+/* Copyright 2024 NXP */
+#ifndef __ENETC_DEVLINK_H__
+#define __ENETC_DEVLINK_H__
+
+#include <net/devlink.h>
+
+#define ENETC_MAX_VF_NUM	15
+#define ENETC_MAX_SI_NUM	(ENETC_MAX_VF_NUM + 1)
+
+struct enetc_pf;
+struct enetc_devlink_priv {
+	struct enetc_pf *pf;
+	u32 si_num_rings[ENETC_MAX_SI_NUM];
+
+	int (*pf_load)(struct enetc_pf *pf);
+	int (*pf_unload)(struct enetc_pf *pf);
+};
+
+enum enetc_devlink_param_id {
+	ENETC_DEVLINK_PARAM_ID_BASE = DEVLINK_PARAM_GENERIC_ID_MAX,
+	ENETC_DEVLINK_PARAM_ID_RING_NUM_ASSIGN,
+};
+
+int enetc_devlink_params_register(struct devlink *devlink);
+void enetc_devlink_params_unregister(struct devlink *devlink);
+void enetc_devlink_init_params(struct devlink *devlink);
+int enetc_devlink_alloc(struct enetc_pf *pf);
+
+#endif /* __ENETC_DEVLINK_H__ */
diff --git a/devices/enetc/enetc_ethtool.c b/devices/enetc/enetc_ethtool.c
index 974256a..12c91fb 100644
--- a/devices/enetc/enetc_ethtool.c
+++ b/devices/enetc/enetc_ethtool.c
@@ -1,9 +1,13 @@
 // SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
 /* Copyright 2017-2019 NXP */
 
+#include <linux/ethtool_netlink.h>
+#include <linux/fsl/netc_global.h>
 #include <linux/net_tstamp.h>
 #include <linux/module.h>
-#include "enetc.h"
+#include <linux/of.h>
+
+#include "enetc_pf.h"
 
 static const u32 enetc_si_regs[] = {
 	ENETC_SIMR, ENETC_SIPMAR0, ENETC_SIPMAR1, ENETC_SICBDRMR,
@@ -49,6 +53,45 @@ static int enetc_get_reglen(struct net_device *ndev)
 	return len;
 }
 
+static int enetc_us_to_tx_cycle(struct net_device *dev, u32 *us)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(dev);
+    u32 cycle, max_us;
+
+    max_us = enetc_cycles_to_usecs(PM_EEE_TIMER, priv->si->clk_freq);
+    if (*us > max_us) {
+        netdev_info(dev, "ENETC supports maximum tx_lpi_timer: %uus, using %uus instead.\n",
+                max_us, max_us);
+        *us = max_us;
+    }
+    cycle = enetc_usecs_to_cycles(*us, priv->si->clk_freq);
+
+    return cycle;
+}
+
+void ec_enetc_eee_mode_set(struct net_device *dev, bool enable)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(dev);
+    unsigned int sleep_cycle = 0, wake_cycle = 0;
+    struct ethtool_keee *eee = &priv->eee;
+    struct enetc_si *si = priv->si;
+
+    if (eee->eee_active) {
+        if (enable) {
+            sleep_cycle = enetc_us_to_tx_cycle(dev, &eee->tx_lpi_timer);
+            wake_cycle = sleep_cycle;
+        } else {
+            eee->tx_lpi_timer = 0;
+        }
+        eee->eee_enabled = enable;
+    }
+    eee->tx_lpi_enabled = eee->eee_active;
+
+    ec_enetc_port_mac_wr(si, ENETC4_PM_SLEEP_TIMER(0), sleep_cycle);
+    ec_enetc_port_mac_wr(si, ENETC4_PM_LPWAKE_TIMER(0), wake_cycle);
+}
+//EXPORT_SYMBOL_GPL(ec_enetc_eee_mode_set);
+
 static void enetc_get_regs(struct net_device *ndev, struct ethtool_regs *regs,
 			   void *regbuf)
 {
@@ -252,7 +295,8 @@ static void enetc_get_strings(struct net_device *ndev, u32 stringset, u8 *data)
 	switch (stringset) {
 	case ETH_SS_STATS:
 		for (i = 0; i < ARRAY_SIZE(enetc_si_counters); i++) {
-			strlcpy(p, enetc_si_counters[i].name, ETH_GSTRING_LEN);
+			strscpy(p, enetc_port_counters[i].name,
+                    ETH_GSTRING_LEN);
 			p += ETH_GSTRING_LEN;
 		}
 		for (i = 0; i < priv->num_tx_rings; i++) {
@@ -274,8 +318,8 @@ static void enetc_get_strings(struct net_device *ndev, u32 stringset, u8 *data)
 			break;
 
 		for (i = 0; i < ARRAY_SIZE(enetc_port_counters); i++) {
-			strlcpy(p, enetc_port_counters[i].name,
-				ETH_GSTRING_LEN);
+			strscpy(p, enetc_port_counters[i].name,
+                    ETH_GSTRING_LEN);
 			p += ETH_GSTRING_LEN;
 		}
 
@@ -283,8 +327,8 @@ static void enetc_get_strings(struct net_device *ndev, u32 stringset, u8 *data)
 			break;
 
 		for (i = 0; i < ARRAY_SIZE(enetc_pmac_counters); i++) {
-			strlcpy(p, enetc_pmac_counters[i].name,
-				ETH_GSTRING_LEN);
+			strscpy(p, enetc_port_counters[i].name,
+                    ETH_GSTRING_LEN);
 			p += ETH_GSTRING_LEN;
 		}
 
@@ -448,7 +492,7 @@ l4ip4:
 		rfse.result = fs->ring_cookie;
 	}
 done:
-	return enetc_set_fs_entry(si, &rfse, fs->location);
+	return ec_enetc_set_fs_entry(si, &rfse, fs->location);
 }
 
 static int enetc_get_rxnfc(struct net_device *ndev, struct ethtool_rxnfc *rxnfc,
@@ -554,53 +598,79 @@ static u32 enetc_get_rxfh_indir_size(struct net_device *ndev)
 	return priv->si->num_rss;
 }
 
-static int enetc_get_rxfh(struct net_device *ndev, u32 *indir, u8 *key,
-			  u8 *hfunc)
+static int enetc_get_rxfh(struct net_device *ndev,
+              struct ethtool_rxfh_param *rxfh)
 {
-	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-	struct enetc_hw *hw = &priv->si->hw;
-	int err = 0, i;
-
-	/* return hash function */
-	if (hfunc)
-		*hfunc = ETH_RSS_HASH_TOP;
-
-	/* return hash key */
-	if (key && hw->port)
-		for (i = 0; i < ENETC_RSSHASH_KEY_SIZE / 4; i++)
-			((u32 *)key)[i] = enetc_port_rd(hw, ENETC_PRSSK(i));
-
-	/* return RSS table */
-	if (indir)
-		err = enetc_get_rss_table(priv->si, indir, priv->si->num_rss);
-
-	return err;
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct enetc_si *si = priv->si;
+    struct enetc_hw *hw = &si->hw;
+    int err = 0, i;
+
+    /* return hash function */
+    rxfh->hfunc = ETH_RSS_HASH_TOP;
+
+    /* return hash key */
+    if (rxfh->key && enetc_si_is_pf(si)) {
+        u32 reg_off;
+
+        for (i = 0; i < ENETC_RSSHASH_KEY_SIZE / 4; i++) {
+            if (is_enetc_rev1(si))
+                reg_off = ENETC_PRSSK(i);
+            else
+                reg_off = ENETC4_PRSSKR(i);
+
+            ((u32 *)rxfh->key)[i] = enetc_port_rd(hw, reg_off);
+        }
+    }
+
+    /* return RSS table */
+    if (rxfh->indir) {
+        if (si->get_rss_table)
+            err = si->get_rss_table(si, rxfh->indir, si->num_rss);
+        else
+            err = -EOPNOTSUPP;
+    }
+
+    return err;
 }
 
-void enetc_set_rss_key(struct enetc_hw *hw, const u8 *bytes)
+void ec_enetc_set_rss_key(struct enetc_hw *hw, const u8 *bytes)
 {
 	int i;
 
 	for (i = 0; i < ENETC_RSSHASH_KEY_SIZE / 4; i++)
 		enetc_port_wr(hw, ENETC_PRSSK(i), ((u32 *)bytes)[i]);
 }
+//EXPORT_SYMBOL_GPL(ec_enetc_set_rss_key);
 
-static int enetc_set_rxfh(struct net_device *ndev, const u32 *indir,
-			  const u8 *key, const u8 hfunc)
+static int enetc_set_rxfh(struct net_device *ndev,
+              struct ethtool_rxfh_param *rxfh,
+              struct netlink_ext_ack *extack)
 {
-	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-	struct enetc_hw *hw = &priv->si->hw;
-	int err = 0;
-
-	/* set hash key, if PF */
-	if (key && hw->port)
-		enetc_set_rss_key(hw, key);
-
-	/* set RSS table */
-	if (indir)
-		err = enetc_set_rss_table(priv->si, indir, priv->si->num_rss);
-
-	return err;
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct enetc_si *si = priv->si;
+    struct enetc_hw *hw = &si->hw;
+    int err = 0;
+
+    if (rxfh->hfunc != ETH_RSS_HASH_NO_CHANGE &&
+        rxfh->hfunc != ETH_RSS_HASH_TOP) {
+        netdev_err(ndev, "unsupported hash function\n");
+        return -EOPNOTSUPP;
+    }
+
+    /* set hash key, if PF */
+    if (rxfh->key && enetc_si_is_pf(si))
+        ec_enetc_set_rss_key(hw, rxfh->key);
+
+    /* set RSS table */
+    if (rxfh->indir) {
+        if (si->set_rss_table)
+            err = si->set_rss_table(si, rxfh->indir, si->num_rss);
+        else
+            err = -EOPNOTSUPP;
+    }
+
+    return err;
 }
 
 static void enetc_get_ringparam(struct net_device *ndev,
@@ -633,9 +703,10 @@ static int enetc_get_coalesce(struct net_device *ndev,
 {
 	struct enetc_ndev_priv *priv = netdev_priv(ndev);
 	struct enetc_int_vector *v = priv->int_vector[0];
+	u64 clk_freq = priv->si->clk_freq;
 
-	ic->tx_coalesce_usecs = enetc_cycles_to_usecs(priv->tx_ictt);
-	ic->rx_coalesce_usecs = enetc_cycles_to_usecs(v->rx_ictt);
+	ic->tx_coalesce_usecs = enetc_cycles_to_usecs(priv->tx_ictt, clk_freq);
+	ic->rx_coalesce_usecs = enetc_cycles_to_usecs(v->rx_ictt, clk_freq);
 
 	ic->tx_max_coalesced_frames = ENETC_TXIC_PKTTHR;
 	ic->rx_max_coalesced_frames = ENETC_RXIC_PKTTHR;
@@ -651,12 +722,13 @@ static int enetc_set_coalesce(struct net_device *ndev,
 			      struct netlink_ext_ack *extack)
 {
 	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	u64 clk_freq = priv->si->clk_freq;
 	u32 rx_ictt, tx_ictt;
 	int i, ic_mode;
 	bool changed;
 
-	tx_ictt = enetc_usecs_to_cycles(ic->tx_coalesce_usecs);
-	rx_ictt = enetc_usecs_to_cycles(ic->rx_coalesce_usecs);
+	tx_ictt = enetc_usecs_to_cycles(ic->tx_coalesce_usecs, clk_freq);
+	rx_ictt = enetc_usecs_to_cycles(ic->rx_coalesce_usecs, clk_freq);
 
 	if (ic->rx_max_coalesced_frames != ENETC_RXIC_PKTTHR)
 		return -EOPNOTSUPP;
@@ -691,41 +763,89 @@ static int enetc_set_coalesce(struct net_device *ndev,
 		/* reconfigure the operation mode of h/w interrupts,
 		 * traffic needs to be paused in the process
 		 */
-		enetc_stop(ndev);
-		enetc_start(ndev);
+		ec_enetc_stop(ndev);
+		ec_enetc_start(ndev);
 	}
 
 	return 0;
 }
 
-static int enetc_get_ts_info(struct net_device *ndev,
-			     struct ethtool_ts_info *info)
+static struct pci_dev *enetc4_get_default_timer_pdev(struct enetc_si *si)
 {
-	int *phc_idx;
+    int domain, bus_number, devfn;
+
+    domain = pci_domain_nr(si->pdev->bus);
+    bus_number = si->pdev->bus->number;
+    switch (si->revision) {
+    case NETC_REVISION_4_1:
+        devfn = PCI_DEVFN(24, 0);
+        break;
+    case NETC_REVISION_4_3:
+        devfn = PCI_DEVFN(0, 1);
+        break;
+    default:
+        return NULL;
+    }
+
+    return pci_get_domain_bus_and_slot(domain, bus_number, devfn);
+}
 
-	phc_idx = symbol_get(enetc_phc_index);
-	if (phc_idx) {
-		info->phc_index = *phc_idx;
-		symbol_put(enetc_phc_index);
-	} else {
-		info->phc_index = -1;
-	}
+static struct pci_dev *enetc_get_timer_pdev(struct enetc_ndev_priv *priv)
+{
+    struct fwnode_handle *timer_fwnode;
+    struct enetc_si *si = priv->si;
+    struct device_node *timer_np;
 
-#ifdef CONFIG_FSL_ENETC_PTP_CLOCK
-	info->so_timestamping = SOF_TIMESTAMPING_TX_HARDWARE |
-				SOF_TIMESTAMPING_RX_HARDWARE |
-				SOF_TIMESTAMPING_RAW_HARDWARE;
-
-	info->tx_types = (1 << HWTSTAMP_TX_OFF) |
-			 (1 << HWTSTAMP_TX_ON);
-	info->rx_filters = (1 << HWTSTAMP_FILTER_NONE) |
-			   (1 << HWTSTAMP_FILTER_ALL);
-#else
-	info->so_timestamping = SOF_TIMESTAMPING_RX_SOFTWARE |
-				SOF_TIMESTAMPING_TX_SOFTWARE |
-				SOF_TIMESTAMPING_SOFTWARE;
-#endif
-	return 0;
+    timer_np = of_parse_phandle(si->pdev->dev.of_node, "nxp,ptp-timer", 0);
+    if (!timer_np)
+        return enetc4_get_default_timer_pdev(si);
+
+    timer_fwnode = of_fwnode_handle(timer_np);
+    of_node_put(timer_np);
+    if (!timer_fwnode)
+        return NULL;
+
+    return to_pci_dev(timer_fwnode->dev);
+}
+
+static int enetc_get_ts_info(struct net_device *ndev,
+                 struct kernel_ethtool_ts_info *info)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct pci_dev *timer_pdev;
+    int *phc_idx;
+
+    if (is_enetc_rev1(priv->si)) {
+        phc_idx = symbol_get(enetc_phc_index);
+        if (phc_idx) {
+            info->phc_index = *phc_idx;
+            symbol_put(enetc_phc_index);
+        }
+    } else {
+        timer_pdev = enetc_get_timer_pdev(priv);
+        info->phc_index = netc_timer_get_phc_index(timer_pdev);
+        if (info->phc_index < 0) {
+            info->so_timestamping = SOF_TIMESTAMPING_TX_SOFTWARE;
+            return 0;
+        }
+    }
+
+    if (enetc_ptp_clock_is_enabled(priv->si)) {
+        info->so_timestamping = SOF_TIMESTAMPING_TX_HARDWARE |
+                    SOF_TIMESTAMPING_RX_HARDWARE |
+                    SOF_TIMESTAMPING_RAW_HARDWARE |
+                    SOF_TIMESTAMPING_TX_SOFTWARE;
+
+        info->tx_types = (1 << HWTSTAMP_TX_OFF) |
+                 (1 << HWTSTAMP_TX_ON) |
+                 (1 << HWTSTAMP_TX_ONESTEP_SYNC);
+        info->rx_filters = (1 << HWTSTAMP_FILTER_NONE) |
+                   (1 << HWTSTAMP_FILTER_ALL);
+    } else {
+        info->so_timestamping = SOF_TIMESTAMPING_TX_SOFTWARE;
+    }
+
+    return 0;
 }
 
 static void enetc_get_wol(struct net_device *dev,
@@ -753,6 +873,51 @@ static int enetc_set_wol(struct net_device *dev,
 	return ret;
 }
 
+
+static int enetc_ethtool_op_get_eee(struct net_device *dev,
+                    struct ethtool_keee *edata)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(dev);
+    struct enetc_si *si = priv->si;
+
+    if (is_enetc_rev1(si))
+        return -EOPNOTSUPP;
+
+    edata->eee_enabled = priv->eee.eee_enabled;
+    edata->tx_lpi_timer = priv->eee.tx_lpi_timer;
+    edata->tx_lpi_enabled = priv->eee.tx_lpi_enabled;
+
+    return phylink_ethtool_get_eee(priv->phylink, edata);
+}
+
+static int enetc_ethtool_op_set_eee(struct net_device *dev,
+                    struct ethtool_keee *edata)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(dev);
+    struct ethtool_keee *eee = &priv->eee;
+
+    if (is_enetc_rev1(priv->si))
+        return -EOPNOTSUPP;
+
+    if (!netif_running(dev))
+        return -ENETDOWN;
+
+    eee->tx_lpi_timer = edata->tx_lpi_timer;
+
+    if (!edata->eee_enabled || !edata->tx_lpi_enabled ||
+        !edata->tx_lpi_timer) {
+        ec_enetc_eee_mode_set(dev, false);
+        if (edata->eee_enabled) {
+            netdev_info(dev, "Please set tx_lpi_timer at same time. EEE not enabled.\n");
+            edata->eee_enabled = false;
+        }
+    } else {
+        ec_enetc_eee_mode_set(dev, true);
+    }
+
+    return phylink_ethtool_set_eee(priv->phylink, edata);
+}
+
 static void enetc_get_pauseparam(struct net_device *dev,
 				 struct ethtool_pauseparam *pause)
 {
@@ -791,6 +956,391 @@ static int enetc_set_link_ksettings(struct net_device *dev,
 	return phylink_ethtool_ksettings_set(priv->phylink, cmd);
 }
 
+static void enetc_get_mm_stats(struct net_device *ndev,
+                   struct ethtool_mm_stats *s)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct enetc_hw *hw = &priv->si->hw;
+    struct enetc_si *si = priv->si;
+
+    if (!(si->hw_features & ENETC_SI_F_QBU))
+        return;
+
+    if (is_enetc_rev1(si)) {
+        s->MACMergeFrameAssErrorCount = enetc_port_rd(hw, ENETC_MMFAECR);
+        s->MACMergeFrameSmdErrorCount = enetc_port_rd(hw, ENETC_MMFSECR);
+        s->MACMergeFrameAssOkCount = enetc_port_rd(hw, ENETC_MMFAOCR);
+        s->MACMergeFragCountRx = enetc_port_rd(hw, ENETC_MMFCRXR);
+        s->MACMergeFragCountTx = enetc_port_rd(hw, ENETC_MMFCTXR);
+        s->MACMergeHoldCount = enetc_port_rd(hw, ENETC_MMHCR);
+    } else {
+        s->MACMergeFrameAssErrorCount = enetc_port_rd(hw, ENETC4_MMFAECR);
+        s->MACMergeFrameSmdErrorCount = enetc_port_rd(hw, ENETC4_MMFSECR);
+        s->MACMergeFrameAssOkCount = enetc_port_rd(hw, ENETC4_MMFAOCR);
+        s->MACMergeFragCountRx = enetc_port_rd(hw, ENETC4_MMFCRXR);
+        s->MACMergeFragCountTx = enetc_port_rd(hw, ENETC4_MMFCTXR);
+        s->MACMergeHoldCount = enetc_port_rd(hw, ENETC4_MMHCR);
+    }
+}
+
+static int enetc_get_mm(struct net_device *ndev, struct ethtool_mm_state *state)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct enetc_si *si = priv->si;
+    struct enetc_hw *hw = &si->hw;
+    u32 lafs, rafs, val;
+
+    if (!(si->hw_features & ENETC_SI_F_QBU))
+        return -EOPNOTSUPP;
+
+    mutex_lock(&priv->mm_lock);
+
+    if (is_enetc_rev1(si)) {
+        val = enetc_port_rd(hw, ENETC_PFPMR);
+        state->pmac_enabled = !!(val & ENETC_PFPMR_PMACE);
+
+        val = enetc_port_rd(hw, ENETC_MMCSR);
+    } else {
+        val = enetc_port_rd(hw, ENETC4_MMCSR);
+        state->pmac_enabled = !!(val & MMCSR_ME);
+    }
+
+    switch (ENETC_MMCSR_GET_VSTS(val)) {
+    case 0:
+        state->verify_status = ETHTOOL_MM_VERIFY_STATUS_DISABLED;
+        break;
+    case 2:
+        state->verify_status = ETHTOOL_MM_VERIFY_STATUS_VERIFYING;
+        break;
+    case 3:
+        state->verify_status = ETHTOOL_MM_VERIFY_STATUS_SUCCEEDED;
+        break;
+    case 4:
+        state->verify_status = ETHTOOL_MM_VERIFY_STATUS_FAILED;
+        break;
+    case 5:
+    default:
+        state->verify_status = ETHTOOL_MM_VERIFY_STATUS_UNKNOWN;
+        break;
+    }
+
+    rafs = ENETC_MMCSR_GET_RAFS(val);
+    state->tx_min_frag_size = ethtool_mm_frag_size_add_to_min(rafs);
+    lafs = ENETC_MMCSR_GET_LAFS(val);
+    state->rx_min_frag_size = ethtool_mm_frag_size_add_to_min(lafs);
+    state->tx_enabled = !!(val & ENETC_MMCSR_LPE); /* mirror of MMCSR_ME */
+    state->tx_active = state->tx_enabled &&
+               (state->verify_status == ETHTOOL_MM_VERIFY_STATUS_SUCCEEDED ||
+                state->verify_status == ETHTOOL_MM_VERIFY_STATUS_DISABLED);
+    state->verify_enabled = !(val & ENETC_MMCSR_VDIS);
+    state->verify_time = ENETC_MMCSR_GET_VT(val);
+    /* A verifyTime of 128 ms would exceed the 7 bit width
+     * of the ENETC_MMCSR_VT field
+     */
+    state->max_verify_time = 127;
+
+    mutex_unlock(&priv->mm_lock);
+
+    return 0;
+}
+
+static int enetc_mm_wait_tx_active(struct enetc_hw *hw, int verify_time)
+{
+    int timeout = verify_time * USEC_PER_MSEC * ENETC_MM_VERIFY_RETRIES;
+    u32 val;
+
+    /* This will time out after the standard value of 3 verification
+     * attempts. To not sleep forever, it relies on a non-zero verify_time,
+     * guarantee which is provided by the ethtool nlattr policy.
+     */
+    return read_poll_timeout(enetc_port_rd, val,
+                 ENETC_MMCSR_GET_VSTS(val) == 3,
+                 ENETC_MM_VERIFY_SLEEP_US, timeout,
+                 true, hw, ENETC_MMCSR);
+}
+
+static int enetc4_mm_wait_tx_active(struct enetc_hw *hw, int verify_time)
+{
+    int timeout = verify_time * USEC_PER_MSEC * ENETC_MM_VERIFY_RETRIES;
+    u32 val;
+
+    return read_poll_timeout(enetc_port_rd, val,
+                 MMCSR_GET_VSTS(val) == MMCSR_VSTS_SUCCESSFUL,
+                 ENETC_MM_VERIFY_SLEEP_US, timeout,
+                 true, hw, ENETC4_MMCSR);
+}
+
+static void enetc_set_ptcfpr(struct enetc_hw *hw, u8 preemptible_tcs)
+{
+    u32 val;
+    int tc;
+
+    for (tc = 0; tc < 8; tc++) {
+        val = enetc_port_rd(hw, ENETC_PTCFPR(tc));
+
+        if (preemptible_tcs & BIT(tc))
+            val |= ENETC_PTCFPR_FPE;
+        else
+            val &= ~ENETC_PTCFPR_FPE;
+
+        enetc_port_wr(hw, ENETC_PTCFPR(tc), val);
+    }
+}
+
+static void enetc4_set_pfpcr(struct enetc_hw *hw, u8 preemptible_tcs)
+{
+    enetc_port_wr(hw, ENETC4_PFPCR, preemptible_tcs);
+}
+
+/* ENETC does not have an IRQ to notify changes to the MAC Merge TX status
+ * (active/inactive), but the preemptible traffic classes should only be
+ * committed to hardware once TX is active. Resort to polling.
+ */
+void ec_enetc_mm_commit_preemptible_tcs(struct enetc_ndev_priv *priv)
+{
+    struct enetc_si *si = priv->si;
+    struct enetc_hw *hw = &si->hw;
+    u8 preemptible_tcs = 0;
+    u32 val;
+    int err;
+
+    if (is_enetc_rev1(si)) {
+        val = enetc_port_rd(hw, ENETC_MMCSR);
+        if (!(val & ENETC_MMCSR_ME))
+            goto out;
+
+        if (!(val & ENETC_MMCSR_VDIS)) {
+            err = enetc_mm_wait_tx_active(hw, ENETC_MMCSR_GET_VT(val));
+            if (err)
+                goto out;
+        }
+    } else {
+        val = enetc_port_rd(hw, ENETC4_MMCSR);
+        if (!(val & MMCSR_ME))
+            goto out;
+
+        if (!(val & MMCSR_VDIS)) {
+            err = enetc4_mm_wait_tx_active(hw, MMCSR_GET_VT(val));
+            if (err)
+                goto out;
+        }
+    }
+
+    preemptible_tcs = priv->preemptible_tcs;
+out:
+    if (is_enetc_rev1(si))
+        enetc_set_ptcfpr(hw, preemptible_tcs);
+    else
+        enetc4_set_pfpcr(hw, preemptible_tcs);
+}
+
+/* FIXME: Workaround for the link partner's verification failing if ENETC
+ * priorly received too much express traffic. The documentation doesn't
+ * suggest this is needed.
+ */
+static void enetc_restart_emac_rx(struct enetc_si *si)
+{
+    struct enetc_hw *hw = &si->hw;
+    u32 val;
+
+    if (is_enetc_rev1(si)) {
+        val = enetc_port_rd(hw, ENETC_PM0_CMD_CFG);
+
+        enetc_port_wr(hw, ENETC_PM0_CMD_CFG, val & ~ENETC_PM0_RX_EN);
+
+        if (val & ENETC_PM0_RX_EN)
+            enetc_port_wr(hw, ENETC_PM0_CMD_CFG, val);
+    } else {
+        val = enetc_port_rd(hw, ENETC4_PM_CMD_CFG(0));
+
+        enetc_port_wr(hw, ENETC4_PM_CMD_CFG(0), val & ~PM_CMD_CFG_RX_EN);
+
+        if (val & PM_CMD_CFG_RX_EN)
+            enetc_port_wr(hw, ENETC4_PM_CMD_CFG(0), val);
+    }
+}
+
+static void enetc_mm_config(struct enetc_ndev_priv *priv, struct ethtool_mm_cfg *cfg,
+                u32 add_frag_size)
+{
+    struct enetc_si *si = priv->si;
+    struct enetc_hw *hw = &si->hw;
+    u32 val;
+
+    val = enetc_port_rd(hw, ENETC_PFPMR);
+    if (cfg->pmac_enabled)
+        val |= ENETC_PFPMR_PMACE;
+    else
+        val &= ~ENETC_PFPMR_PMACE;
+    enetc_port_wr(hw, ENETC_PFPMR, val);
+
+    val = enetc_port_rd(hw, ENETC_MMCSR);
+
+    if (cfg->verify_enabled)
+        val &= ~ENETC_MMCSR_VDIS;
+    else
+        val |= ENETC_MMCSR_VDIS;
+
+    if (cfg->tx_enabled)
+        priv->active_offloads |= ENETC_F_QBU;
+    else
+        priv->active_offloads &= ~ENETC_F_QBU;
+
+    /* If link is up, enable/disable MAC Merge right away */
+    if (!(val & ENETC_MMCSR_LINK_FAIL)) {
+        if (!!(priv->active_offloads & ENETC_F_QBU))
+            val |= ENETC_MMCSR_ME;
+        else
+            val &= ~ENETC_MMCSR_ME;
+    }
+
+    val &= ~ENETC_MMCSR_VT_MASK;
+    val |= ENETC_MMCSR_VT(cfg->verify_time);
+
+    val &= ~ENETC_MMCSR_RAFS_MASK;
+    val |= ENETC_MMCSR_RAFS(add_frag_size);
+
+    enetc_port_wr(hw, ENETC_MMCSR, val);
+
+    enetc_restart_emac_rx(si);
+
+    ec_enetc_mm_commit_preemptible_tcs(priv);
+}
+
+static void enetc4_mm_config(struct enetc_ndev_priv *priv, struct ethtool_mm_cfg *cfg,
+                 u32 add_frag_size)
+{
+    struct enetc_si *si = priv->si;
+    struct enetc_hw *hw = &si->hw;
+    u32 val;
+
+    val = enetc_port_rd(hw, ENETC4_MMCSR);
+    if (cfg->pmac_enabled)
+        val = u32_replace_bits(val, MMCSR_ME_4B_BOUNDARY, MMCSR_ME);
+    else
+        val = u32_replace_bits(val, 0, MMCSR_ME);
+
+    if (cfg->verify_enabled)
+        val &= ~MMCSR_VDIS;
+    else
+        val |= MMCSR_VDIS;
+
+    if (cfg->tx_enabled) {
+        priv->active_offloads |= ENETC_F_QBU;
+
+        /* When preemption is enabled on a port, IEEE 1588 PTP
+         * one-step timestamping is not supported.
+         */
+        priv->active_offloads &= ~ENETC_F_TX_ONESTEP_SYNC_TSTAMP;
+    } else {
+        priv->active_offloads &= ~ENETC_F_QBU;
+    }
+
+    /* If link is up, enable/disable MAC Merge right away */
+    if (!(val & MMCSR_LINK_FAIL)) {
+        if (!!(priv->active_offloads & ENETC_F_QBU)) {
+            val = u32_replace_bits(val, MMCSR_ME_4B_BOUNDARY, MMCSR_ME);
+
+            /* When preemption is enabled, generation of PAUSE must be
+             * disabled.
+             */
+            enetc_port_wr(hw, ENETC4_PPAUONTR, 0);
+            enetc_port_wr(hw, ENETC4_PPAUOFFTR, 0);
+        } else {
+            val = u32_replace_bits(val, 0, MMCSR_ME);
+        }
+    }
+
+    val = u32_replace_bits(val, cfg->verify_time, MMCSR_VT);
+    val = u32_replace_bits(val, add_frag_size, MMCSR_RAFS);
+
+    enetc_port_wr(hw, ENETC4_MMCSR, val);
+
+    enetc_restart_emac_rx(si);
+
+    ec_enetc_mm_commit_preemptible_tcs(priv);
+}
+
+static int enetc_set_mm(struct net_device *ndev, struct ethtool_mm_cfg *cfg,
+            struct netlink_ext_ack *extack)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct enetc_si *si = priv->si;
+    u32 add_frag_size;
+    int err;
+
+    if (!(si->hw_features & ENETC_SI_F_QBU))
+        return -EOPNOTSUPP;
+
+    err = ethtool_mm_frag_size_min_to_add(cfg->tx_min_frag_size,
+                          &add_frag_size, extack);
+    if (err)
+        return err;
+
+    mutex_lock(&priv->mm_lock);
+
+    if (is_enetc_rev1(si))
+        enetc_mm_config(priv, cfg, add_frag_size);
+    else
+        enetc4_mm_config(priv, cfg, add_frag_size);
+
+    mutex_unlock(&priv->mm_lock);
+
+    return 0;
+}
+
+/* When the link is lost, the verification state machine goes to the FAILED
+ * state and doesn't restart on its own after a new link up event.
+ * According to 802.3 Figure 99-8 - Verify state diagram, the LINK_FAIL bit
+ * should have been sufficient to re-trigger verification, but for ENETC it
+ * doesn't. As a workaround, we need to toggle the Merge Enable bit to
+ * re-trigger verification when link comes up.
+ */
+void ec_enetc_mm_link_state_update(struct enetc_ndev_priv *priv, bool link)
+{
+    struct enetc_si *si = priv->si;
+    struct enetc_hw *hw = &si->hw;
+    u32 val;
+
+    mutex_lock(&priv->mm_lock);
+
+    if (is_enetc_rev1(si)) {
+        val = enetc_port_rd(hw, ENETC_MMCSR);
+
+        if (link) {
+            val &= ~ENETC_MMCSR_LINK_FAIL;
+            if (priv->active_offloads & ENETC_F_QBU)
+                val |= ENETC_MMCSR_ME;
+        } else {
+            val |= ENETC_MMCSR_LINK_FAIL;
+            if (priv->active_offloads & ENETC_F_QBU)
+                val &= ~ENETC_MMCSR_ME;
+        }
+
+        enetc_port_wr(hw, ENETC_MMCSR, val);
+    } else {
+        val = enetc_port_rd(hw, ENETC4_MMCSR);
+
+        if (link) {
+            val &= ~MMCSR_LINK_FAIL;
+            if (priv->active_offloads & ENETC_F_QBU)
+                val = u32_replace_bits(val, MMCSR_ME_4B_BOUNDARY,
+                               MMCSR_ME);
+        } else {
+            val |= MMCSR_LINK_FAIL;
+            if (priv->active_offloads & ENETC_F_QBU)
+                val = u32_replace_bits(val, 0, MMCSR_ME);
+        }
+
+        enetc_port_wr(hw, ENETC4_MMCSR, val);
+    }
+
+    ec_enetc_mm_commit_preemptible_tcs(priv);
+
+    mutex_unlock(&priv->mm_lock);
+}
+//EXPORT_SYMBOL_GPL(ec_enetc_mm_link_state_update);
+
 static const struct ethtool_ops enetc_pf_ethtool_ops = {
 	.supported_coalesce_params = ETHTOOL_COALESCE_USECS |
 				     ETHTOOL_COALESCE_MAX_FRAMES |
@@ -811,11 +1361,16 @@ static const struct ethtool_ops enetc_pf_ethtool_ops = {
 	.get_link_ksettings = enetc_get_link_ksettings,
 	.set_link_ksettings = enetc_set_link_ksettings,
 	.get_link = ethtool_op_get_link,
-	.get_ts_info = enetc_get_ts_info,
-	.get_wol = enetc_get_wol,
-	.set_wol = enetc_set_wol,
-	.get_pauseparam = enetc_get_pauseparam,
-	.set_pauseparam = enetc_set_pauseparam,
+    .get_ts_info = enetc_get_ts_info,
+    .get_wol = enetc_get_wol,
+    .set_wol = enetc_set_wol,
+    .get_eee = enetc_ethtool_op_get_eee,
+    .set_eee = enetc_ethtool_op_set_eee,
+    .get_pauseparam = enetc_get_pauseparam,
+    .set_pauseparam = enetc_set_pauseparam,
+    .get_mm = enetc_get_mm,
+    .set_mm = enetc_set_mm,
+    .get_mm_stats = enetc_get_mm_stats,
 };
 
 static const struct ethtool_ops enetc_vf_ethtool_ops = {
@@ -838,7 +1393,7 @@ static const struct ethtool_ops enetc_vf_ethtool_ops = {
 	.get_ts_info = enetc_get_ts_info,
 };
 
-void enetc_set_ethtool_ops(struct net_device *ndev)
+void ec_enetc_set_ethtool_ops(struct net_device *ndev)
 {
 	struct enetc_ndev_priv *priv = netdev_priv(ndev);
 
@@ -847,3 +1402,5 @@ void enetc_set_ethtool_ops(struct net_device *ndev)
 	else
 		ndev->ethtool_ops = &enetc_vf_ethtool_ops;
 }
+//EXPORT_SYMBOL_GPL(ec_enetc_set_ethtool_ops);
+
diff --git a/devices/enetc/enetc_hw.h b/devices/enetc/enetc_hw.h
index 911b01a..172686e 100644
--- a/devices/enetc/enetc_hw.h
+++ b/devices/enetc/enetc_hw.h
@@ -3,6 +3,9 @@
 
 #include <linux/bitops.h>
 
+#define ENETC_MM_VERIFY_SLEEP_US    USEC_PER_MSEC
+#define ENETC_MM_VERIFY_RETRIES     3
+
 /* ENETC device IDs */
 #define ENETC_DEV_ID_PF		0xe100
 #define ENETC_DEV_ID_VF		0xef00
@@ -63,6 +66,14 @@ static inline u32 enetc_vsi_set_msize(u32 size)
 #define ENETC_SIMSGSR_SET_MC(val) ((val) << 16)
 #define ENETC_SIMSGSR_GET_MC(val) ((val) >> 16)
 
+#define ENETC_PSIMSGSR      0x208
+#define  PSIMSGSR_MS(n)     BIT((n) + 1) /* m = VF index */
+#define  PSIMSGSR_SET_MC(val)   ((val) << 16)
+
+#define ENETC_VSIMSGRR      0x208
+#define  VSIMSGRR_MR        BIT(0)
+#define  VSIMSGRR_GET_MC(val)   ((val) >> 16)
+
 /* SI statistics */
 #define ENETC_SIROCT	0x300
 #define ENETC_SIRFRM	0x308
@@ -87,6 +98,7 @@ static inline u32 enetc_vsi_set_msize(u32 size)
 #define ENETC_SICAPR1	0x904
 
 #define ENETC_PSIIER	0xa00
+#define  PSIIER_MR(n)   BIT((n) + 1) /* n = VSI index */
 #define ENETC_PSIIER_MR_MASK	GENMASK(2, 1)
 #define ENETC_PSIIDR	0xa08
 #define ENETC_SITXIDR	0xa18
@@ -108,48 +120,55 @@ enum enetc_bdr_type {TX, RX};
 #define ENETC_BDR_OFF(i)	((i) * 0x200)
 #define ENETC_BDR(t, i, r)	(0x8000 + (t) * 0x100 + ENETC_BDR_OFF(i) + (r))
 /* RX BDR reg offsets */
-#define ENETC_RBMR	0
-#define ENETC_RBMR_BDS	BIT(2)
-#define ENETC_RBMR_VTE	BIT(5)
-#define ENETC_RBMR_EN	BIT(31)
-#define ENETC_RBSR	0x4
-#define ENETC_RBBSR	0x8
-#define ENETC_RBCIR	0xc
-#define ENETC_RBBAR0	0x10
-#define ENETC_RBBAR1	0x14
-#define ENETC_RBPIR	0x18
-#define ENETC_RBLENR	0x20
-#define ENETC_RBIER	0xa0
-#define ENETC_RBIER_RXTIE	BIT(0)
-#define ENETC_RBIDR	0xa4
-#define ENETC_RBICR0	0xa8
-#define ENETC_RBICR0_ICEN		BIT(31)
-#define ENETC_RBICR0_ICPT_MASK		0x1ff
-#define ENETC_RBICR0_SET_ICPT(n)	((n) & ENETC_RBICR0_ICPT_MASK)
-#define ENETC_RBICR1	0xac
+#define ENETC_RBMR  0
+#define ENETC_RBMR_BDS  BIT(2)
+#define ENETC_RBMR_CM   BIT(4)
+#define ENETC_RBMR_VTE  BIT(5)
+#define ENETC_RBMR_EN   BIT(31)
+#define ENETC_RBSR  0x4
+#define ENETC_RBBSR 0x8
+#define ENETC_RBCIR 0xc
+#define ENETC_RBBAR0    0x10
+#define ENETC_RBBAR1    0x14
+#define ENETC_RBPIR 0x18
+#define ENETC_RBLENR    0x20
+#define ENETC_RBRSCR    0x30
+#define ENETC_RBRSCR_EN BIT(31)
+#define ENETC_RBRSCR_SIZE_MASK  0xffff
+#define ENETC_RBRSCR_SIZE(n)    ((n) & ENETC_RBRSCR_SIZE_MASK)
+#define ENETC_RBIER 0xa0
+#define ENETC_RBIER_RXTIE   BIT(0)
+#define ENETC_RBIDR 0xa4
+#define ENETC_RBICR0    0xa8
+#define ENETC_RBICR0_ICEN       BIT(31)
+#define ENETC_RBICR0_ICPT_MASK      0x1ff
+#define ENETC_RBICR0_SET_ICPT(n)    ((n) & ENETC_RBICR0_ICPT_MASK)
+#define ENETC_RBICR1    0xac
 
 /* TX BDR reg offsets */
-#define ENETC_TBMR	0
-#define ENETC_TBSR_BUSY	BIT(0)
-#define ENETC_TBMR_VIH	BIT(9)
-#define ENETC_TBMR_PRIO_MASK		GENMASK(2, 0)
-#define ENETC_TBMR_SET_PRIO(val)	((val) & ENETC_TBMR_PRIO_MASK)
-#define ENETC_TBMR_EN	BIT(31)
-#define ENETC_TBSR	0x4
-#define ENETC_TBBAR0	0x10
-#define ENETC_TBBAR1	0x14
-#define ENETC_TBPIR	0x18
-#define ENETC_TBCIR	0x1c
-#define ENETC_TBCIR_IDX_MASK	0xffff
-#define ENETC_TBLENR	0x20
-#define ENETC_TBIER	0xa0
-#define ENETC_TBIER_TXTIE	BIT(0)
-#define ENETC_TBIDR	0xa4
-#define ENETC_TBICR0	0xa8
-#define ENETC_TBICR0_ICEN		BIT(31)
-#define ENETC_TBICR0_ICPT_MASK		0xf
+#define ENETC_TBMR  0
+#define ENETC_TBSR_BUSY BIT(0)
+#define ENETC_TBMR_VIH  BIT(9)
+#define ENETC_TBMR_PRIO_MASK        GENMASK(2, 0)
+#define ENETC_TBMR_SET_PRIO(val)    ((val) & ENETC_TBMR_PRIO_MASK)
+#define ENETC_TBMR_WRR  GENMASK(6, 4)
+#define ENETC_TBMR_FWB  BIT(24)
+#define ENETC_TBMR_EN   BIT(31)
+#define ENETC_TBSR  0x4
+#define ENETC_TBBAR0    0x10
+#define ENETC_TBBAR1    0x14
+#define ENETC_TBPIR 0x18
+#define ENETC_TBCIR 0x1c
+#define ENETC_TBCIR_IDX_MASK    0xffff
+#define ENETC_TBLENR    0x20
+#define ENETC_TBIER 0xa0
+#define ENETC_TBIER_TXTIE   BIT(0)
+#define ENETC_TBIDR 0xa4
+#define ENETC_TBICR0    0xa8
+#define ENETC_TBICR0_ICEN       BIT(31)
+#define ENETC_TBICR0_ICPT_MASK      0xf
 #define ENETC_TBICR0_SET_ICPT(n) ((ilog2(n) + 1) & ENETC_TBICR0_ICPT_MASK)
-#define ENETC_TBICR1	0xac
+#define ENETC_TBICR1    0xac
 
 #define ENETC_RTBLENR_LEN(n)	((n) & ~0x7)
 
@@ -192,35 +211,59 @@ enum enetc_bdr_type {TX, RX};
 #define ENETC_PSICFGR0_ASE	BIT(15)
 #define ENETC_PSICFGR0_SIVC(bmp)	(((bmp) & 0xff) << 24) /* VLAN_TYPE */
 
-#define ENETC_PTCCBSR0(n)	(0x1110 + (n) * 8) /* n = 0 to 7*/
-#define ENETC_CBSE		BIT(31)
-#define ENETC_CBS_BW_MASK	GENMASK(6, 0)
-#define ENETC_PTCCBSR1(n)	(0x1114 + (n) * 8) /* n = 0 to 7*/
-#define ENETC_RSSHASH_KEY_SIZE	40
-#define ENETC_PRSSK(n)		(0x1410 + (n) * 4) /* n = [0..9] */
-#define ENETC_PSIVLANFMR	0x1700
-#define ENETC_PSIVLANFMR_VS	BIT(0)
-#define ENETC_PRFSMR		0x1800
-#define ENETC_PRFSMR_RFSE	BIT(31)
-#define ENETC_PRFSCAPR		0x1804
-#define ENETC_PRFSCAPR_GET_NUM_RFS(val)	((((val) & 0xf) + 1) * 16)
-#define ENETC_PSIRFSCFGR(n)	(0x1814 + (n) * 4) /* n = SI index */
-#define ENETC_PFPMR		0x1900
-#define ENETC_PFPMR_PMACE	BIT(1)
-#define ENETC_PFPMR_MWLM	BIT(0)
-#define ENETC_EMDIO_BASE	0x1c00
-#define ENETC_PSIUMHFR0(n, err)	(((err) ? 0x1d08 : 0x1d00) + (n) * 0x10)
-#define ENETC_PSIUMHFR1(n)	(0x1d04 + (n) * 0x10)
-#define ENETC_PSIMMHFR0(n, err)	(((err) ? 0x1d00 : 0x1d08) + (n) * 0x10)
-#define ENETC_PSIMMHFR1(n)	(0x1d0c + (n) * 0x10)
-#define ENETC_PSIVHFR0(n)	(0x1e00 + (n) * 8) /* n = SI index */
-#define ENETC_PSIVHFR1(n)	(0x1e04 + (n) * 8) /* n = SI index */
-#define ENETC_MMCSR		0x1f00
-#define ENETC_MMCSR_ME		BIT(16)
-#define ENETC_MMCSR_RAFS_MASK	GENMASK(9, 8)
-#define ENETC_MMCSR_RAFS(x)	(((x) << 8) & ENETC_MMCSR_RAFS_MASK)
-#define ENETC_MMCSR_GET_RAFS(x)	(((x) & ENETC_MMCSR_RAFS_MASK) >> 8)
-#define ENETC_PTCMSDUR(n)	(0x2020 + (n) * 4) /* n = TC index [0..7] */
+#define ENETC_PTCCBSR0(n)   (0x1110 + (n) * 8) /* n = 0 to 7*/
+#define ENETC_CBSE      BIT(31)
+#define ENETC_CBS_BW_MASK   GENMASK(6, 0)
+#define ENETC_PTCCBSR1(n)   (0x1114 + (n) * 8) /* n = 0 to 7*/
+#define ENETC_RSSHASH_KEY_SIZE  40
+#define ENETC_PRSSCAPR      0x1404
+#define ENETC_PRSSCAPR_GET_NUM_RSS(val) (BIT((val) & 0xf) * 32)
+#define ENETC_PRSSK(n)      (0x1410 + (n) * 4) /* n = [0..9] */
+#define ENETC_PSIVLANFMR    0x1700
+#define ENETC_PSIVLANFMR_VS BIT(0)
+#define ENETC_PRFSMR        0x1800
+#define ENETC_PRFSMR_RFSE   BIT(31)
+#define ENETC_PRFSCAPR      0x1804
+#define ENETC_PRFSCAPR_GET_NUM_RFS(val) ((((val) & 0xf) + 1) * 16)
+#define ENETC_PSIRFSCFGR(n) (0x1814 + (n) * 4) /* n = SI index */
+#define ENETC_PFPMR     0x1900
+#define ENETC_PFPMR_PMACE   BIT(1)
+#define ENETC_EMDIO_BASE    0x1c00
+#define ENETC_PSIUMHFR0(n, err) (((err) ? 0x1d08 : 0x1d00) + (n) * 0x10)
+#define ENETC_PSIUMHFR1(n)  (0x1d04 + (n) * 0x10)
+#define ENETC_PSIMMHFR0(n, err) (((err) ? 0x1d00 : 0x1d08) + (n) * 0x10)
+#define ENETC_PSIMMHFR1(n)  (0x1d0c + (n) * 0x10)
+#define ENETC_PSIVHFR0(n)   (0x1e00 + (n) * 8) /* n = SI index */
+#define ENETC_PSIVHFR1(n)   (0x1e04 + (n) * 8) /* n = SI index */
+#define ENETC_MMCSR     0x1f00
+#define ENETC_MMCSR_LINK_FAIL   BIT(31)
+#define ENETC_MMCSR_VT_MASK GENMASK(29, 23) /* Verify Time */
+#define ENETC_MMCSR_VT(x)   (((x) << 23) & ENETC_MMCSR_VT_MASK)
+#define ENETC_MMCSR_GET_VT(x)   (((x) & ENETC_MMCSR_VT_MASK) >> 23)
+#define ENETC_MMCSR_TXSTS_MASK  GENMASK(22, 21) /* Merge Status */
+#define ENETC_MMCSR_GET_TXSTS(x) (((x) & ENETC_MMCSR_TXSTS_MASK) >> 21)
+#define ENETC_MMCSR_VSTS_MASK   GENMASK(20, 18) /* Verify Status */
+#define ENETC_MMCSR_GET_VSTS(x) (((x) & ENETC_MMCSR_VSTS_MASK) >> 18)
+#define ENETC_MMCSR_VDIS    BIT(17) /* Verify Disabled */
+#define ENETC_MMCSR_ME      BIT(16) /* Merge Enabled */
+#define ENETC_MMCSR_RAFS_MASK   GENMASK(9, 8) /* Remote Additional Fragment Size */
+#define ENETC_MMCSR_RAFS(x) (((x) << 8) & ENETC_MMCSR_RAFS_MASK)
+#define ENETC_MMCSR_GET_RAFS(x) (((x) & ENETC_MMCSR_RAFS_MASK) >> 8)
+#define ENETC_MMCSR_LAFS_MASK   GENMASK(4, 3) /* Local Additional Fragment Size */
+#define ENETC_MMCSR_GET_LAFS(x) (((x) & ENETC_MMCSR_LAFS_MASK) >> 3)
+#define ENETC_MMCSR_LPA     BIT(2) /* Local Preemption Active */
+#define ENETC_MMCSR_LPE     BIT(1) /* Local Preemption Enabled */
+#define ENETC_MMCSR_LPS     BIT(0) /* Local Preemption Supported */
+#define ENETC_MMFAECR       0x1f08
+#define ENETC_MMFSECR       0x1f0c
+#define ENETC_MMFAOCR       0x1f10
+#define ENETC_MMFCRXR       0x1f14
+#define ENETC_MMFCTXR       0x1f18
+#define ENETC_MMHCR     0x1f1c
+#define ENETC_PTCMSDUR(n)   (0x2020 + (n) * 4) /* n = TC index [0..7] */
+#define ENETC_PTCMSDUR_MAXSDU   GENMASK(15, 0)
+
+#define ENETC_PMAC_OFFSET   0x1000
 
 #define ENETC_PM0_CMD_CFG	0x8008
 #define ENETC_PM1_CMD_CFG	0x9008
@@ -239,21 +282,36 @@ enum enetc_bdr_type {TX, RX};
 
 #define ENETC_PM_IMDIO_BASE	0x8030
 
-#define ENETC_PM0_IF_MODE	0x8300
+#define ENETC_PM0_PAUSE_QUANTA  0x8054
+#define ENETC_PM0_PAUSE_THRESH  0x8064
+
+#define ENETC_PM0_SINGLE_STEP       0x80c0
+#define ENETC_PM0_SINGLE_STEP_CH    BIT(7)
+#define ENETC_PM0_SINGLE_STEP_EN    BIT(31)
+#define ENETC_SET_SINGLE_STEP_OFFSET(v) (((v) & 0xff) << 8)
+
 #define ENETC_PM1_IF_MODE       0x9300
-#define ENETC_PMO_IFM_RG	BIT(2)
-#define ENETC_PM0_IFM_RLP	(BIT(5) | BIT(11))
-#define ENETC_PM0_IFM_RGAUTO	(BIT(15) | ENETC_PMO_IFM_RG | BIT(1))
-#define ENETC_PM0_IFM_XGMII	BIT(12)
-#define ENETC_PSIDCAPR		0x1b08
-#define ENETC_PSIDCAPR_MSK	GENMASK(15, 0)
-#define ENETC_PSFCAPR		0x1b18
-#define ENETC_PSFCAPR_MSK	GENMASK(15, 0)
-#define ENETC_PSGCAPR		0x1b28
-#define ENETC_PSGCAPR_GCL_MSK	GENMASK(18, 16)
-#define ENETC_PSGCAPR_SGIT_MSK	GENMASK(15, 0)
-#define ENETC_PFMCAPR		0x1b38
-#define ENETC_PFMCAPR_MSK	GENMASK(15, 0)
+#define ENETC_PM0_IF_MODE   0x8300
+#define ENETC_PM0_IFM_RG    BIT(2)
+#define ENETC_PM0_IFM_RLP   (BIT(5) | BIT(11))
+#define ENETC_PM0_IFM_EN_AUTO   BIT(15)
+#define ENETC_PM0_IFM_SSP_MASK  GENMASK(14, 13)
+#define ENETC_PM0_IFM_SSP_1000  (2 << 13)
+#define ENETC_PM0_IFM_SSP_100   (0 << 13)
+#define ENETC_PM0_IFM_SSP_10    (1 << 13)
+#define ENETC_PM0_IFM_FULL_DPX  BIT(12)
+#define ENETC_PM0_IFM_IFMODE_MASK GENMASK(1, 0)
+#define ENETC_PM0_IFM_IFMODE_XGMII 0
+#define ENETC_PM0_IFM_IFMODE_GMII 2
+#define ENETC_PSIDCAPR      0x1b08
+#define ENETC_PSIDCAPR_MSK  GENMASK(15, 0)
+#define ENETC_PSFCAPR       0x1b18
+#define ENETC_PSFCAPR_MSK   GENMASK(15, 0)
+#define ENETC_PSGCAPR       0x1b28
+#define ENETC_PSGCAPR_GCL_MSK   GENMASK(18, 16)
+#define ENETC_PSGCAPR_SGIT_MSK  GENMASK(15, 0)
+#define ENETC_PFMCAPR       0x1b38
+#define ENETC_PFMCAPR_MSK   GENMASK(15, 0)
 
 /* MAC counters */
 #define ENETC_PM0_REOCT		0x8100
@@ -330,6 +388,7 @@ enum enetc_bdr_type {TX, RX};
 /** Global regs, offset: 2_0000h */
 #define ENETC_GLOBAL_BASE	0x20000
 #define ENETC_G_EIPBRR0		0x0bf8
+#define  EIPBRR0_REVISION   GENMASK(15, 0)
 #define ENETC_G_EIPBRR1		0x0bfc
 #define ENETC_G_EPFBLPR(n)	(0xd00 + 4 * (n))
 #define ENETC_G_EPFBLPR1_XGMII	0x80000000
@@ -467,9 +526,12 @@ static inline u64 _enetc_rd_reg64_wa(void __iomem *reg)
 #define enetc_wr_reg(reg, val)		_enetc_wr_reg_wa((reg), (val))
 #define enetc_rd(hw, off)		enetc_rd_reg((hw)->reg + (off))
 #define enetc_wr(hw, off, val)		enetc_wr_reg((hw)->reg + (off), val)
+#define enetc_rd_hot(hw, off)       enetc_rd_reg_hot((hw)->reg + (off))
+#define enetc_wr_hot(hw, off, val)  enetc_wr_reg_hot((hw)->reg + (off), val)
 #define enetc_rd64(hw, off)		_enetc_rd_reg64_wa((hw)->reg + (off))
 /* port register accessors - PF only */
 #define enetc_port_rd(hw, off)		enetc_rd_reg((hw)->port + (off))
+#define enetc_port_rd64(hw, off)    _enetc_rd_reg64_wa((hw)->port + (off))
 #define enetc_port_wr(hw, off, val)	enetc_wr_reg((hw)->port + (off), val)
 #define enetc_port_rd_mdio(hw, off)	_enetc_rd_mdio_reg_wa((hw)->port + (off))
 #define enetc_port_wr_mdio(hw, off, val)	_enetc_wr_mdio_reg_wa(\
@@ -491,34 +553,57 @@ static inline u64 _enetc_rd_reg64_wa(void __iomem *reg)
 
 /* Buffer Descriptors (BD) */
 union enetc_tx_bd {
-	struct {
-		__le64 addr;
-		__le16 buf_len;
-		__le16 frm_len;
-		union {
-			struct {
-				__le16 l3_csoff;
-				u8 l4_csoff;
-				u8 flags;
-			}; /* default layout */
-			__le32 txstart;
-			__le32 lstatus;
-		};
-	};
-	struct {
-		__le32 tstamp;
-		__le16 tpid;
-		__le16 vid;
-		u8 reserved[6];
-		u8 e_flags;
-		u8 flags;
-	} ext; /* Tx BD extension */
-	struct {
-		__le32 tstamp;
-		u8 reserved[10];
-		u8 status;
-		u8 flags;
-	} wb; /* writeback descriptor */
+    struct {
+        __le64 addr;
+        struct {
+            union {
+                __le16 buf_len;
+                __le16 hdr_len; // For LSO only
+            };
+            __le16 frm_len;
+        };
+        union {
+            struct {
+                u8 l3_start:7;
+                u8 ipcs:1;
+                u8 l3_hdr_size:7;
+                u8 l3t:1;
+                u8 resv:5;
+                u8 l4t:3;
+                u8 flags;
+            }; /* default layout */
+            __le32 txstart;
+            __le32 lstatus;
+        };
+    };
+    struct {
+        __le32 tstamp;
+        __le16 tpid;
+        __le16 vid;
+        __le16 lso_sg_size; // For enetc4
+        __le16 frm_len_ext; // For enetc4
+        u8 resv[2];
+        u8 e_flags;
+        u8 flags;
+    } ext; /* Tx BD extension */
+    struct {
+        __le32 tstamp;
+        u8 resv[8];
+        __le16 lso_err_count;   // For enetc4
+        u8 status;
+        u8 flags;
+    } wb; /* writeback descriptor */
+};
+
+enum enetc_txbd_flags {
+    ENETC_TXBD_FLAGS_L4CS = BIT(0),
+    ENETC_TXBD_FLAGS_TSE = BIT(1),
+    ENETC_TXBD_FLAGS_LSO = BIT(1), // For ENETC4
+    ENETC_TXBD_FLAGS_W = BIT(2),
+    ENETC_TXBD_FLAGS_CSUM_LSO = BIT(3), // For ENETC4
+    ENETC_TXBD_FLAGS_TXSTART = BIT(4),
+    ENETC_TXBD_FLAGS_EX = BIT(6),
+    ENETC_TXBD_FLAGS_F = BIT(7)
 };
 
 #define ENETC_TXBD_FLAGS_L4CS	BIT(0)
@@ -532,6 +617,20 @@ union enetc_tx_bd {
 #define ENETC_TXBD_TXSTART_MASK GENMASK(24, 0)
 #define ENETC_TXBD_FLAGS_OFFSET 24
 
+#define ENETC_TXBD_L4T_NONE 0
+#define ENETC_TXBD_L4T_UDP  BIT(0)
+#define ENETC_TXBD_L4T_TCP  BIT(1)
+
+static inline __le32 enetc_txbd_set_tx_start(u64 tx_start, u8 flags)
+{
+    u32 temp;
+
+    temp = (tx_start >> 5 & ENETC_TXBD_TXSTART_MASK) |
+           (flags << ENETC_TXBD_FLAGS_OFFSET);
+
+    return cpu_to_le32(temp);
+}
+
 static inline void enetc_clear_tx_bd(union enetc_tx_bd *txbd)
 {
 	memset(txbd, 0, sizeof(*txbd));
@@ -545,8 +644,9 @@ static inline void enetc_clear_tx_bd(union enetc_tx_bd *txbd)
 #define ENETC_TXBD_L3_SET_HSIZE(val)	((((val) >> 2) & 0x7f) << 8)
 
 /* Extension flags */
-#define ENETC_TXBD_E_FLAGS_VLAN_INS	BIT(0)
-#define ENETC_TXBD_E_FLAGS_TWO_STEP_PTP	BIT(2)
+#define ENETC_TXBD_E_FLAGS_VLAN_INS BIT(0)
+#define ENETC_TXBD_E_FLAGS_ONE_STEP_PTP BIT(1)
+#define ENETC_TXBD_E_FLAGS_TWO_STEP_PTP BIT(2)
 
 static inline __le16 enetc_txbd_l3_csoff(int start, int hdr_sz, u16 l3_flags)
 {
@@ -634,6 +734,22 @@ static inline void enetc_get_primary_mac_addr(struct enetc_hw *hw, u8 *addr)
 	*(u16 *)(addr + 4) = __raw_readw(hw->reg + ENETC_SIPMAR1);
 }
 
+static inline void enetc_load_primary_mac_addr(struct enetc_hw *hw,
+                           struct net_device *ndev)
+{
+    u8 addr[ETH_ALEN] __aligned(4);
+
+    *(u32 *)addr = __raw_readl(hw->reg + ENETC_SIPMAR0);
+    *(u16 *)(addr + 4) = __raw_readw(hw->reg + ENETC_SIPMAR1);
+    eth_hw_addr_set(ndev, addr);
+}
+
+static inline void enetc_get_si_primary_mac(struct enetc_hw *hw, u8 *addr)
+{
+    *(u32 *)addr = __raw_readl(hw->reg + ENETC_SIPMAR0);
+    *(u16 *)(addr + 4) = __raw_readw(hw->reg + ENETC_SIPMAR1);
+}
+
 #define ENETC_SI_INT_IDX	0
 /* base index for Rx/Tx interrupts */
 #define ENETC_BDR_INT_BASE_IDX	1
@@ -1073,19 +1189,30 @@ struct enetc_cbd {
 };
 
 #define ENETC_CLK  400000000ULL
-static inline u32 enetc_cycles_to_usecs(u32 cycles)
+#define ENETC4_CLK 333000000ULL
+static inline u32 enetc_cycles_to_usecs(u32 cycles, u64 clk_freq)
 {
-	return (u32)div_u64(cycles * 1000000ULL, ENETC_CLK);
+    return (u32)div_u64(cycles * 1000000ULL, clk_freq);
 }
 
-static inline u32 enetc_usecs_to_cycles(u32 usecs)
+static inline u32 enetc_usecs_to_cycles(u32 usecs, u64 clk_freq)
 {
-	return (u32)div_u64(usecs * ENETC_CLK, 1000000ULL);
+    return (u32)div_u64(usecs * clk_freq, 1000000ULL);
 }
 
 #define ENETC_PTCFPR(n)		(0x1910 + (n) * 4) /* n = [0 ..7] */
+#define ENETC_PTCFPR_FPE        BIT(31)
 #define ENETC_FPE		BIT(31)
 
+/* Port time gating capability register */
+#define ENETC_PTGCAPR           0x11a08
+#define ENETC_PTGCAPR_MAX_GCL_LEN_MASK  GENMASK(15, 0)
+
+/* port time gating control register */
+#define ENETC_PTGCR         0x11a00
+#define ENETC_PTGCR_TGE         BIT(31)
+#define ENETC_PTGCR_TGPE        BIT(30)
+
 /* Port capability register 0 */
 #define ENETC_PCAPR0_PSFPM	BIT(10)
 #define ENETC_PCAPR0_PSFP	BIT(9)
diff --git a/devices/enetc/enetc_msg.c b/devices/enetc/enetc_msg.c
new file mode 100755
index 0000000..66468ad
--- /dev/null
+++ b/devices/enetc/enetc_msg.c
@@ -0,0 +1,177 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+/* Copyright 2017-2019 NXP */
+
+#include "enetc_pf.h"
+
+static void enetc_msg_enable_mr_int(struct enetc_pf *pf, bool en)
+{
+	struct enetc_hw *hw = &pf->si->hw;
+	u32 psiier, i, mr_mask = 0;
+
+	for (i = 0; i < pf->num_vfs; i++)
+		mr_mask |= PSIIER_MR(i);
+
+	psiier = enetc_rd(hw, ENETC_PSIIER);
+	if (en)
+		psiier |= mr_mask;
+	else
+		psiier &= ~mr_mask;
+
+	enetc_wr(hw, ENETC_PSIIER, psiier);
+}
+
+static irqreturn_t enetc_msg_psi_msix(int irq, void *data)
+{
+	struct enetc_si *si = (struct enetc_si *)data;
+	struct enetc_pf *pf = enetc_si_priv(si);
+
+	enetc_msg_enable_mr_int(pf, false);
+	schedule_work(&si->msg_task);
+
+	return IRQ_HANDLED;
+}
+
+static void enetc_msg_task(struct work_struct *work)
+{
+	struct enetc_si *si = container_of(work, struct enetc_si, msg_task);
+	struct enetc_pf *pf = enetc_si_priv(si);
+	struct enetc_hw *hw = &si->hw;
+	u32 mr_mask = 0, mr_status;
+	int i;
+
+	for (i = 0; i < pf->num_vfs; i++)
+		mr_mask |= ENETC_PSIMSGRR_MR(i);
+
+	for (;;) {
+		mr_status = enetc_rd(hw, ENETC_PSIMSGRR) & mr_mask;
+		if (!mr_status) {
+			/* re-arm MR interrupts, w1c the IDR reg */
+			enetc_wr(hw, ENETC_PSIIDR, mr_mask);
+			enetc_msg_enable_mr_int(pf, true);
+			return;
+		}
+
+		for (i = 0; i < pf->num_vfs; i++) {
+			u32 psimsgrr;
+			u16 msg_code;
+
+			if (!(ENETC_PSIMSGRR_MR(i) & mr_status))
+				continue;
+
+			enetc_msg_handle_rxmsg(pf, i, &msg_code);
+
+			/* If msg_code is 0, it means that PF has already replied
+			 * to VF, and we don't need to reply here.
+			 */
+			if (!msg_code)
+				continue;
+
+			psimsgrr = ENETC_SIMSGSR_SET_MC(msg_code);
+			psimsgrr |= ENETC_PSIMSGRR_MR(i); /* w1c */
+			enetc_wr(hw, ENETC_PSIMSGRR, psimsgrr);
+		}
+	}
+}
+
+/* Init */
+static int enetc_msg_alloc_mbx(struct enetc_si *si, int idx)
+{
+	struct enetc_pf *pf = enetc_si_priv(si);
+	struct device *dev = &si->pdev->dev;
+	struct enetc_hw *hw = &si->hw;
+	struct enetc_msg_swbd *msg;
+	u32 val;
+
+	msg = &pf->rxmsg[idx];
+	/* allocate and set receive buffer */
+	msg->size = ENETC_DEFAULT_MSG_SIZE;
+
+	msg->vaddr = dma_alloc_coherent(dev, msg->size, &msg->dma,
+					GFP_KERNEL);
+	if (!msg->vaddr) {
+		dev_err(dev, "msg: fail to alloc dma buffer of size: %d\n",
+			msg->size);
+		return -ENOMEM;
+	}
+
+	/* set multiple of 32 bytes */
+	val = lower_32_bits(msg->dma);
+	enetc_wr(hw, ENETC_PSIVMSGRCVAR0(idx), val);
+	val = upper_32_bits(msg->dma);
+	enetc_wr(hw, ENETC_PSIVMSGRCVAR1(idx), val);
+
+	return 0;
+}
+
+static void enetc_msg_free_mbx(struct enetc_si *si, int idx)
+{
+	struct enetc_pf *pf = enetc_si_priv(si);
+	struct enetc_hw *hw = &si->hw;
+	struct enetc_msg_swbd *msg;
+
+	msg = &pf->rxmsg[idx];
+	dma_free_coherent(&si->pdev->dev, msg->size, msg->vaddr, msg->dma);
+	memset(msg, 0, sizeof(*msg));
+
+	enetc_wr(hw, ENETC_PSIVMSGRCVAR0(idx), 0);
+	enetc_wr(hw, ENETC_PSIVMSGRCVAR1(idx), 0);
+}
+
+int enetc_msg_psi_init(struct enetc_pf *pf)
+{
+	struct enetc_si *si = pf->si;
+	int vector, i, err;
+
+	/* register message passing interrupt handler */
+	snprintf(si->msg_int_name, sizeof(si->msg_int_name), "%s-vfmsg",
+		 si->ndev->name);
+	vector = pci_irq_vector(si->pdev, ENETC_SI_INT_IDX);
+	err = request_irq(vector, enetc_msg_psi_msix, 0, si->msg_int_name, si);
+	if (err) {
+		dev_err(&si->pdev->dev,
+			"PSI messaging: request_irq() failed!\n");
+		return err;
+	}
+
+	/* set one IRQ entry for PSI message receive notification (SI int) */
+	enetc_wr(&si->hw, ENETC_SIMSIVR, ENETC_SI_INT_IDX);
+
+	/* initialize PSI mailbox */
+	INIT_WORK(&si->msg_task, enetc_msg_task);
+
+	for (i = 0; i < pf->num_vfs; i++) {
+		err = enetc_msg_alloc_mbx(si, i);
+		if (err)
+			goto err_init_mbx;
+	}
+
+	/* enable MR interrupts */
+	enetc_msg_enable_mr_int(pf, true);
+
+	return 0;
+
+err_init_mbx:
+	for (i--; i >= 0; i--)
+		enetc_msg_free_mbx(si, i);
+
+	free_irq(vector, si);
+
+	return err;
+}
+
+void enetc_msg_psi_free(struct enetc_pf *pf)
+{
+	struct enetc_si *si = pf->si;
+	int i;
+
+	cancel_work_sync(&si->msg_task);
+
+	/* disable MR interrupts */
+	enetc_msg_enable_mr_int(pf, false);
+
+	for (i = 0; i < pf->num_vfs; i++)
+		enetc_msg_free_mbx(si, i);
+
+	/* de-register message passing interrupt handler */
+	free_irq(pci_irq_vector(si->pdev, ENETC_SI_INT_IDX), si);
+}
diff --git a/devices/enetc/enetc_msg.h b/devices/enetc/enetc_msg.h
new file mode 100755
index 0000000..ac5e077
--- /dev/null
+++ b/devices/enetc/enetc_msg.h
@@ -0,0 +1,242 @@
+/* SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause) */
+/* Copyright 2024 NXP */
+#include <linux/crc-itu-t.h>
+
+#define ENETC_CRC_INIT		0xffff
+#define ENETC_MSG_ALIGN		32
+
+#define ENETC_MSG_EXT_BODY_LEN(l)	((l) / ENETC_MSG_ALIGN - 1)
+#define ENETC_MSG_SIZE(l)		(((l) + 1) * ENETC_MSG_ALIGN)
+
+/* Common Class ID for PSI-TO-VSI and VSI-TO-PSI messages */
+#define ENETC_MSG_CLASS_ID_MAC_FILTER		0x20
+#define ENETC_MSG_CLASS_ID_VLAN_FILTER		0x21
+#define ENETC_MSG_CLASS_ID_LINK_STATUS		0x80
+#define ENETC_MSG_CLASS_ID_LINK_SPEED		0x81
+
+/* Class ID for PSI-TO-VSI messages */
+#define ENETC_MSG_CLASS_ID_CMD_SUCCESS		0x1
+#define ENETC_MSG_CLASS_ID_PERMISSION_DENY	0x2
+#define ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT	0x3
+#define ENETC_MSG_CLASS_ID_PSI_BUSY		0x4
+#define ENETC_MSG_CLASS_ID_CRC_ERROR		0x5
+#define ENETC_MSG_CLASS_ID_PROTO_NOT_SUPPORT	0x6
+#define ENETC_MSG_CLASS_ID_INVALID_MSG_LEN	0x7
+#define ENETC_MSG_CLASS_ID_CMD_TIMEOUT		0x8
+#define ENETC_MSG_CLASS_ID_CMD_DEFERED		0xf
+
+/* Class-specific error return codes for MAC filter */
+#define ENETC_PF_RC_MAC_FILTER_INVALID_MAC		0x0
+#define ENETC_PF_RC_MAC_FILTER_DUPLICATE_MAC		0x1
+#define ENETC_PF_RC_MAC_FILTER_MAC_NOT_FOUND		0x2
+#define ENETC_PF_RC_MAC_FILTER_NO_RESOURCE		0x3
+
+/* Class-specific error return codes for VLAN filter */
+#define ENETC_PF_RC_VLAN_FILTER_INVALID_VLAN		0x0
+#define ENETC_PF_RC_VLAN_FILTER_DUPLICATE_VLAN		0x1
+#define ENETC_PF_RC_VLAN_FILTER_VLAN_NOT_FOUND		0x2
+#define ENETC_PF_RC_VLAN_FILTER_NO_RESOURCE		0x3
+
+/* Class-specific notification codes for link status */
+#define ENETC_PF_NC_LINK_STATUS_UP			0x0
+#define ENETC_PF_NC_LINK_STATUS_DOWN			0x1
+
+#define ENETC_MAC_FILTER_TYPE_UC	BIT(0)
+#define ENETC_MAC_FILTER_TYPE_MC	BIT(1)
+#define ENETC_MAC_FILTER_TYPE_ALL	(ENETC_MAC_FILTER_TYPE_UC | \
+					 ENETC_MAC_FILTER_TYPE_MC)
+#define ENETC_MAC_PROMISC_MODE_DISABLE	0
+#define ENETC_MAC_PROMISC_MODE_ENABLE	1
+#define ENETC_MAC_FILTER_FLUSH		1
+#define ENETC_MAC_HASH_TABLE_SIZE_64	0
+#define ENETC_MAC_HASH_TABLE_SIZE_128	1
+#define ENETC_MAC_HASH_TABLE_SIZE_256	2
+#define ENETC_MAC_HASH_TABLE_SIZE_512	3
+
+#define ENETC_VLAN_CTAG_TPID		0
+#define ENETC_VLAN_STAG_TPID		1
+#define ENETC_VLAN_HASH_TABLE_SIZE_64	0
+#define ENETC_VLAN_HASH_TABLE_SIZE_128	1
+#define ENETC_VLAN_HASH_TABLE_SIZE_256	2
+#define ENETC_VLAN_HASH_TABLE_SIZE_512	3
+#define ENETC_VLAN_PROMISC_MODE_DISABLE	0
+#define ENETC_VLAN_PROMISC_MODE_ENABLE	1
+
+enum enetc_msg_mac_filter_cmd_id {
+	ENETC_MSG_SET_PRIMARY_MAC,
+	ENETC_MSG_ADD_EXACT_MAC_ENTRIES,
+	ENETC_MSG_DEL_EXACT_MAC_ENTRIES,
+	ENETC_MSG_SET_MAC_HASH_TABLE,
+	ENETC_MSG_FLUSH_MAC_ENTRIES,
+	ENETC_MSG_SET_MAC_PROMISC_MODE,
+};
+
+enum enetc_msg_vlan_filter_cmd_id {
+	ENETC_MSG_ADD_EXACT_VLAN_ENTRIES,
+	ENETC_MSG_DEL_EXACT_VLAN_ENTRIES,
+	ENETC_MSG_SET_VLAN_HASH_TABLE,
+	ENETC_MSG_FLUSH_VLAN_ENTRIES,
+	ENETC_MSG_SET_VLAN_PROMISC_MODE,
+};
+
+enum enetc_msg_link_status_cmd_id {
+	ENETC_MSG_GET_CURRENT_LINK_STATUS,
+	ENETC_MSG_REGISTER_LINK_CHANGE_NOTIFY,
+	ENETC_MSG_UNREGISTER_LINK_CHANGE_NOTIFY,
+};
+
+enum enetc_msg_link_speed_cmd_id {
+	ENETC_MSG_GET_CURRENT_LINK_SPEED,
+	ENETC_MSG_REGISTER_SPEED_CHANGE_NOTIFY,
+	ENETC_MSG_UNREGISTER_SPEED_CHANGE_NOTIFY,
+};
+
+enum enetc_msg_link_speed_val {
+	ENETC_MSG_SPEED_UNKNOWN,
+	ENETC_MSG_SPEED_10M_HD,
+	ENETC_MSG_SPEED_10M_FD,
+	ENETC_MSG_SPEED_100M_HD,
+	ENETC_MSG_SPEED_100M_FD,
+	ENETC_MSG_SPEED_1000M,
+	ENETC_MSG_SPEED_2500M,
+	ENETC_MSG_SPEED_5G,
+	ENETC_MSG_SPEED_10G,
+	ENETC_MSG_SPEED_25G,
+	ENETC_MSG_SPEED_50G,
+	ENETC_MSG_SPEED_100G,
+};
+
+struct enetc_msg_swbd {
+	void *vaddr;
+	dma_addr_t dma;
+	int size;
+};
+
+/* The format of PSI-TO-VSI message, only a 16-bits code */
+union enetc_pf_msg {
+	struct {
+		u8 cookie:4;
+		u8 class_code:4;
+		u8 class_id;
+	};
+	u16 code;
+};
+
+/* The formats of VSI-TO-PSI messages */
+#pragma pack(1)
+
+struct enetc_msg_header {
+	__be16 crc16;
+	u8 class_id;
+	u8 cmd_id;
+	u8 proto_ver;
+	u8 len;
+	u8 resv0;
+	u8 cookie:4;
+	u8 resv1:4;
+	u8 resv2[8];
+};
+
+/* message format of class_id 0x20, cmd_id: 0x0~0x2
+ * cmd_id 0: set primary MAC
+ * cmd_id 1: add MAC address filter table entries
+ * cmd_id 2: delete MAC address filter table entries
+ */
+struct enetc_mac_entry {
+	u8 addr[ETH_ALEN];
+};
+
+struct enetc_msg_mac_exact_filter {
+	struct enetc_msg_header hdr;
+	u8 mac_cnt;
+	u8 resv[3];
+	struct enetc_mac_entry mac[];
+};
+
+/* message format of class_id 0x20, cmd_id: 0x3, set MAC hash table */
+struct enetc_msg_mac_hash_filter {
+	struct enetc_msg_header hdr;
+	u8 size:6;
+	u8 type:2;
+	u32 hash_tbl[];
+};
+
+/* message format of class_id 0x20, cmd_id: 0x4, flush MAC address
+ * filter table entries
+ */
+struct enetc_msg_mac_filter_flush {
+	struct enetc_msg_header hdr;
+	u8 resv:6;
+	u8 type:2;
+};
+
+/* message format of class_id 0x20, cmd_id: 0x5, set MAC promiscuous mode */
+struct enetc_msg_mac_promsic_mode {
+	struct enetc_msg_header hdr;
+	u8 flush_macs:1;
+	u8 promisc_mode:1;
+	u8 resv:4;
+	u8 type:2;
+};
+
+/* message format of class_id 0x21, cmd_id: 0x0~0x1
+ * cmd_id 0: add VLAN address filter table entries
+ * cmd_id 1: delete VLAN address filter table entries
+ */
+struct enetc_vlan_entry {
+	u16 vid:12;
+	u16 resv0:4;
+	u8 tpid:2;
+	u8 resv1:6;
+	u8 resv2;
+};
+
+struct enetc_msg_vlan_exact_filter {
+	struct enetc_msg_header hdr;
+	u8 vlan_cnt;
+	u8 resv[3];
+	struct enetc_vlan_entry vlan[];
+};
+
+/* message format of class_id 0x21, cmd_id: 0x2, set VLAN hash table */
+struct enetc_msg_vlan_hash_filter {
+	struct enetc_msg_header hdr;
+	u8 size;
+	u8 resv[3];
+	u32 hash_tbl[];
+};
+
+/* message format of class_id 0x21, cmd_id: 0x3, flush VLAN address
+ * filter table entries
+ */
+struct enetc_msg_vlan_filter_flush {
+	struct enetc_msg_header hdr;
+};
+
+/* message format of class_id 0x21, cmd_id: 0x4, set VLAN promiscuous mode */
+struct enetc_msg_vlan_promsic_mode {
+	struct enetc_msg_header hdr;
+	u8 flush_vlans:1;
+	u8 promisc_mode:1;
+	u8 resv:6;
+};
+
+/* message format of class_id 0x80, cmd_id 0x0~0x2
+ * cmd_id 0x0: get the current link status
+ * cmd_id 0x1: register to link status change notification
+ * cmd_id 0x2: unregister from link status change notification
+ */
+struct enetc_msg_link_status {
+	struct enetc_msg_header hdr;
+};
+
+/* message format of class_id 0x80, cmd_id 0x0~0x2
+ * cmd_id 0x0: get the current link speed
+ * cmd_id 0x1: register to link speed change notification
+ * cmd_id 0x2: unregister from link speed change notification
+ */
+struct enetc_msg_link_speed {
+	struct enetc_msg_header hdr;
+};
+
+#pragma pack()
diff --git a/devices/enetc/enetc_pf.c b/devices/enetc/enetc_pf.c
index fc9a1b2..87fba23 100644
--- a/devices/enetc/enetc_pf.c
+++ b/devices/enetc/enetc_pf.c
@@ -32,6 +32,189 @@ static void enetc_pf_set_primary_mac_addr(struct enetc_hw *hw, int si,
 	__raw_writew(lower, hw->port + ENETC_PSIPMAR1(si));
 }
 
+static void enetc_set_si_vlan_promisc(struct enetc_hw *hw, int si, bool en)
+{
+    u32 val = enetc_port_rd(hw, ENETC_PSIPVMR);
+
+    if (en)
+        val |= BIT(si);
+    else
+        val &= ~BIT(si);
+
+    enetc_port_wr(hw, ENETC_PSIPVMR, val);
+}
+
+static void enetc_set_isol_vlan(struct enetc_hw *hw, int si, u16 vlan, u8 qos)
+{
+    u32 val = 0;
+
+    if (vlan)
+        val = ENETC_PSIVLAN_EN | ENETC_PSIVLAN_SET_QOS(qos) | vlan;
+
+    enetc_port_wr(hw, ENETC_PSIVLANR(si), val);
+}
+
+static void enetc_add_mac_addr_em_filter(struct enetc_mac_filter *filter,
+                     const unsigned char *addr)
+{
+    /* add exact match addr */
+    ether_addr_copy(filter->mac_addr, addr);
+    filter->mac_addr_cnt++;
+}
+
+static void enetc_pf_set_si_mac_promisc(struct enetc_hw *hw, int si, int type, bool en)
+{
+    u32 val = enetc_port_rd(hw, ENETC_PSIPMR);
+
+    if (type == UC) {
+        if (en)
+            val |= ENETC_PSIPMR_SET_UP(si);
+        else
+            val &= ~ENETC_PSIPMR_SET_UP(si);
+    } else { /* Multicast promiscuous mode. */
+        if (en)
+            val |= ENETC_PSIPMR_SET_MP(si);
+        else
+            val &= ~ENETC_PSIPMR_SET_MP(si);
+    }
+
+    enetc_port_wr(hw, ENETC_PSIPMR, val);
+}
+
+static void enetc_set_mac_ht_flt(struct enetc_hw *hw, int si_idx, int type,
+                 u64 hash)
+{
+    struct enetc_si *si = container_of(hw, struct enetc_si, hw);
+    bool err = si->errata & ENETC_ERR_UCMCSWP;
+
+    if (type == UC) {
+        enetc_port_wr(hw, ENETC_PSIUMHFR0(si_idx, err),
+                  lower_32_bits(hash));
+        enetc_port_wr(hw, ENETC_PSIUMHFR1(si_idx),
+                  upper_32_bits(hash));
+    } else { /* MC */
+        enetc_port_wr(hw, ENETC_PSIMMHFR0(si_idx, err),
+                  lower_32_bits(hash));
+        enetc_port_wr(hw, ENETC_PSIMMHFR1(si_idx),
+                  upper_32_bits(hash));
+    }
+}
+
+static void enetc_sync_mac_filters(struct enetc_si *si)
+{
+    struct enetc_mac_filter *f = si->mac_filter;
+    struct enetc_pf *pf = enetc_si_priv(si);
+    struct enetc_hw *hw = &si->hw;
+    int i, pos;
+
+    if (!pf->hw_ops->set_si_mac_hash_filter)
+        return;
+
+    pos = EMETC_MAC_ADDR_FILT_RES;
+
+    for (i = 0; i < MADDR_TYPE; i++, f++) {
+        bool em = (f->mac_addr_cnt == 1) && (i == UC);
+        bool clear = !f->mac_addr_cnt;
+
+        if (clear) {
+            if (i == UC)
+                ec_enetc_clear_mac_flt_entry(si, pos);
+
+            /* Clean MAC hash filter. */
+            pf->hw_ops->set_si_mac_hash_filter(hw, 0, i, 0);
+            continue;
+        }
+
+        /* exact match filter */
+        if (em) {
+            int err;
+
+            /* Clean MAC hash filter. */
+            pf->hw_ops->set_si_mac_hash_filter(hw, 0, i, 0);
+
+            err = ec_enetc_set_mac_flt_entry(si, pos, f->mac_addr,
+                              BIT(0));
+            if (!err)
+                continue;
+
+            /* fallback to HT filtering */
+            dev_warn(&si->pdev->dev, "fallback to HT filt (%d)\n",
+                 err);
+        }
+
+        /* hash table filter, clear EM filter for UC entries */
+        if (i == UC)
+            ec_enetc_clear_mac_flt_entry(si, pos);
+
+        pf->hw_ops->set_si_mac_hash_filter(hw, 0, i, *f->mac_hash_table);
+    }
+}
+
+static void enetc_pf_set_rx_mode(struct net_device *ndev)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct enetc_pf *pf = enetc_si_priv(priv->si);
+    bool uprom = false, mprom = false;
+    struct enetc_mac_filter *filter;
+    struct enetc_si *si = priv->si;
+    struct enetc_hw *hw = &si->hw;
+    struct netdev_hw_addr *ha;
+    bool em;
+
+    if (ndev->flags & IFF_PROMISC) {
+        uprom = true;
+        mprom = true;
+    } else if (ndev->flags & IFF_ALLMULTI) {
+        mprom = true;
+    }
+
+    /* first 2 filter entries belong to PF */
+    if (!uprom) {
+        /* Update unicast filters */
+        filter = &si->mac_filter[UC];
+        ec_enetc_reset_mac_addr_filter(filter);
+
+        em = (netdev_uc_count(ndev) == 1);
+        netdev_for_each_uc_addr(ha, ndev) {
+            if (em) {
+                enetc_add_mac_addr_em_filter(filter, ha->addr);
+                break;
+            }
+
+            ec_enetc_add_mac_addr_ht_filter(filter, ha->addr);
+        }
+    }
+
+    if (!mprom) {
+        /* Update multicast filters */
+        filter = &si->mac_filter[MC];
+        ec_enetc_reset_mac_addr_filter(filter);
+
+        netdev_for_each_mc_addr(ha, ndev) {
+            if (!is_multicast_ether_addr(ha->addr))
+                continue;
+
+            ec_enetc_add_mac_addr_ht_filter(filter, ha->addr);
+        }
+    }
+
+    if (!uprom || !mprom)
+        /* update PF entries */
+        enetc_sync_mac_filters(si);
+
+    if (pf->hw_ops->set_si_mac_promisc) {
+        pf->hw_ops->set_si_mac_promisc(hw, 0, UC, uprom);
+        pf->hw_ops->set_si_mac_promisc(hw, 0, MC, mprom);
+    }
+}
+
+static void enetc_set_vlan_ht_filter(struct enetc_hw *hw, int si_idx,
+                     u64 hash)
+{
+    enetc_port_wr(hw, ENETC_PSIVHFR0(si_idx), lower_32_bits(hash));
+    enetc_port_wr(hw, ENETC_PSIVHFR1(si_idx), upper_32_bits(hash));
+}
+
 static void enetc_port_setup_primary_mac_address(struct enetc_si *si)
 {
 	unsigned char mac_addr[MAX_ADDR_LEN];
@@ -123,40 +306,43 @@ static void enetc_port_si_configure(struct enetc_si *si)
 	enetc_port_wr(hw, ENETC_PSIVLANFMR, ENETC_PSIVLANFMR_VS);
 }
 
-static void enetc_configure_port_mac(struct enetc_hw *hw)
+static void enetc_set_ptcmsdur(struct enetc_hw *hw, u32 *max_sdu)
 {
-	enetc_port_wr(hw, ENETC_PM0_MAXFRM,
-		      ENETC_SET_MAXFRM(ENETC_RX_MAXFRM_SIZE));
+    int tc;
 
-	enetc_port_wr(hw, ENETC_PTCMSDUR(0), ENETC_MAC_MAXFRM_SIZE);
-	enetc_port_wr(hw, ENETC_PTXMBAR, 2 * ENETC_MAC_MAXFRM_SIZE);
+    for (tc = 0; tc < 8; tc++) {
+        u32 val = ENETC_MAC_MAXFRM_SIZE;
 
-	enetc_port_wr(hw, ENETC_PM0_CMD_CFG, ENETC_PM0_CMD_PHY_TX_EN |
-		      ENETC_PM0_CMD_TXP	| ENETC_PM0_PROMISC);
+        if (max_sdu[tc])
+            val = max_sdu[tc] + VLAN_ETH_HLEN;
 
-	enetc_port_wr(hw, ENETC_PM1_CMD_CFG, ENETC_PM0_CMD_PHY_TX_EN |
-		      ENETC_PM0_CMD_TXP	| ENETC_PM0_PROMISC);
+        enetc_port_wr(hw, ENETC_PTCMSDUR(tc), val);
+    }
 }
 
-static void enetc_mac_config(struct enetc_hw *hw, phy_interface_t phy_mode)
+static void enetc_reset_ptcmsdur(struct enetc_hw *hw)
 {
-	/* set auto-speed for RGMII */
-	if (enetc_port_rd(hw, ENETC_PM0_IF_MODE) & ENETC_PMO_IFM_RG ||
-	    phy_interface_mode_is_rgmii(phy_mode)) {
-		enetc_port_wr(hw, ENETC_PM0_IF_MODE, ENETC_PM0_IFM_RGAUTO);
-		enetc_port_wr(hw, ENETC_PM1_IF_MODE, ENETC_PM0_IFM_RGAUTO);
-	}
+    int tc;
 
-	if (phy_mode == PHY_INTERFACE_MODE_USXGMII) {
-		enetc_port_wr(hw, ENETC_PM0_IF_MODE, ENETC_PM0_IFM_XGMII);
-		enetc_port_wr(hw, ENETC_PM1_IF_MODE, ENETC_PM0_IFM_XGMII);
-	}
+    for (tc = 0; tc < 8; tc++)
+        enetc_port_wr(hw, ENETC_PTCMSDUR(tc), ENETC_MAC_MAXFRM_SIZE);
+}
+
+static void enetc_mac_config(struct enetc_si *si, phy_interface_t phy_mode)
+{
+    u32 val;
 
-	/* on LS1028A the MAC Rx FIFO defaults to value 2, which is too high and
-	 * may lead to Rx lock-up under traffic.  Set it to 1 instead, as
-	 * recommended by the hardware team.
-	 */
-	enetc_port_wr(hw, ENETC_PM0_RX_FIFO, ENETC_PM0_RX_FIFO_VAL);
+    if (phy_interface_mode_is_rgmii(phy_mode)) {
+        val = ec_enetc_port_mac_rd(si, ENETC_PM0_IF_MODE);
+        val &= ~(ENETC_PM0_IFM_EN_AUTO | ENETC_PM0_IFM_IFMODE_MASK);
+        val |= ENETC_PM0_IFM_IFMODE_GMII | ENETC_PM0_IFM_RG;
+        ec_enetc_port_mac_wr(si, ENETC_PM0_IF_MODE, val);
+    }
+
+    if (phy_mode == PHY_INTERFACE_MODE_USXGMII) {
+        val = ENETC_PM0_IFM_FULL_DPX | ENETC_PM0_IFM_IFMODE_XGMII;
+        ec_enetc_port_mac_wr(si, ENETC_PM0_IF_MODE, val);
+    }
 }
 
 static void enetc_mac_enable(struct enetc_hw *hw, bool en)
@@ -170,265 +356,125 @@ static void enetc_mac_enable(struct enetc_hw *hw, bool en)
 	enetc_port_wr(hw, ENETC_PM1_CMD_CFG, val);
 }
 
-static void enetc_configure_port_pmac(struct enetc_hw *hw)
+static void enetc_configure_port_mac(struct enetc_si *si)
 {
-	u32 temp;
-
-	/* Set pMAC step lock */
-	temp = enetc_port_rd(hw, ENETC_PFPMR);
-	enetc_port_wr(hw, ENETC_PFPMR,
-		      temp | ENETC_PFPMR_PMACE | ENETC_PFPMR_MWLM);
-
-	temp = enetc_port_rd(hw, ENETC_MMCSR);
-	enetc_port_wr(hw, ENETC_MMCSR, temp | ENETC_MMCSR_ME);
-}
-
-static void enetc_configure_port(struct enetc_pf *pf)
-{
-	u8 hash_key[ENETC_RSSHASH_KEY_SIZE];
-	struct enetc_hw *hw = &pf->si->hw;
-
-	enetc_configure_port_pmac(hw);
-
-	enetc_configure_port_mac(hw);
-
-	enetc_port_si_configure(pf->si);
-
-	/* set up hash key */
-	get_random_bytes(hash_key, ENETC_RSSHASH_KEY_SIZE);
-	enetc_set_rss_key(hw, hash_key);
-
-	/* split up RFS entries */
-	enetc_port_assign_rfs_entries(pf->si);
+    struct enetc_hw *hw = &si->hw;
 
-	/* fix-up primary MAC addresses, if not set already */
-	enetc_port_setup_primary_mac_address(pf->si);
+    ec_enetc_port_mac_wr(si, ENETC_PM0_MAXFRM,
+              ENETC_SET_MAXFRM(ENETC_RX_MAXFRM_SIZE));
 
-	/* enforce VLAN promisc mode for all SIs */
-	pf->vlan_promisc_simap = ENETC_VLAN_PROMISC_MAP_ALL;
+    enetc_reset_ptcmsdur(hw);
 
-	enetc_port_wr(hw, ENETC_PSIPMR, 0);
+    ec_enetc_port_mac_wr(si, ENETC_PM0_CMD_CFG, ENETC_PM0_CMD_PHY_TX_EN |
+              ENETC_PM0_CMD_TXP | ENETC_PM0_PROMISC);
 
-	/* enable port */
-	enetc_port_wr(hw, ENETC_PMR, ENETC_PMR_EN);
+    /* On LS1028A, the MAC RX FIFO defaults to 2, which is too high
+     * and may lead to RX lock-up under traffic. Set it to 1 instead,
+     * as recommended by the hardware team.
+     */
+    ec_enetc_port_mac_wr(si, ENETC_PM0_RX_FIFO, ENETC_PM0_RX_FIFO_VAL);
 }
 
-static const struct net_device_ops enetc_ndev_ops = {
-	.ndo_open		= enetc_open,
-	.ndo_stop		= enetc_close,
-	.ndo_start_xmit		= enetc_xmit,
-};
-
-static void enetc_pf_netdev_setup(struct enetc_si *si, struct net_device *ndev,
-				  const struct net_device_ops *ndev_ops)
-{
-	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-
-	SET_NETDEV_DEV(ndev, &si->pdev->dev);
-	priv->ndev = ndev;
-	priv->si = si;
-	priv->dev = &si->pdev->dev;
-	si->ndev = ndev;
-
-	priv->msg_enable = (NETIF_MSG_WOL << 1) - 1;
-	ndev->netdev_ops = ndev_ops;
-	enetc_set_ethtool_ops(ndev);
-	ndev->watchdog_timeo = 5 * HZ;
-	ndev->max_mtu = ENETC_MAX_MTU;
-
-	ndev->hw_features = NETIF_F_SG | NETIF_F_RXCSUM | NETIF_F_HW_CSUM |
-			    NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX |
-			    NETIF_F_HW_VLAN_CTAG_FILTER | NETIF_F_LOOPBACK;
-	ndev->features = NETIF_F_HIGHDMA | NETIF_F_SG |
-			 NETIF_F_RXCSUM | NETIF_F_HW_CSUM |
-			 NETIF_F_HW_VLAN_CTAG_TX |
-			 NETIF_F_HW_VLAN_CTAG_RX;
-
-	if (si->num_rss)
-		ndev->hw_features |= NETIF_F_RXHASH;
-
-	if (si->errata & ENETC_ERR_TXCSUM) {
-		ndev->hw_features &= ~NETIF_F_HW_CSUM;
-		ndev->features &= ~NETIF_F_HW_CSUM;
-	}
-
-	ndev->priv_flags |= IFF_UNICAST_FLT;
-
-	if (si->hw_features & ENETC_SI_F_QBV)
-		priv->active_offloads |= ENETC_F_QBV;
-
-	if (si->hw_features & ENETC_SI_F_PSFP && !enetc_psfp_enable(priv)) {
-		priv->active_offloads |= ENETC_F_QCI;
-		ndev->features |= NETIF_F_HW_TC;
-		ndev->hw_features |= NETIF_F_HW_TC;
-	}
-
-	/* pick up primary MAC address from SI */
-	enetc_get_primary_mac_addr(&si->hw, ndev->dev_addr);
+static void enetc_set_loopback(struct net_device *ndev, bool en)
+{
+    struct enetc_ndev_priv *priv = netdev_priv(ndev);
+    struct enetc_si *si = priv->si;
+    u32 reg;
+
+    reg = ec_enetc_port_mac_rd(si, ENETC_PM0_IF_MODE);
+    if (reg & ENETC_PM0_IFM_RG) {
+        /* RGMII mode */
+        reg = (reg & ~ENETC_PM0_IFM_RLP) |
+              (en ? ENETC_PM0_IFM_RLP : 0);
+        ec_enetc_port_mac_wr(si, ENETC_PM0_IF_MODE, reg);
+    } else {
+        /* assume SGMII mode */
+        reg = ec_enetc_port_mac_rd(si, ENETC_PM0_CMD_CFG);
+        reg = (reg & ~ENETC_PM0_CMD_XGLP) |
+              (en ? ENETC_PM0_CMD_XGLP : 0);
+        reg = (reg & ~ENETC_PM0_CMD_PHY_TX_EN) |
+              (en ? ENETC_PM0_CMD_PHY_TX_EN : 0);
+        ec_enetc_port_mac_wr(si, ENETC_PM0_CMD_CFG, reg);
+    }
 }
 
-static int enetc_mdio_probe(struct enetc_pf *pf, struct device_node *np)
+static void enetc_pf_set_si_anti_spoofing(struct enetc_hw *hw, int si, bool en)
 {
-	struct device *dev = &pf->si->pdev->dev;
-	struct enetc_mdio_priv *mdio_priv;
-	struct mii_bus *bus;
-	int err;
-
-	bus = devm_mdiobus_alloc_size(dev, sizeof(*mdio_priv));
-	if (!bus)
-		return -ENOMEM;
-
-	bus->name = "Freescale ENETC MDIO Bus";
-    bus->read = enetc_mdio_read_c22;
-    bus->write = enetc_mdio_write_c22;
-    bus->read_c45 = enetc_mdio_read_c45;
-    bus->write_c45 = enetc_mdio_write_c45;
-    bus->parent = dev;
-	mdio_priv = bus->priv;
-	mdio_priv->hw = &pf->si->hw;
-	mdio_priv->mdio_base = ENETC_EMDIO_BASE;
-	snprintf(bus->id, MII_BUS_ID_SIZE, "%s", dev_name(dev));
-
-	err = of_mdiobus_register(bus, np);
-	if (err) {
-		dev_err(dev, "cannot register MDIO bus\n");
-		return err;
-	}
-
-	pf->mdio = bus;
+    u32 val = enetc_port_rd(hw, ENETC_PSICFGR0(si));
 
-	return 0;
+    val = (val & ~ENETC_PSICFGR0_ASE) | (en ? ENETC_PSICFGR0_ASE : 0);
+    enetc_port_wr(hw, ENETC_PSICFGR0(si), val);
 }
 
-static void enetc_mdio_remove(struct enetc_pf *pf)
+static void enetc_pf_set_tc_tsd(struct enetc_hw *hw, int tc, bool en)
 {
-	if (pf->mdio)
-		mdiobus_unregister(pf->mdio);
+    enetc_port_wr(hw, ENETC_PTCTSDR(tc), en ? ENETC_TSDE : 0);
 }
 
-static int enetc_imdio_create(struct enetc_pf *pf)
+static bool enetc_pf_get_time_gating(struct enetc_hw *hw)
 {
-    struct device *dev = &pf->si->pdev->dev;
-    struct enetc_mdio_priv *mdio_priv;
-    struct phylink_pcs *phylink_pcs;
-    struct mii_bus *bus;
-    struct phy *serdes;
-    size_t num_phys;
-    int err;
-
-    serdes = devm_of_phy_optional_get(dev, dev->of_node, NULL);
-    if (IS_ERR(serdes))
-        return PTR_ERR(serdes);
-
-    num_phys = serdes ? 1 : 0;
-
-    bus = mdiobus_alloc_size(sizeof(*mdio_priv));
-    if (!bus)
-        return -ENOMEM;
-
-    bus->name = "Freescale ENETC internal MDIO Bus";
-    bus->read = enetc_mdio_read_c22;
-    bus->write = enetc_mdio_write_c22;
-    bus->read_c45 = enetc_mdio_read_c45;
-    bus->write_c45 = enetc_mdio_write_c45;
-    bus->parent = dev;
-    bus->phy_mask = ~0;
-    mdio_priv = bus->priv;
-    mdio_priv->hw = &pf->si->hw;
-    mdio_priv->mdio_base = ENETC_PM_IMDIO_BASE;
-    snprintf(bus->id, MII_BUS_ID_SIZE, "%s-imdio", dev_name(dev));
-
-    mdio_priv->regulator = devm_regulator_get_optional(dev, "serdes");
-    if (IS_ERR(mdio_priv->regulator)) {
-        err = PTR_ERR(mdio_priv->regulator);
-        if (err == -EPROBE_DEFER)
-            goto free_mdio_bus;
-        mdio_priv->regulator = NULL;
-    }
-
-    if (mdio_priv->regulator) {
-        err = regulator_enable(mdio_priv->regulator);
-        if (err) {
-            dev_err(dev, "fail to enable phy-supply\n");
-            goto free_mdio_bus;
-        }
-    }
-
-    err = mdiobus_register(bus);
-    if (err) {
-        dev_err(dev, "cannot register internal MDIO bus (%d)\n", err);
-        goto free_mdio_bus;
-    }
-
-    phylink_pcs = lynx_pcs_create_mdiodev(bus, 0, &serdes, num_phys);
-    if (IS_ERR(phylink_pcs)) {
-        err = PTR_ERR(phylink_pcs);
-        dev_err(dev, "cannot create lynx pcs (%d)\n", err);
-        goto unregister_mdiobus;
-    }
-
-    pf->imdio = bus;
-    pf->pcs = phylink_pcs;
-
-    return 0;
-
-unregister_mdiobus:
-    mdiobus_unregister(bus);
-free_mdio_bus:
-    mdiobus_free(bus);
-    return err;
+    return !!(enetc_rd(hw, ENETC_PTGCR) & ENETC_PTGCR_TGE);
 }
 
-static void enetc_imdio_remove(struct enetc_pf *pf)
+static void enetc_pf_set_time_gating(struct enetc_hw *hw, bool en)
 {
-	if (pf->pcs) {
-		lynx_pcs_destroy(pf->pcs);
-	}
-	if (pf->imdio) {
-		mdiobus_unregister(pf->imdio);
-		mdiobus_free(pf->imdio);
-	}
-}
+    u32 old_val, val;
 
-static bool enetc_port_has_pcs(struct enetc_pf *pf)
-{
-	return (pf->if_mode == PHY_INTERFACE_MODE_SGMII ||
-		pf->if_mode == PHY_INTERFACE_MODE_2500BASEX ||
-		pf->if_mode == PHY_INTERFACE_MODE_USXGMII);
+    old_val = enetc_rd(hw, ENETC_PTGCR);
+    val = u32_replace_bits(old_val, en ? 1 : 0, ENETC_PTGCR_TGE);
+    if (val != old_val)
+        enetc_wr(hw, ENETC_PTGCR, val);
 }
 
-static int enetc_mdiobus_create(struct enetc_pf *pf)
+static void enetc_configure_port(struct enetc_pf *pf)
 {
-	struct device *dev = &pf->si->pdev->dev;
-	struct device_node *mdio_np;
-	int err;
+    u8 hash_key[ENETC_RSSHASH_KEY_SIZE];
+    struct enetc_hw *hw = &pf->si->hw;
+    u32 val;
 
-	mdio_np = of_get_child_by_name(dev->of_node, "mdio");
-	if (mdio_np) {
-		err = enetc_mdio_probe(pf, mdio_np);
+    enetc_configure_port_mac(pf->si);
 
-		of_node_put(mdio_np);
-		if (err)
-			return err;
-	}
+    enetc_port_si_configure(pf->si);
 
-	if (enetc_port_has_pcs(pf)) {
-		err = enetc_imdio_create(pf);
-		if (err) {
-			enetc_mdio_remove(pf);
-			return err;
-		}
-	}
+    /* set up hash key */
+    get_random_bytes(hash_key, ENETC_RSSHASH_KEY_SIZE);
+    ec_enetc_set_rss_key(hw, hash_key);
 
-	return 0;
-}
+    /* split up RFS entries */
+    enetc_port_assign_rfs_entries(pf->si);
 
-static void enetc_mdiobus_destroy(struct enetc_pf *pf)
-{
-	enetc_mdio_remove(pf);
-	enetc_imdio_remove(pf);
+    /* enforce VLAN promisc mode for all SIs */
+    val = enetc_port_rd(hw, ENETC_PSIPVMR);
+    val |= ENETC_VLAN_PROMISC_MAP_ALL;
+    enetc_port_wr(hw, ENETC_PSIPVMR, val);
+
+    enetc_port_wr(hw, ENETC_PSIPMR, 0);
+
+    /* enable port */
+    enetc_port_wr(hw, ENETC_PMR, ENETC_PMR_EN);
 }
 
+static const struct net_device_ops enetc_ndev_ops = {
+	.ndo_open		= ec_enetc_open,
+	.ndo_stop		= ec_enetc_close,
+	.ndo_start_xmit		= ec_enetc_xmit,
+    .ndo_get_stats      = ec_enetc_get_stats,
+    .ndo_set_mac_address    = enetc_pf_set_mac_addr,
+    .ndo_set_rx_mode    = enetc_pf_set_rx_mode,
+    .ndo_vlan_rx_add_vid    = ec_enetc_vlan_rx_add_vid,
+    .ndo_vlan_rx_kill_vid   = ec_enetc_vlan_rx_del_vid,
+    .ndo_set_vf_mac     = enetc_pf_set_vf_mac,
+    .ndo_set_vf_vlan    = enetc_pf_set_vf_vlan,
+    .ndo_set_vf_spoofchk    = enetc_pf_set_vf_spoofchk,
+    .ndo_set_vf_trust   = enetc_pf_set_vf_trust,
+    .ndo_get_vf_config  = enetc_pf_get_vf_config,
+    .ndo_set_features   = enetc_pf_set_features,
+    .ndo_eth_ioctl      = ec_enetc_ioctl,
+    .ndo_setup_tc       = enetc_pf_setup_tc,
+    .ndo_bpf        = ec_enetc_setup_bpf,
+    .ndo_xdp_xmit       = ec_enetc_xdp_xmit,
+};
+
 static void enetc_pl_mac_validate(struct phylink_config *config,
 				  unsigned long *supported,
 				  struct phylink_link_state *state)
@@ -470,27 +516,50 @@ static void enetc_pl_mac_validate(struct phylink_config *config,
 		   __ETHTOOL_LINK_MODE_MASK_NBITS);
 }
 
+static struct phylink_pcs *
+enetc_pl_mac_select_pcs(struct phylink_config *config, phy_interface_t iface)
+{
+    struct enetc_pf *pf = phylink_to_enetc_pf(config);
+
+    return pf->pcs;
+}
+
 static void enetc_pl_mac_config(struct phylink_config *config,
-				unsigned int mode,
-				const struct phylink_link_state *state)
+                unsigned int mode,
+                const struct phylink_link_state *state)
 {
-	struct enetc_pf *pf = phylink_to_enetc_pf(config);
-	struct enetc_ndev_priv *priv;
+    struct enetc_pf *pf = phylink_to_enetc_pf(config);
 
-	enetc_mac_config(&pf->si->hw, state->interface);
+    enetc_mac_config(pf->si, state->interface);
+}
 
-	priv = netdev_priv(pf->si->ndev);
-	if (pf->pcs) {
-		priv->phylink->pcs = pf->pcs;
-
-		if (!priv->phylink->phylink_disable_state &&
-			priv->phylink->cfg_link_an_mode == MLO_AN_INBAND) {
-			if (pf->pcs->poll)
-				mod_timer(&priv->phylink->link_poll, jiffies + HZ);
-			else
-				del_timer(&priv->phylink->link_poll);
-		}
-	}
+static void enetc_sched_speed_set(struct enetc_ndev_priv *priv, int speed)
+{
+    struct enetc_hw *hw = &priv->si->hw;
+    u32 old_speed = priv->speed;
+    u32 pspeed, tmp;
+
+    if (speed == old_speed)
+        return;
+
+    switch (speed) {
+    case SPEED_1000:
+        pspeed = ENETC_PMR_PSPEED_1000M;
+        break;
+    case SPEED_2500:
+        pspeed = ENETC_PMR_PSPEED_2500M;
+        break;
+    case SPEED_100:
+        pspeed = ENETC_PMR_PSPEED_100M;
+        break;
+    case SPEED_10:
+    default:
+        pspeed = ENETC_PMR_PSPEED_10M;
+    }
+
+    priv->speed = speed;
+    tmp = enetc_port_rd(hw, ENETC_PMR);
+    enetc_port_wr(hw, ENETC_PMR, (tmp & ~ENETC_PMR_PSPEED_MASK) | pspeed);
 }
 
 static void enetc_pl_mac_link_up(struct phylink_config *config,
@@ -523,44 +592,16 @@ static void enetc_pl_mac_link_down(struct phylink_config *config,
 }
 
 static const struct phylink_mac_ops enetc_mac_phylink_ops = {
-	.validate = enetc_pl_mac_validate,
+	.mac_select_pcs = enetc_pl_mac_select_pcs,
 	.mac_config = enetc_pl_mac_config,
 	.mac_link_up = enetc_pl_mac_link_up,
 	.mac_link_down = enetc_pl_mac_link_down,
 };
 
-static int enetc_phylink_create(struct enetc_ndev_priv *priv)
-{
-	struct enetc_pf *pf = enetc_si_priv(priv->si);
-	struct device *dev = &pf->si->pdev->dev;
-	struct phylink *phylink;
-	int err;
-
-	pf->phylink_config.dev = &priv->ndev->dev;
-	pf->phylink_config.type = PHYLINK_NETDEV;
-
-	phylink = phylink_create(&pf->phylink_config,
-				 of_fwnode_handle(dev->of_node),
-				 pf->if_mode, &enetc_mac_phylink_ops);
-	if (IS_ERR(phylink)) {
-		err = PTR_ERR(phylink);
-		return err;
-	}
-
-	priv->phylink = phylink;
-
-	return 0;
-}
-
-static void enetc_phylink_destroy(struct enetc_ndev_priv *priv)
-{
-	if (priv->phylink)
-		phylink_destroy(priv->phylink);
-}
-
 static int enetc_pf_probe(struct pci_dev *pdev,
 			  const struct pci_device_id *ent)
 {
+	struct device_node *node = pdev->dev.of_node;
 	struct enetc_ndev_priv *priv;
 	struct pci_dev *ptp_pdev;
 	struct net_device *ndev;
@@ -573,7 +614,7 @@ static int enetc_pf_probe(struct pci_dev *pdev,
 		return -ENODEV;
 	}
 
-	err = enetc_pci_probe(pdev, KBUILD_MODNAME, sizeof(*pf));
+	err = ec_enetc_pci_probe(pdev, KBUILD_MODNAME, sizeof(*pf));
 	if (err) {
 		dev_err(&pdev->dev, "PCI probing failed\n");
 		return err;
@@ -592,7 +633,7 @@ static int enetc_pf_probe(struct pci_dev *pdev,
 
 	enetc_configure_port(pf);
 
-	enetc_get_si_caps(si);
+	ec_enetc_get_si_caps(si);
 
 	ndev = alloc_etherdev(sizeof(*priv));
 	if (!ndev) {
@@ -605,9 +646,9 @@ static int enetc_pf_probe(struct pci_dev *pdev,
 
 	priv = netdev_priv(ndev);
 
-	enetc_init_si_rings_params(priv);
+	ec_enetc_init_si_rings_params(priv);
 
-	err = enetc_alloc_si_resources(priv);
+	err = ec_enetc_alloc_si_resources(priv);
 	if (err) {
 		dev_err(&pdev->dev, "SI resource alloc failed\n");
 		goto err_alloc_si_res;
@@ -626,11 +667,11 @@ static int enetc_pf_probe(struct pci_dev *pdev,
 		goto err_get_ptp;
 
 	if (!of_get_phy_mode(pdev->dev.of_node, &pf->if_mode)) {
-		err = enetc_mdiobus_create(pf);
+		err = enetc_mdiobus_create(pf, node);
 		if (err)
 			goto err_mdiobus_create;
 
-		err = enetc_phylink_create(priv);
+		err = enetc_phylink_create(priv, node, &enetc_mac_phylink_ops);
 		if (err)
 			goto err_phylink_create;
 	}
@@ -649,14 +690,14 @@ err_phylink_create:
 	enetc_mdiobus_destroy(pf);
 err_mdiobus_create:
 err_get_ptp:
-	enetc_free_si_resources(priv);
+	ec_enetc_free_si_resources(priv);
 	enetc_free_rings(priv);
 err_alloc_si_res:
 	si->ndev = NULL;
 	free_netdev(ndev);
 err_alloc_netdev:
 err_map_pf_space:
-	enetc_pci_remove(pdev);
+	ec_enetc_pci_remove(pdev);
 
 	return err;
 }
@@ -675,13 +716,30 @@ static void enetc_pf_remove(struct pci_dev *pdev)
 	enetc_phylink_destroy(priv);
 	enetc_mdiobus_destroy(pf);
 
-	enetc_free_si_resources(priv);
+	ec_enetc_free_si_resources(priv);
 
 	free_netdev(si->ndev);
 
-	enetc_pci_remove(pdev);
+	ec_enetc_pci_remove(pdev);
 }
 
+static const struct enetc_pf_hw_ops enetc_pf_hw_ops = {
+    .set_si_primary_mac = enetc_pf_set_primary_mac_addr,
+    .get_si_primary_mac = enetc_pf_get_primary_mac_addr,
+    .set_si_based_vlan = enetc_set_isol_vlan,
+    .set_si_vlan_promisc = enetc_set_si_vlan_promisc,
+    .set_si_mac_promisc = enetc_pf_set_si_mac_promisc,
+    .set_si_mac_hash_filter = enetc_set_mac_ht_flt,
+    .set_si_vlan_hash_filter = enetc_set_vlan_ht_filter,
+    .set_loopback = enetc_set_loopback,
+    .set_si_anti_spoofing = enetc_pf_set_si_anti_spoofing,
+    .set_tc_tsd = enetc_pf_set_tc_tsd,
+    .set_tc_msdu = enetc_set_ptcmsdur,
+    .reset_tc_msdu = enetc_reset_ptcmsdur,
+    .get_time_gating = enetc_pf_get_time_gating,
+    .set_time_gating = enetc_pf_set_time_gating,
+};
+
 static const struct pci_device_id enetc_pf_id_table[] = {
 	{ PCI_DEVICE(PCI_VENDOR_ID_FREESCALE, ENETC_DEV_ID_PF) },
 	{ 0, } /* End of table. */
diff --git a/devices/enetc/enetc_pf.h b/devices/enetc/enetc_pf.h
index 693e500..21b388c 100644
--- a/devices/enetc/enetc_pf.h
+++ b/devices/enetc/enetc_pf.h
@@ -2,21 +2,17 @@
 /* Copyright 2017-2019 NXP */
 
 #include "enetc.h"
+#include "enetc_devlink.h"
 #include <linux/phylink.h>
 
 #define ENETC_PF_NUM_RINGS	8
 
-enum enetc_mac_addr_type {UC, MC, MADDR_TYPE};
-#define ENETC_MAX_NUM_MAC_FLT	((ENETC_MAX_NUM_VFS + 1) * MADDR_TYPE)
+/* This means the ENETC PF is owned by M core, but its VFs are
+ * owned by A core.
+ */
+#define ENETC_PF_VIRTUAL_DEVID  0x080b
 
-#define ENETC_MADDR_HASH_TBL_SZ	64
-struct enetc_mac_filter {
-	union {
-		char mac_addr[ETH_ALEN];
-		DECLARE_BITMAP(mac_hash_table, ENETC_MADDR_HASH_TBL_SZ);
-	};
-	int mac_addr_cnt;
-};
+#define ENETC_MAX_NUM_MAC_FLT	((ENETC_MAX_NUM_VFS + 1) * MADDR_TYPE)
 
 struct phylink {
 	/* private: */
@@ -66,21 +62,76 @@ struct phylink {
 
 enum enetc_vf_flags {
 	ENETC_VF_FLAG_PF_SET_MAC	= BIT(0),
+	ENETC_VF_FLAG_TRUSTED       = BIT(1)
 };
 
 struct enetc_vf_state {
 	enum enetc_vf_flags flags;
 };
 
+struct enetc_port_caps {
+    u32 half_duplex:1;
+    u32 wol:1;
+    int num_vsi;
+    int num_msix;
+    int num_rx_bdr;
+    int num_tx_bdr;
+    int mac_filter_num;
+    int vlan_filter_num;
+    int ipf_words_num;
+};
+
+struct enetc_pf_hw_ops {
+    void (*set_si_primary_mac)(struct enetc_hw *hw, int si, const u8 *addr);
+    void (*get_si_primary_mac)(struct enetc_hw *hw, int si, u8 *addr);
+    void (*set_si_based_vlan)(struct enetc_hw *hw, int si, u16 vlan, u8 qos);
+    void (*get_si_based_vlan)(struct enetc_hw *hw, int si, u32 *vlan, u32 *qos);
+    void (*set_si_anti_spoofing)(struct enetc_hw *hw, int si, bool en);
+    void (*set_si_vlan_promisc)(struct enetc_hw *hw, int si, bool en);
+    void (*set_si_mac_promisc)(struct enetc_hw *hw, int si, int type, bool en);
+    void (*set_si_mac_hash_filter)(struct enetc_hw *hw, int si, int type, u64 hash);
+    void (*set_si_vlan_hash_filter)(struct enetc_hw *hw, int si, u64 hash);
+    void (*set_loopback)(struct net_device *ndev, bool en);
+    void (*set_tc_tsd)(struct enetc_hw *hw, int tc, bool en);
+    void (*set_tc_msdu)(struct enetc_hw *hw, u32 *max_sdu);
+    void (*reset_tc_msdu)(struct enetc_hw *hw);
+    bool (*get_time_gating)(struct enetc_hw *hw);
+    void (*set_time_gating)(struct enetc_hw *hw, bool en);
+};
+
+struct enetc_mfe {
+    u8 mac[ETH_ALEN];
+    u16 si_bitmap;
+};
+
+struct enetc_mac_list_entry {
+    struct enetc_mfe mfe;
+    struct hlist_node node;
+};
+
+struct enetc_vfe {
+    u16 vid;
+    u8 tpid;
+    u16 si_bitmap;
+};
+
+struct enetc_vlan_list_entry {
+    struct enetc_vfe vfe;
+    struct hlist_node node;
+};
+
 struct enetc_pf {
 	struct enetc_si *si;
 	int num_vfs; /* number of active VFs, after sriov_init */
 	int total_vfs; /* max number of VFs, set for PF at probe */
+	struct enetc_port_caps caps;
 	struct enetc_vf_state *vf_state;
 
 	struct enetc_mac_filter mac_filter[ENETC_MAX_NUM_MAC_FLT];
 
 	struct enetc_msg_swbd rxmsg[ENETC_MAX_NUM_VFS];
+	bool vf_link_status_notify[ENETC_MAX_NUM_VFS];
+
 	struct work_struct msg_task;
 	char msg_int_name[ENETC_INT_NAME_MAX];
 
@@ -94,6 +145,19 @@ struct enetc_pf {
 
 	phy_interface_t if_mode;
 	struct phylink_config phylink_config;
+
+	const struct enetc_pf_hw_ops *hw_ops;
+    struct enetc_devlink_priv *devl_priv;
+
+    u8 mac_addr_base[ETH_ALEN];
+
+    struct hlist_head mac_list; /* MAC address filter table */
+    struct mutex mac_list_lock; /* mac_list lock */
+    int num_mac_fe; /* number of mac address filter table entries */
+
+    struct hlist_head vlan_list; /* VLAN address filter table */
+    struct mutex vlan_list_lock; /* mac_list lock */
+    int num_vlan_fe; /* number of VLAN address filter table entries */
 };
 
 #define phylink_to_enetc_pf(config) \
@@ -102,3 +166,58 @@ struct enetc_pf {
 int enetc_msg_psi_init(struct enetc_pf *pf);
 void enetc_msg_psi_free(struct enetc_pf *pf);
 void enetc_msg_handle_rxmsg(struct enetc_pf *pf, int mbox_id, u16 *status);
+int enetc_mdiobus_create(struct enetc_pf *pf, struct device_node *node);
+void enetc_mdiobus_destroy(struct enetc_pf *pf);
+void enetc_phylink_destroy(struct enetc_ndev_priv *priv);
+int enetc_phylink_create(struct enetc_ndev_priv *priv,
+             struct device_node *node,
+             const struct phylink_mac_ops *pl_mac_ops);
+
+void enetc_pf_netdev_setup(struct enetc_si *si, struct net_device *ndev,
+               const struct net_device_ops *ndev_ops);
+int enetc_setup_mac_addresses(struct device_node *np, struct enetc_pf *pf);
+int enetc_pf_set_mac_addr(struct net_device *ndev, void *addr);
+int ec_enetc_vlan_rx_add_vid(struct net_device *ndev, __be16 prot, u16 vid);
+int ec_enetc_vlan_rx_del_vid(struct net_device *ndev, __be16 prot, u16 vid);
+int enetc_pf_set_vf_mac(struct net_device *ndev, int vf, u8 *mac);
+int enetc_pf_set_vf_vlan(struct net_device *ndev, int vf, u16 vlan,
+             u8 qos, __be16 proto);
+int enetc_pf_set_vf_spoofchk(struct net_device *ndev, int vf, bool en);
+int enetc_pf_set_vf_trust(struct net_device *ndev, int vf, bool setting);
+int enetc_pf_get_vf_config(struct net_device *ndev, int vf,
+               struct ifla_vf_info *ivi);
+int enetc_pf_set_features(struct net_device *ndev, netdev_features_t features);
+int enetc_pf_setup_tc(struct net_device *ndev, enum tc_setup_type type,
+              void *type_data);
+int enetc_sriov_configure(struct pci_dev *pdev, int num_vfs);
+void enetc_pf_flush_mac_exact_filter(struct enetc_pf *pf, int si_id,
+                     int mac_type);
+int enetc_pf_set_mac_exact_filter(struct enetc_pf *pf, int si_id,
+                  struct enetc_mac_entry *mac,
+                  int mac_cnt);
+int enetc_pf_send_msg(struct enetc_pf *pf, u32 msg_code, u16 ms_mask);
+void enetc_get_ip_revision(struct enetc_si *si);
+
+static inline void enetc_pf_register_hw_ops(struct enetc_pf *pf,
+                        const struct enetc_pf_hw_ops *hw_ops)
+{
+    pf->hw_ops = hw_ops;
+}
+
+static inline bool enetc_pf_is_owned_by_mcore(struct pci_dev *pdev)
+{
+    if (pdev->vendor == PCI_VENDOR_ID_NXP2 &&
+        pdev->device == ENETC_PF_VIRTUAL_DEVID)
+        return true;
+
+    return false;
+}
+
+static inline bool enetc_pf_is_vf_trusted(struct enetc_pf *pf, int vf_id)
+{
+    if (vf_id >= pf->total_vfs)
+        return false;
+
+    return !!(pf->vf_state[vf_id].flags & ENETC_VF_FLAG_TRUSTED);
+}
+
diff --git a/devices/enetc/enetc_pf_common.c b/devices/enetc/enetc_pf_common.c
new file mode 100755
index 0000000..ae2b973
--- /dev/null
+++ b/devices/enetc/enetc_pf_common.c
@@ -0,0 +1,1757 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+/* Copyright 2023 NXP */
+#include <linux/if_vlan.h>
+#include <linux/fsl/enetc_mdio.h>
+#include <linux/of_mdio.h>
+#include <linux/of_net.h>
+#include <linux/pcs-lynx.h>
+#include <linux/phy/phy.h>
+#include <linux/pcs/pcs-xpcs.h>
+#include <linux/regulator/consumer.h>
+
+#include "enetc_pf.h"
+
+void enetc_get_ip_revision(struct enetc_si *si)
+{
+	struct enetc_hw *hw = &si->hw;
+	u32 val;
+
+	val = enetc_global_rd(hw, ENETC_G_EIPBRR0);
+	si->revision = val & EIPBRR0_REVISION;
+}
+
+static int enetc_set_si_hw_addr(struct enetc_pf *pf, int si, u8 *mac_addr)
+{
+	struct enetc_hw *hw = &pf->si->hw;
+	struct device *dev = &pf->si->pdev->dev;
+	dev_info(dev, "Jony mac: %d %d %d %d %d %d\n", mac_addr[0],mac_addr[1],mac_addr[2],mac_addr[3],mac_addr[4],mac_addr[5] );
+
+	if (pf->hw_ops->set_si_primary_mac)
+		pf->hw_ops->set_si_primary_mac(hw, si, mac_addr);
+	else
+		return -EOPNOTSUPP;
+
+	return 0;
+}
+
+int enetc_pf_set_mac_addr(struct net_device *ndev, void *addr)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	struct enetc_pf *pf = enetc_si_priv(priv->si);
+	struct sockaddr *saddr = addr;
+	int err;
+
+	if (!is_valid_ether_addr(saddr->sa_data))
+		return -EADDRNOTAVAIL;
+
+	err = enetc_set_si_hw_addr(pf, 0, saddr->sa_data);
+	if (err)
+		return err;
+
+	eth_hw_addr_set(ndev, saddr->sa_data);
+
+	return 0;
+}
+
+static int enetc_setup_mac_address(struct device_node *np, struct enetc_pf *pf,
+				   int si)
+{
+	struct device *dev = &pf->si->pdev->dev;
+	struct enetc_hw *hw = &pf->si->hw;
+	u8 mac_addr[ETH_ALEN] = { 0 };
+	int err;
+
+	/* (1) try to get the MAC address from the device tree */
+	if (np) {
+		err = of_get_mac_address(np, mac_addr);
+		if (err == -EPROBE_DEFER)
+			return err;
+	}
+
+	/* (2) bootloader supplied MAC address */
+	if (is_zero_ether_addr(mac_addr) && pf->hw_ops->get_si_primary_mac)
+		pf->hw_ops->get_si_primary_mac(hw, si, mac_addr);
+
+	/* (3) choose a random one */
+	if (is_zero_ether_addr(mac_addr)) {
+		eth_random_addr(mac_addr);
+		dev_info(dev, "no MAC address specified for SI%d, using %pM\n",
+			 si, mac_addr);
+	}
+
+	err = enetc_set_si_hw_addr(pf, si, mac_addr);
+	if (err)
+		return err;
+
+	if (!si)
+		memcpy(pf->mac_addr_base, mac_addr, ETH_ALEN);
+
+	return 0;
+}
+
+int enetc_setup_mac_addresses(struct device_node *np, struct enetc_pf *pf)
+{
+	int err, i;
+
+	/* The PF might take its MAC from the device tree */
+	err = enetc_setup_mac_address(np, pf, 0);
+	if (err)
+		return err;
+
+	for (i = 0; i < pf->total_vfs; i++) {
+		if (is_enetc_rev1(pf->si)) {
+			err = enetc_setup_mac_address(NULL, pf, i + 1);
+		} else {
+			u8 mac_addr[ETH_ALEN];
+
+			memcpy(mac_addr, pf->mac_addr_base, ETH_ALEN);
+			eth_addr_add(mac_addr, i + 1);
+			if (!is_valid_ether_addr(mac_addr)) {
+				eth_random_addr(mac_addr);
+				dev_info(&pf->si->pdev->dev,
+					 "SI%d: Invalid MAC addr, using %pM\n",
+					 i + 1, mac_addr);
+			}
+			err = enetc_set_si_hw_addr(pf, i + 1, mac_addr);
+		}
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+static void enetc_set_si_vlan_promisc(struct enetc_pf *pf, int index, bool en)
+{
+	struct enetc_hw *hw = &pf->si->hw;
+
+	if (pf->hw_ops->set_si_vlan_promisc)
+		pf->hw_ops->set_si_vlan_promisc(hw, index, en);
+}
+
+int ec_enetc_vlan_rx_add_vid(struct net_device *ndev, __be16 prot, u16 vid)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	struct enetc_si *si = priv->si;
+	struct enetc_hw *hw = &si->hw;
+	struct enetc_pf *pf;
+	int idx;
+
+	pf = enetc_si_priv(si);
+
+	__set_bit(vid, si->active_vlans);
+
+	idx = ec_enetc_vid_hash_idx(vid);
+	if (!__test_and_set_bit(idx, si->vlan_ht_filter) &&
+	    pf->hw_ops->set_si_vlan_hash_filter)
+		pf->hw_ops->set_si_vlan_hash_filter(hw, 0, *si->vlan_ht_filter);
+
+	return 0;
+}
+
+int ec_enetc_vlan_rx_del_vid(struct net_device *ndev, __be16 prot, u16 vid)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	struct enetc_si *si = priv->si;
+	struct enetc_hw *hw = &si->hw;
+	struct enetc_pf *pf;
+	int idx;
+
+	pf = enetc_si_priv(si);
+
+	if (__test_and_clear_bit(vid, si->active_vlans)) {
+		idx = ec_enetc_vid_hash_idx(vid);
+		ec_enetc_refresh_vlan_ht_filter(si);
+		if (!test_bit(idx, si->vlan_ht_filter) &&
+		    pf->hw_ops->set_si_vlan_hash_filter)
+			pf->hw_ops->set_si_vlan_hash_filter(hw, 0, *si->vlan_ht_filter);
+	}
+
+	return 0;
+}
+
+int enetc_pf_set_vf_mac(struct net_device *ndev, int vf, u8 *mac)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	struct enetc_pf *pf = enetc_si_priv(priv->si);
+	struct enetc_vf_state *vf_state;
+	int err;
+
+	if (vf >= pf->total_vfs)
+		return -EINVAL;
+
+	if (!is_valid_ether_addr(mac))
+		return -EADDRNOTAVAIL;
+
+	vf_state = &pf->vf_state[vf];
+	vf_state->flags |= ENETC_VF_FLAG_PF_SET_MAC;
+
+	err = enetc_set_si_hw_addr(pf, vf + 1, mac);
+	if (err)
+		return err;
+
+	return 0;
+}
+
+int enetc_pf_set_vf_vlan(struct net_device *ndev, int vf, u16 vlan,
+			 u8 qos, __be16 proto)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	struct enetc_pf *pf = enetc_si_priv(priv->si);
+	struct enetc_hw *hw = &pf->si->hw;
+
+	if (priv->si->errata & ENETC_ERR_VLAN_ISOL)
+		return -EOPNOTSUPP;
+
+	if (vf >= pf->total_vfs || vlan >= VLAN_N_VID || qos > 7)
+		return -EINVAL;
+
+	if (proto != htons(ETH_P_8021Q))
+		/* only C-tags supported for now */
+		return -EPROTONOSUPPORT;
+
+	if (pf->hw_ops->set_si_based_vlan)
+		pf->hw_ops->set_si_based_vlan(hw, vf + 1, vlan, qos);
+	else
+		return -EOPNOTSUPP;
+
+	return 0;
+}
+
+int enetc_pf_set_vf_spoofchk(struct net_device *ndev, int vf, bool en)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	struct enetc_pf *pf = enetc_si_priv(priv->si);
+	struct enetc_hw *hw = &pf->si->hw;
+
+	if (vf >= pf->total_vfs)
+		return -EINVAL;
+
+	if (pf->hw_ops->set_si_anti_spoofing)
+		pf->hw_ops->set_si_anti_spoofing(hw, vf + 1, en);
+	else
+		return -EOPNOTSUPP;
+
+	return 0;
+}
+
+int enetc_pf_set_vf_trust(struct net_device *ndev, int vf, bool setting)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	struct enetc_pf *pf = enetc_si_priv(priv->si);
+	struct enetc_vf_state *vf_state;
+
+	if (vf >= pf->total_vfs)
+		return -EINVAL;
+
+	vf_state = &pf->vf_state[vf];
+
+	if (setting)
+		vf_state->flags |= ENETC_VF_FLAG_TRUSTED;
+	else
+		vf_state->flags &= ~ENETC_VF_FLAG_TRUSTED;
+
+	return 0;
+}
+
+int enetc_pf_get_vf_config(struct net_device *ndev, int vf,
+			   struct ifla_vf_info *ivi)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	struct enetc_pf *pf = enetc_si_priv(priv->si);
+	struct enetc_hw *hw = &pf->si->hw;
+	struct enetc_vf_state *vf_state;
+
+	if (vf >= pf->num_vfs)
+		return -EINVAL;
+
+	vf_state = &pf->vf_state[vf];
+
+	ivi->vf = vf;
+
+	if (pf->hw_ops->get_si_primary_mac)
+		pf->hw_ops->get_si_primary_mac(hw, vf + 1, ivi->mac);
+
+	if (pf->hw_ops->get_si_based_vlan)
+		pf->hw_ops->get_si_based_vlan(hw, vf + 1, &ivi->vlan,
+					      &ivi->qos);
+
+	ivi->trusted = !!(vf_state->flags & ENETC_VF_FLAG_TRUSTED);
+
+	return 0;
+}
+
+int enetc_pf_set_features(struct net_device *ndev, netdev_features_t features)
+{
+	netdev_features_t changed = ndev->features ^ features;
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	struct enetc_pf *pf = enetc_si_priv(priv->si);
+	int err;
+
+	if (changed & NETIF_F_HW_TC) {
+		err = enetc_set_tc_flower(ndev, !!(features & NETIF_F_HW_TC));
+		if (err)
+			return err;
+	}
+
+	if (changed & NETIF_F_HW_VLAN_CTAG_FILTER) {
+		struct enetc_pf *pf = enetc_si_priv(priv->si);
+		bool en = !(features & NETIF_F_HW_VLAN_CTAG_FILTER);
+
+		enetc_set_si_vlan_promisc(pf, 0, en);
+	}
+
+	if (changed & NETIF_F_LOOPBACK && pf->hw_ops->set_loopback)
+		pf->hw_ops->set_loopback(ndev, !!(features & NETIF_F_LOOPBACK));
+
+	ec_enetc_set_features(ndev, features);
+
+	return 0;
+}
+
+int enetc_pf_setup_tc(struct net_device *ndev, enum tc_setup_type type,
+		      void *type_data)
+{
+	switch (type) {
+	case TC_QUERY_CAPS:
+		return enetc_qos_query_caps(ndev, type_data);
+	case TC_SETUP_QDISC_MQPRIO:
+		return ec_enetc_setup_tc_mqprio(ndev, type_data);
+	case TC_SETUP_QDISC_TAPRIO:	/* time aware priority shaper */
+		return enetc_setup_tc_taprio(ndev, type_data);
+	case TC_SETUP_QDISC_CBS:
+		return enetc_setup_tc_cbs(ndev, type_data);
+	case TC_SETUP_QDISC_ETF:	/* earliest txtime first */
+		return enetc_setup_tc_txtime(ndev, type_data);
+	case TC_SETUP_BLOCK:
+		return enetc_setup_tc_psfp(ndev, type_data);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+void enetc_pf_netdev_setup(struct enetc_si *si, struct net_device *ndev,
+			   const struct net_device_ops *ndev_ops)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+
+	SET_NETDEV_DEV(ndev, &si->pdev->dev);
+	priv->ndev = ndev;
+	priv->si = si;
+	priv->dev = &si->pdev->dev;
+	si->ndev = ndev;
+
+	priv->msg_enable = (NETIF_MSG_WOL << 1) - 1;
+	ndev->netdev_ops = ndev_ops;
+	ec_enetc_set_ethtool_ops(ndev);
+	ndev->watchdog_timeo = 5 * HZ;
+
+	ndev->hw_features = NETIF_F_SG | NETIF_F_RXCSUM |
+			    NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX |
+			    //NETIF_F_HW_VLAN_CTAG_FILTER |
+			    NETIF_F_HW_CSUM | NETIF_F_TSO | NETIF_F_TSO6 |
+			    NETIF_F_GSO_UDP_L4;
+	ndev->features = NETIF_F_HIGHDMA | NETIF_F_SG | NETIF_F_RXCSUM |
+			 NETIF_F_HW_VLAN_CTAG_TX |
+			 NETIF_F_HW_VLAN_CTAG_RX |
+			 NETIF_F_HW_CSUM | NETIF_F_TSO | NETIF_F_TSO6 |
+			 NETIF_F_GSO_UDP_L4;
+	ndev->vlan_features = NETIF_F_SG | NETIF_F_HW_CSUM |
+			      NETIF_F_TSO | NETIF_F_TSO6 | NETIF_F_GSO_UDP_L4;
+
+	if (si->num_rss) {
+		ndev->hw_features |= NETIF_F_RXHASH;
+		ndev->features |= NETIF_F_RXHASH;
+	}
+
+	/* If driver handles unicast address filtering, it should set
+	 * IFF_UNICAST_FLT in its priv_flags. (Refer to the description
+	 * of the ndo_set_rx_mode())
+	 */
+	ndev->priv_flags |= IFF_UNICAST_FLT;
+	ndev->xdp_features = NETDEV_XDP_ACT_BASIC | NETDEV_XDP_ACT_REDIRECT |
+			     NETDEV_XDP_ACT_NDO_XMIT | NETDEV_XDP_ACT_RX_SG |
+			     NETDEV_XDP_ACT_NDO_XMIT_SG;
+
+	if (is_enetc_rev1(si)) {
+		ndev->max_mtu = ENETC_MAX_MTU;
+		priv->max_frags_bd = ENETC_MAX_SKB_FRAGS;
+	} else {
+		ndev->max_mtu = ENETC4_MAX_MTU;
+		priv->max_frags_bd = ENETC4_MAX_SKB_FRAGS;
+		priv->active_offloads |= ENETC_F_CHECKSUM;
+		priv->shared_tx_rings = true;
+	}
+
+	if (si->hw_features & ENETC_SI_F_RSC)
+		ndev->hw_features |= NETIF_F_LRO;
+
+	if (si->hw_features & ENETC_SI_F_LSO)
+		priv->active_offloads |= ENETC_F_LSO;
+
+	if (si->hw_features & ENETC_SI_F_PSFP && !enetc_set_tc_flower(ndev, true)) {
+		ndev->features |= NETIF_F_HW_TC;
+		ndev->hw_features |= NETIF_F_HW_TC;
+	}
+
+	if (!(si->hw_features & ENETC_SI_F_PPM))
+		ndev->hw_features |= NETIF_F_LOOPBACK;
+
+	/* pick up primary MAC address from SI */
+	enetc_load_primary_mac_addr(&si->hw, ndev);
+}
+
+static int enetc_mdio_probe(struct enetc_pf *pf, struct device_node *np)
+{
+	struct device *dev = &pf->si->pdev->dev;
+	struct enetc_mdio_priv *mdio_priv;
+	struct mii_bus *bus;
+	int err;
+
+	bus = devm_mdiobus_alloc_size(dev, sizeof(*mdio_priv));
+	if (!bus)
+		return -ENOMEM;
+
+	bus->name = "Freescale ENETC MDIO Bus";
+	bus->read = enetc_mdio_read_c22;
+	bus->write = enetc_mdio_write_c22;
+	bus->read_c45 = enetc_mdio_read_c45;
+	bus->write_c45 = enetc_mdio_write_c45;
+	bus->parent = dev;
+	mdio_priv = bus->priv;
+	mdio_priv->hw = &pf->si->hw;
+	if (is_enetc_rev4(pf->si))
+		mdio_priv->mdio_base = ENETC4_EMDIO_BASE;
+	else
+		mdio_priv->mdio_base = ENETC_EMDIO_BASE;
+	snprintf(bus->id, MII_BUS_ID_SIZE, "%s", dev_name(dev));
+
+	err = of_mdiobus_register(bus, np);
+	if (err)
+		return dev_err_probe(dev, err, "cannot register MDIO bus\n");
+
+	pf->mdio = bus;
+
+	return 0;
+}
+
+static void enetc_mdio_remove(struct enetc_pf *pf)
+{
+	if (pf->mdio)
+		mdiobus_unregister(pf->mdio);
+}
+
+static int enetc_imdio_create(struct enetc_pf *pf)
+{
+	struct device *dev = &pf->si->pdev->dev;
+	struct enetc_mdio_priv *mdio_priv;
+	struct phylink_pcs *phylink_pcs;
+	struct mii_bus *bus;
+	struct phy *serdes;
+	int err, xpcs_ver;
+	size_t num_phys;
+
+	serdes = devm_of_phy_optional_get(dev, dev->of_node, NULL);
+	if (IS_ERR(serdes))
+		return PTR_ERR(serdes);
+
+	num_phys = serdes ? 1 : 0;
+
+	bus = mdiobus_alloc_size(sizeof(*mdio_priv));
+	if (!bus)
+		return -ENOMEM;
+
+	bus->name = "Freescale ENETC internal MDIO Bus";
+	bus->read = enetc_mdio_read_c22;
+	bus->write = enetc_mdio_write_c22;
+	bus->read_c45 = enetc_mdio_read_c45;
+	bus->write_c45 = enetc_mdio_write_c45;
+	bus->parent = dev;
+	bus->phy_mask = ~0;
+	mdio_priv = bus->priv;
+	mdio_priv->hw = &pf->si->hw;
+	if (is_enetc_rev4(pf->si))
+		mdio_priv->mdio_base = ENETC4_PM_IMDIO_BASE;
+	else
+		mdio_priv->mdio_base = ENETC_PM_IMDIO_BASE;
+	snprintf(bus->id, MII_BUS_ID_SIZE, "%s-imdio", dev_name(dev));
+
+	mdio_priv->regulator = devm_regulator_get_optional(dev, "serdes");
+	if (IS_ERR(mdio_priv->regulator)) {
+		err = PTR_ERR(mdio_priv->regulator);
+		if (err == -EPROBE_DEFER)
+			goto free_mdio_bus;
+		mdio_priv->regulator = NULL;
+	}
+
+	if (mdio_priv->regulator) {
+		err = regulator_enable(mdio_priv->regulator);
+		if (err) {
+			dev_err(dev, "fail to enable phy-supply\n");
+			goto free_mdio_bus;
+		}
+	}
+
+	err = mdiobus_register(bus);
+	if (err) {
+		dev_err(dev, "cannot register internal MDIO bus (%d)\n", err);
+		goto free_mdio_bus;
+	}
+
+	if (is_enetc_rev1(pf->si)) {
+		phylink_pcs = lynx_pcs_create_mdiodev(bus, 0, &serdes, num_phys);
+		if (IS_ERR(phylink_pcs)) {
+			err = PTR_ERR(phylink_pcs);
+			dev_err(dev, "cannot create lynx pcs (%d)\n", err);
+			goto unregister_mdiobus;
+		}
+	} else {
+		switch (pf->si->revision) {
+		case NETC_REVISION_4_1:
+			xpcs_ver = DW_XPCS_VER_MX95;
+			break;
+		default:
+			dev_err(dev, "unsupported xpcs version\n");
+			goto unregister_mdiobus;
+		}
+		phylink_pcs = xpcs_create_mdiodev_with_phy(bus, 0, 16, 0,
+							   xpcs_ver,
+							   pf->if_mode);
+		if (IS_ERR(phylink_pcs)) {
+			err = PTR_ERR(phylink_pcs);
+			dev_err(dev, "cannot create xpcs mdiodev (%d)\n", err);
+			goto unregister_mdiobus;
+		}
+	}
+
+	pf->imdio = bus;
+	pf->pcs = phylink_pcs;
+
+	return 0;
+
+unregister_mdiobus:
+	mdiobus_unregister(bus);
+free_mdio_bus:
+	mdiobus_free(bus);
+	return err;
+}
+
+static void enetc_imdio_remove(struct enetc_pf *pf)
+{
+	struct enetc_mdio_priv *mdio_priv;
+
+	if (pf->pcs) {
+		if (is_enetc_rev1(pf->si))
+			lynx_pcs_destroy(pf->pcs);
+		else
+			xpcs_pcs_destroy(pf->pcs);
+	}
+
+	if (pf->imdio) {
+		mdio_priv = pf->imdio->priv;
+
+		mdiobus_unregister(pf->imdio);
+		if (mdio_priv && mdio_priv->regulator)
+			regulator_disable(mdio_priv->regulator);
+		mdiobus_free(pf->imdio);
+	}
+}
+
+static bool enetc_port_has_pcs(struct enetc_pf *pf)
+{
+	return (pf->if_mode == PHY_INTERFACE_MODE_SGMII ||
+		pf->if_mode == PHY_INTERFACE_MODE_1000BASEX ||
+		pf->if_mode == PHY_INTERFACE_MODE_2500BASEX ||
+		pf->if_mode == PHY_INTERFACE_MODE_10GBASER ||
+		pf->if_mode == PHY_INTERFACE_MODE_USXGMII ||
+		pf->if_mode == PHY_INTERFACE_MODE_XGMII);
+}
+
+int enetc_mdiobus_create(struct enetc_pf *pf, struct device_node *node)
+{
+	struct device_node *mdio_np;
+	int err;
+
+	mdio_np = of_get_child_by_name(node, "mdio");
+	if (mdio_np) {
+		err = enetc_mdio_probe(pf, mdio_np);
+
+		of_node_put(mdio_np);
+		if (err)
+			return err;
+	}
+
+	if (enetc_port_has_pcs(pf)) {
+		err = enetc_imdio_create(pf);
+		if (err) {
+			enetc_mdio_remove(pf);
+			return err;
+		}
+	}
+
+	return 0;
+}
+
+void enetc_mdiobus_destroy(struct enetc_pf *pf)
+{
+	enetc_mdio_remove(pf);
+
+	if (enetc_port_has_pcs(pf))
+		enetc_imdio_remove(pf);
+}
+
+int enetc_phylink_create(struct enetc_ndev_priv *priv,
+			 struct device_node *node,
+			 const struct phylink_mac_ops *pl_mac_ops)
+{
+	struct enetc_pf *pf = enetc_si_priv(priv->si);
+	struct enetc_si *si = priv->si;
+	struct phylink *phylink;
+	int err;
+
+	pf->phylink_config.dev = &priv->ndev->dev;
+	pf->phylink_config.type = PHYLINK_NETDEV;
+
+	if (is_enetc_rev1(si))
+		pf->phylink_config.mac_capabilities = MAC_ASYM_PAUSE | MAC_SYM_PAUSE |
+			MAC_10 | MAC_100 | MAC_1000 | MAC_2500FD;
+	else
+		pf->phylink_config.mac_capabilities = MAC_ASYM_PAUSE | MAC_SYM_PAUSE |
+			MAC_10 | MAC_100 | MAC_1000FD | MAC_2500FD | MAC_10000FD;
+
+	__set_bit(PHY_INTERFACE_MODE_INTERNAL,
+		  pf->phylink_config.supported_interfaces);
+	__set_bit(PHY_INTERFACE_MODE_SGMII,
+		  pf->phylink_config.supported_interfaces);
+	__set_bit(PHY_INTERFACE_MODE_RMII,
+		  pf->phylink_config.supported_interfaces);
+	__set_bit(PHY_INTERFACE_MODE_1000BASEX,
+		  pf->phylink_config.supported_interfaces);
+	__set_bit(PHY_INTERFACE_MODE_2500BASEX,
+		  pf->phylink_config.supported_interfaces);
+	__set_bit(PHY_INTERFACE_MODE_10GBASER,
+		  pf->phylink_config.supported_interfaces);
+	__set_bit(PHY_INTERFACE_MODE_USXGMII,
+		  pf->phylink_config.supported_interfaces);
+	__set_bit(PHY_INTERFACE_MODE_XGMII,
+		  pf->phylink_config.supported_interfaces);
+
+	phy_interface_set_rgmii(pf->phylink_config.supported_interfaces);
+
+	phylink = phylink_create(&pf->phylink_config, of_fwnode_handle(node),
+				 pf->if_mode, pl_mac_ops);
+	if (IS_ERR(phylink)) {
+		err = PTR_ERR(phylink);
+		return err;
+	}
+
+	priv->phylink = phylink;
+
+	return 0;
+}
+
+void enetc_phylink_destroy(struct enetc_ndev_priv *priv)
+{
+	phylink_destroy(priv->phylink);
+}
+
+/* Messaging */
+static u16 enetc_msg_pf_set_vf_primary_mac_addr(struct enetc_pf *pf, int vf_id)
+{
+	struct enetc_vf_state *vf_state = &pf->vf_state[vf_id];
+	struct enetc_msg_swbd *msg_swbd = &pf->rxmsg[vf_id];
+	struct device *dev = &pf->si->pdev->dev;
+	struct enetc_msg_mac_exact_filter *msg;
+	union enetc_pf_msg pf_msg;
+	char *addr;
+
+	msg = (struct enetc_msg_mac_exact_filter *)msg_swbd->vaddr;
+	addr = msg->mac[0].addr;
+	if (vf_state->flags & ENETC_VF_FLAG_PF_SET_MAC) {
+		dev_warn(dev, "Attempt to override PF set mac addr for VF%d\n",
+			 vf_id);
+		if (!enetc_pf_is_vf_trusted(pf, vf_id)) {
+			pf_msg.class_id = ENETC_MSG_CLASS_ID_PERMISSION_DENY;
+			return pf_msg.code;
+		}
+	}
+
+	if (enetc_set_si_hw_addr(pf, vf_id + 1, addr))
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT;
+	else
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_SUCCESS;
+
+	return pf_msg.code;
+}
+
+static struct enetc_mac_list_entry
+	*enetc_mac_list_lookup_entry(struct enetc_pf *pf, const unsigned char *addr)
+{
+	struct enetc_mac_list_entry *entry;
+
+	hlist_for_each_entry(entry, &pf->mac_list, node)
+		if (ether_addr_equal(entry->mfe.mac, addr))
+			return entry;
+
+	return NULL;
+}
+
+static inline void enetc_mac_list_add_entry(struct enetc_pf *pf,
+					    struct enetc_mac_list_entry *entry)
+{
+	hlist_add_head(&entry->node, &pf->mac_list);
+}
+
+static inline void enetc_mac_list_del_entry(struct enetc_mac_list_entry *entry)
+{
+	hlist_del(&entry->node);
+	kfree(entry);
+}
+
+static void enetc_mac_list_del_matched_entries(struct enetc_pf *pf, u16 si_bit,
+					       struct enetc_mac_entry *mac,
+					       int mac_cnt)
+{
+	struct enetc_mac_list_entry *entry;
+	int i;
+
+	for (i = 0; i < mac_cnt; i++) {
+		entry = enetc_mac_list_lookup_entry(pf, mac[i].addr);
+		if (entry) {
+			entry->mfe.si_bitmap &= ~si_bit;
+			if (!entry->mfe.si_bitmap) {
+				enetc_mac_list_del_entry(entry);
+				pf->num_mac_fe--;
+			}
+		}
+	}
+}
+
+int enetc_pf_set_mac_exact_filter(struct enetc_pf *pf, int si_id,
+				  struct enetc_mac_entry *mac,
+				  int mac_cnt)
+{
+	int mf_max_num = pf->caps.mac_filter_num;
+	struct enetc_mac_list_entry *entry;
+	struct maft_entry_data data = {0};
+	struct enetc_si *si = pf->si;
+	int i = 0, used_cnt = 0;
+	u16 si_bit = BIT(si_id);
+	int mf_num;
+
+	guard(mutex)(&pf->mac_list_lock);
+
+	/* Check MAC filter table whether has enough available entries */
+	hlist_for_each_entry(entry, &pf->mac_list, node) {
+		for (i = 0; i < mac_cnt; i++) {
+			if (ether_addr_equal(entry->mfe.mac, mac[i].addr)) {
+				used_cnt++;
+
+				if (mf_max_num - used_cnt < mac_cnt)
+					return -ENOSPC;
+
+				break;
+			}
+		}
+	}
+
+	mf_num = pf->num_mac_fe;
+	/* Update mac_list */
+	for (i = 0; i < mac_cnt; i++) {
+		entry = enetc_mac_list_lookup_entry(pf, mac[i].addr);
+		if (!entry) {
+			entry = kzalloc(sizeof(*entry), GFP_KERNEL);
+			if (unlikely(!entry)) {
+				/* Restore MAC list to before update if an error occurs*/
+				enetc_mac_list_del_matched_entries(pf, si_bit, mac,
+								   i + 1);
+				return -ENOMEM;
+			}
+			ether_addr_copy(entry->mfe.mac, mac[i].addr);
+			entry->mfe.si_bitmap = si_bit;
+			enetc_mac_list_add_entry(pf, entry);
+			pf->num_mac_fe++;
+		} else {
+			entry->mfe.si_bitmap |= si_bit;
+		}
+	}
+
+	/* Clear MAC filter table */
+	for (i = 0; i < mf_num; i++)
+		ntmp_maft_delete_entry(&si->ntmp.cbdrs, i);
+
+	i = 0;
+	hlist_for_each_entry(entry, &pf->mac_list, node) {
+		data.cfge.si_bitmap = cpu_to_le16(entry->mfe.si_bitmap);
+		ether_addr_copy(data.keye.mac_addr, entry->mfe.mac);
+		ntmp_maft_add_entry(&si->ntmp.cbdrs, i++, &data);
+	}
+
+	return 0;
+}
+
+static u16 enetc_msg_pf_add_vf_mac_entries(struct enetc_pf *pf, int vf_id)
+{
+	struct enetc_msg_swbd *msg_swbd = &pf->rxmsg[vf_id];
+	struct enetc_msg_mac_exact_filter *msg;
+	struct enetc_si *si = pf->si;
+	union enetc_pf_msg pf_msg;
+	bool no_resource = false;
+	int err;
+
+	if (is_enetc_rev1(si)) {
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT;
+		return pf_msg.code;
+	}
+
+	msg = (struct enetc_msg_mac_exact_filter *)msg_swbd->vaddr;
+	if (msg->mac_cnt > pf->caps.mac_filter_num) {
+		no_resource = true;
+		goto no_resource_check;
+	}
+
+	err = enetc_pf_set_mac_exact_filter(pf, vf_id + 1, msg->mac,
+					    msg->mac_cnt);
+	if (err)
+		no_resource = true;
+
+no_resource_check:
+	if (no_resource) {
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_MAC_FILTER;
+		pf_msg.class_code = ENETC_PF_RC_MAC_FILTER_NO_RESOURCE;
+		return pf_msg.code;
+	}
+
+	pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_SUCCESS;
+
+	return pf_msg.code;
+}
+
+static int enetc_msg_validate_delete_macs(struct enetc_pf *pf, u16 si_bit,
+					  struct enetc_mac_entry *mac,
+					  int mac_cnt)
+{
+	struct enetc_mac_list_entry *entry;
+	int i;
+
+	for (i = 0; i < mac_cnt; i++) {
+		entry = enetc_mac_list_lookup_entry(pf, mac[i].addr);
+		if (entry && (entry->mfe.si_bitmap & si_bit))
+			continue;
+
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static u16 enetc_msg_pf_del_vf_mac_entries(struct enetc_pf *pf, int vf_id)
+{
+	struct enetc_msg_swbd *msg_swbd = &pf->rxmsg[vf_id];
+	struct enetc_msg_mac_exact_filter *msg;
+	struct enetc_mac_list_entry *entry;
+	struct maft_entry_data data = {0};
+	struct enetc_si *si = pf->si;
+	u16 si_bit = BIT(vf_id + 1);
+	union enetc_pf_msg pf_msg;
+	int i, mf_num, err;
+
+	if (is_enetc_rev1(si)) {
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT;
+		return pf_msg.code;
+	}
+
+	msg = (struct enetc_msg_mac_exact_filter *)msg_swbd->vaddr;
+
+	guard(mutex)(&pf->mac_list_lock);
+
+	err = enetc_msg_validate_delete_macs(pf, si_bit, msg->mac,
+					     msg->mac_cnt);
+	if (err) {
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_MAC_FILTER;
+		pf_msg.class_code = ENETC_PF_RC_MAC_FILTER_MAC_NOT_FOUND;
+		return pf_msg.code;
+	}
+
+	mf_num = pf->num_mac_fe;
+	enetc_mac_list_del_matched_entries(pf, si_bit, msg->mac,
+					   msg->mac_cnt);
+
+	/* Clear MAC filter table */
+	for (i = 0; i < mf_num; i++)
+		ntmp_maft_delete_entry(&si->ntmp.cbdrs, i);
+
+	i = 0;
+	hlist_for_each_entry(entry, &pf->mac_list, node) {
+		data.cfge.si_bitmap = cpu_to_le16(entry->mfe.si_bitmap);
+		ether_addr_copy(data.keye.mac_addr, entry->mfe.mac);
+		ntmp_maft_add_entry(&si->ntmp.cbdrs, i++, &data);
+	}
+
+	pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_SUCCESS;
+
+	return pf_msg.code;
+}
+
+static u16 enetc_msg_pf_set_vf_mac_hash_filter(struct enetc_pf *pf, int vf_id)
+{
+	struct enetc_msg_swbd *msg_swbd = &pf->rxmsg[vf_id];
+	struct device *dev = &pf->si->pdev->dev;
+	struct enetc_msg_mac_hash_filter *msg;
+	struct enetc_hw *hw = &pf->si->hw;
+	union enetc_pf_msg pf_msg;
+	int si_id = vf_id + 1;
+	u64 hash_tbl;
+
+	if (!enetc_pf_is_vf_trusted(pf, vf_id)) {
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_PERMISSION_DENY;
+		return pf_msg.code;
+	}
+
+	if (!pf->hw_ops->set_si_mac_hash_filter) {
+		dev_err(dev, "MAC hash filter is not supported\n");
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT;
+		return pf_msg.code;
+	}
+
+	msg = (struct enetc_msg_mac_hash_filter *)msg_swbd->vaddr;
+	/* Currently, hardware only supports 64 bits table size */
+	if (msg->size != ENETC_MAC_HASH_TABLE_SIZE_64) {
+		dev_err(dev, "MAC hash table size exceeds 64 bits\n");
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT;
+		return pf_msg.code;
+	}
+
+	if (msg->type == ENETC_MAC_FILTER_TYPE_UC) {
+		hash_tbl = (u64)(msg->hash_tbl[1]) << 32 | msg->hash_tbl[0];
+		pf->hw_ops->set_si_mac_hash_filter(hw, si_id, UC, hash_tbl);
+	} else if (msg->type == ENETC_MAC_FILTER_TYPE_MC) {
+		hash_tbl = (u64)(msg->hash_tbl[1]) << 32 | msg->hash_tbl[0];
+		pf->hw_ops->set_si_mac_hash_filter(hw, si_id, MC, hash_tbl);
+	} else {
+		hash_tbl = (u64)(msg->hash_tbl[1]) << 32 | msg->hash_tbl[0];
+		pf->hw_ops->set_si_mac_hash_filter(hw, si_id, UC, hash_tbl);
+		hash_tbl = (u64)(msg->hash_tbl[3]) << 32 | msg->hash_tbl[2];
+		pf->hw_ops->set_si_mac_hash_filter(hw, si_id, MC, hash_tbl);
+	}
+
+	pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_SUCCESS;
+
+	return pf_msg.code;
+}
+
+static bool enetc_msg_mac_type_check(int type, const u8 *addr)
+{
+	if (type == ENETC_MAC_FILTER_TYPE_UC)
+		return !is_multicast_ether_addr(addr);
+	else if (type == ENETC_MAC_FILTER_TYPE_MC)
+		return is_multicast_ether_addr(addr);
+	else
+		return true;
+}
+
+void enetc_pf_flush_mac_exact_filter(struct enetc_pf *pf, int si_id,
+				     int mac_type)
+{
+	struct enetc_mac_list_entry *entry;
+	struct maft_entry_data data = {0};
+	struct enetc_si *si = pf->si;
+	u16 si_bit = BIT(si_id);
+	struct hlist_node *tmp;
+	int i, mf_num;
+
+	if (is_enetc_rev1(si))
+		return;
+
+	guard(mutex)(&pf->mac_list_lock);
+
+	mf_num = pf->num_mac_fe;
+	hlist_for_each_entry_safe(entry, tmp, &pf->mac_list, node) {
+		if (enetc_msg_mac_type_check(mac_type, entry->mfe.mac) &&
+		    entry->mfe.si_bitmap & si_bit) {
+			entry->mfe.si_bitmap &= ~si_bit;
+			if (!entry->mfe.si_bitmap) {
+				enetc_mac_list_del_entry(entry);
+				pf->num_mac_fe--;
+			}
+		}
+	}
+
+	for (i = 0; i < mf_num; i++)
+		ntmp_maft_delete_entry(&si->ntmp.cbdrs, i);
+
+	i = 0;
+	hlist_for_each_entry(entry, &pf->mac_list, node) {
+		data.cfge.si_bitmap = cpu_to_le16(entry->mfe.si_bitmap);
+		ether_addr_copy(data.keye.mac_addr, entry->mfe.mac);
+		ntmp_maft_add_entry(&si->ntmp.cbdrs, i++, &data);
+	}
+}
+
+static void enetc_pf_flush_si_mac_filter(struct enetc_pf *pf, int si_id,
+					 int mac_type)
+{
+	struct enetc_hw *hw = &pf->si->hw;
+
+	enetc_pf_flush_mac_exact_filter(pf, si_id, mac_type);
+
+	if (!pf->hw_ops->set_si_mac_hash_filter)
+		return;
+
+	if (mac_type & ENETC_MAC_FILTER_TYPE_UC)
+		pf->hw_ops->set_si_mac_hash_filter(hw, si_id, UC, 0);
+
+	if (mac_type & ENETC_MAC_FILTER_TYPE_MC)
+		pf->hw_ops->set_si_mac_hash_filter(hw, si_id, MC, 0);
+}
+
+static u16 enetc_msg_pf_flush_vf_mac_entries(struct enetc_pf *pf, int vf_id)
+{
+	struct enetc_msg_swbd *msg_swbd = &pf->rxmsg[vf_id];
+	struct enetc_msg_mac_filter_flush *msg;
+	struct enetc_si *si = pf->si;
+	union enetc_pf_msg pf_msg;
+
+	if (is_enetc_rev1(si)) {
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT;
+		return pf_msg.code;
+	}
+
+	msg = (struct enetc_msg_mac_filter_flush *)msg_swbd->vaddr;
+	enetc_pf_flush_si_mac_filter(pf, vf_id + 1, msg->type);
+
+	pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_SUCCESS;
+
+	return pf_msg.code;
+}
+
+static u16 enetc_msg_pf_set_vf_mac_promisc_mode(struct enetc_pf *pf, int vf_id)
+{
+	struct enetc_msg_swbd *msg_swbd = &pf->rxmsg[vf_id];
+	struct enetc_msg_mac_promsic_mode *msg;
+	struct enetc_hw *hw = &pf->si->hw;
+	union enetc_pf_msg pf_msg;
+	bool promisc_mode = false;
+	int si_id = vf_id + 1;
+	int mac_type;
+
+	if (!pf->hw_ops->set_si_mac_promisc) {
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT;
+		return pf_msg.code;
+	}
+
+	msg = (struct enetc_msg_mac_promsic_mode *)msg_swbd->vaddr;
+	if (msg->type == ENETC_MAC_FILTER_TYPE_UC)
+		mac_type = ENETC_MAC_FILTER_TYPE_UC;
+	else if (msg->type == ENETC_MAC_FILTER_TYPE_MC)
+		mac_type = ENETC_MAC_FILTER_TYPE_MC;
+	else
+		mac_type = ENETC_MAC_FILTER_TYPE_ALL;
+
+	if (msg->promisc_mode == ENETC_MAC_PROMISC_MODE_ENABLE) {
+		if (!enetc_pf_is_vf_trusted(pf, vf_id)) {
+			pf_msg.class_id = ENETC_MSG_CLASS_ID_PERMISSION_DENY;
+
+			return pf_msg.code;
+		}
+
+		promisc_mode = true;
+	}
+
+	if (msg->flush_macs)
+		enetc_pf_flush_si_mac_filter(pf, si_id, msg->type);
+
+	if (mac_type & ENETC_MAC_FILTER_TYPE_UC)
+		pf->hw_ops->set_si_mac_promisc(hw, si_id, UC, promisc_mode);
+
+	if (mac_type & ENETC_MAC_FILTER_TYPE_MC)
+		pf->hw_ops->set_si_mac_promisc(hw, si_id, MC, promisc_mode);
+
+	pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_SUCCESS;
+
+	return pf_msg.code;
+}
+
+static u16 enetc_msg_handle_mac_filter(struct enetc_msg_header *msg_hdr,
+				       struct enetc_pf *pf, int vf_id)
+{
+	union enetc_pf_msg pf_msg;
+
+	switch (msg_hdr->cmd_id) {
+	case ENETC_MSG_SET_PRIMARY_MAC:
+		return enetc_msg_pf_set_vf_primary_mac_addr(pf, vf_id);
+	case ENETC_MSG_ADD_EXACT_MAC_ENTRIES:
+		return enetc_msg_pf_add_vf_mac_entries(pf, vf_id);
+	case ENETC_MSG_DEL_EXACT_MAC_ENTRIES:
+		return enetc_msg_pf_del_vf_mac_entries(pf, vf_id);
+	case ENETC_MSG_SET_MAC_HASH_TABLE:
+		return enetc_msg_pf_set_vf_mac_hash_filter(pf, vf_id);
+	case ENETC_MSG_FLUSH_MAC_ENTRIES:
+		return enetc_msg_pf_flush_vf_mac_entries(pf, vf_id);
+	case ENETC_MSG_SET_MAC_PROMISC_MODE:
+		return enetc_msg_pf_set_vf_mac_promisc_mode(pf, vf_id);
+	default:
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT;
+
+		return pf_msg.code;
+	}
+}
+
+static struct enetc_vlan_list_entry
+	*enetc_vlan_list_lookup_entry(struct enetc_pf *pf,
+				      struct enetc_vlan_entry *vlan)
+{
+	struct enetc_vlan_list_entry *entry;
+
+	hlist_for_each_entry(entry, &pf->vlan_list, node)
+		if (entry->vfe.vid == vlan->vid &&
+		    entry->vfe.tpid == vlan->tpid)
+			return entry;
+
+	return NULL;
+}
+
+static inline void enetc_vlan_list_add_entry(struct enetc_pf *pf,
+					     struct enetc_vlan_list_entry *entry)
+{
+	hlist_add_head(&entry->node, &pf->vlan_list);
+}
+
+static inline void enetc_vlan_list_del_entry(struct enetc_vlan_list_entry *entry)
+{
+	hlist_del(&entry->node);
+	kfree(entry);
+}
+
+static void enetc_vlan_list_del_matched_entries(struct enetc_pf *pf, u16 si_bit,
+						struct enetc_vlan_entry *vlan,
+						int vlan_cnt)
+{
+	struct enetc_vlan_list_entry *entry;
+	int i;
+
+	for (i = 0; i < vlan_cnt; i++) {
+		entry = enetc_vlan_list_lookup_entry(pf, &vlan[i]);
+		if (entry) {
+			entry->vfe.si_bitmap &= ~si_bit;
+			if (!entry->vfe.si_bitmap) {
+				enetc_vlan_list_del_entry(entry);
+				pf->num_vlan_fe--;
+			}
+		}
+	}
+}
+
+static void enetc_vfe_to_vaft_data(struct enetc_vfe *vfe,
+				   struct vaft_entry_data *vaft)
+{
+	u16 vid = FIELD_PREP(VAFT_VLAN_ID, vfe->vid);
+
+	vaft->keye.tpid = FIELD_PREP(VAFT_TPID, vfe->tpid);
+	vaft->keye.vlan_id = cpu_to_le16(vid);
+	vaft->cfge.si_bitmap = cpu_to_le16(vfe->si_bitmap);
+}
+
+static int enetc_pf_set_vlan_exact_filter(struct enetc_pf *pf, int si_id,
+					  struct enetc_vlan_entry *vlan,
+					  int vlan_cnt)
+{
+	int vf_max_num = pf->caps.vlan_filter_num;
+	struct enetc_vlan_list_entry *entry;
+	struct vaft_entry_data data = {0};
+	struct enetc_si *si = pf->si;
+	int i = 0, used_cnt = 0;
+	u16 si_bit = BIT(si_id);
+	int vf_num;
+
+	guard(mutex)(&pf->vlan_list_lock);
+
+	/* Check VLAN filter table whether has enough available entries */
+	hlist_for_each_entry(entry, &pf->vlan_list, node) {
+		for (i = 0; i < vlan_cnt; i++) {
+			if (entry->vfe.vid == vlan[i].vid &&
+			    entry->vfe.tpid == vlan[i].tpid) {
+				used_cnt++;
+
+				if (vf_max_num - used_cnt < vlan_cnt)
+					return -ENOSPC;
+
+				break;
+			}
+		}
+	}
+
+	vf_num = pf->num_vlan_fe;
+	/* Update VLAN list */
+	for (i = 0; i < vlan_cnt; i++) {
+		entry = enetc_vlan_list_lookup_entry(pf, &vlan[i]);
+		if (!entry) {
+			entry = kzalloc(sizeof(*entry), GFP_KERNEL);
+			if (unlikely(!entry)) {
+				enetc_vlan_list_del_matched_entries(pf, si_bit, vlan,
+								    i + 1);
+				return -ENOMEM;
+			}
+			entry->vfe.vid = vlan[i].vid;
+			entry->vfe.tpid = vlan[i].tpid;
+			entry->vfe.si_bitmap = si_bit;
+			enetc_vlan_list_add_entry(pf, entry);
+			pf->num_vlan_fe++;
+		} else {
+			entry->vfe.si_bitmap |= si_bit;
+		}
+	}
+
+	/* Clear VLAN filter table */
+	for (i = 0; i < vf_num; i++)
+		ntmp_vaft_delete_entry(&si->ntmp.cbdrs, i);
+
+	i = 0;
+	hlist_for_each_entry(entry, &pf->vlan_list, node) {
+		enetc_vfe_to_vaft_data(&entry->vfe, &data);
+		ntmp_vaft_add_entry(&si->ntmp.cbdrs, i++, &data);
+	}
+
+	return 0;
+}
+
+static u16 enetc_msg_pf_add_vf_vlan_entries(struct enetc_pf *pf, int vf_id)
+{
+	struct enetc_msg_swbd *msg_swbd = &pf->rxmsg[vf_id];
+	struct enetc_msg_vlan_exact_filter *msg;
+	struct enetc_si *si = pf->si;
+	union enetc_pf_msg pf_msg;
+	bool no_resource = false;
+	int err;
+
+	if (is_enetc_rev1(si)) {
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT;
+		return pf_msg.code;
+	}
+
+	msg = (struct enetc_msg_vlan_exact_filter *)msg_swbd->vaddr;
+	if (msg->vlan_cnt > pf->caps.vlan_filter_num) {
+		no_resource = true;
+		goto no_resource_check;
+	}
+
+	err = enetc_pf_set_vlan_exact_filter(pf, vf_id + 1, msg->vlan,
+					     msg->vlan_cnt);
+	if (err)
+		no_resource = true;
+
+no_resource_check:
+	if (no_resource) {
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_VLAN_FILTER;
+		pf_msg.class_code = ENETC_PF_RC_VLAN_FILTER_NO_RESOURCE;
+		return pf_msg.code;
+	}
+
+	pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_SUCCESS;
+
+	return pf_msg.code;
+}
+
+static int enetc_msg_validate_delete_vlans(struct enetc_pf *pf, u16 si_bit,
+					   struct enetc_vlan_entry *vlan,
+					   int vlan_cnt)
+{
+	struct enetc_vlan_list_entry *entry;
+	int i;
+
+	for (i = 0; i < vlan_cnt; i++) {
+		entry = enetc_vlan_list_lookup_entry(pf, &vlan[i]);
+		if (entry && (entry->vfe.si_bitmap & si_bit))
+			continue;
+
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static u16 enetc_msg_pf_del_vf_vlan_entries(struct enetc_pf *pf, int vf_id)
+{
+	struct enetc_msg_swbd *msg_swbd = &pf->rxmsg[vf_id];
+	struct enetc_msg_vlan_exact_filter *msg;
+	struct enetc_vlan_list_entry *entry;
+	struct vaft_entry_data data = {0};
+	struct enetc_si *si = pf->si;
+	u16 si_bit = BIT(vf_id + 1);
+	union enetc_pf_msg pf_msg;
+	int i, vf_num, err;
+
+	if (is_enetc_rev1(si)) {
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT;
+		return pf_msg.code;
+	}
+
+	msg = (struct enetc_msg_vlan_exact_filter *)msg_swbd->vaddr;
+	guard(mutex)(&pf->vlan_list_lock);
+
+	err = enetc_msg_validate_delete_vlans(pf, si_bit, msg->vlan,
+					      msg->vlan_cnt);
+	if (err) {
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_VLAN_FILTER;
+		pf_msg.class_code = ENETC_PF_RC_VLAN_FILTER_VLAN_NOT_FOUND;
+		return pf_msg.code;
+	}
+
+	vf_num = pf->num_vlan_fe;
+	enetc_vlan_list_del_matched_entries(pf, si_bit, msg->vlan,
+					    msg->vlan_cnt);
+
+	for (i = 0; i < vf_num; i++)
+		ntmp_vaft_delete_entry(&si->ntmp.cbdrs, i);
+
+	i = 0;
+	hlist_for_each_entry(entry, &pf->vlan_list, node) {
+		enetc_vfe_to_vaft_data(&entry->vfe, &data);
+		ntmp_vaft_add_entry(&si->ntmp.cbdrs, i++, &data);
+	}
+
+	pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_SUCCESS;
+
+	return pf_msg.code;
+}
+
+static u16 enetc_msg_pf_set_vf_vlan_hash_filter(struct enetc_pf *pf, int vf_id)
+{
+	struct enetc_msg_swbd *msg_swbd = &pf->rxmsg[vf_id];
+	struct device *dev = &pf->si->pdev->dev;
+	struct enetc_msg_vlan_hash_filter *msg;
+	struct enetc_hw *hw = &pf->si->hw;
+	union enetc_pf_msg pf_msg;
+	int si_id = vf_id + 1;
+	u64 hash_tbl;
+
+	if (!enetc_pf_is_vf_trusted(pf, vf_id)) {
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_PERMISSION_DENY;
+		return pf_msg.code;
+	}
+
+	if (!pf->hw_ops->set_si_vlan_hash_filter) {
+		dev_err(dev, "VLAN hash filter is not supported\n");
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT;
+		return pf_msg.code;
+	}
+
+	msg = (struct enetc_msg_vlan_hash_filter *)msg_swbd->vaddr;
+	/* Currently, hardware only supports 64 bits table size */
+	if (msg->size != ENETC_VLAN_HASH_TABLE_SIZE_64) {
+		dev_err(dev, "VLAN hash table size exceeds 64 bits\n");
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT;
+		return pf_msg.code;
+	}
+
+	hash_tbl = (u64)(msg->hash_tbl[1]) << 32 | msg->hash_tbl[0];
+	pf->hw_ops->set_si_vlan_hash_filter(hw, si_id, hash_tbl);
+
+	pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_SUCCESS;
+
+	return pf_msg.code;
+}
+
+static void enetc_pf_flush_vlan_exact_filter(struct enetc_pf *pf, int si_id)
+{
+	struct enetc_vlan_list_entry *entry;
+	struct vaft_entry_data data = {0};
+	struct enetc_si *si = pf->si;
+	u16 si_bit = BIT(si_id);
+	struct hlist_node *tmp;
+	int i, vf_num;
+
+	if (is_enetc_rev1(si))
+		return;
+
+	guard(mutex)(&pf->vlan_list_lock);
+
+	vf_num = pf->num_vlan_fe;
+	hlist_for_each_entry_safe(entry, tmp, &pf->vlan_list, node) {
+		if (entry->vfe.si_bitmap & si_bit) {
+			entry->vfe.si_bitmap &= ~si_bit;
+			if (!entry->vfe.si_bitmap) {
+				enetc_vlan_list_del_entry(entry);
+				pf->num_vlan_fe--;
+			}
+		}
+	}
+
+	for (i = 0; i < vf_num; i++)
+		ntmp_vaft_delete_entry(&si->ntmp.cbdrs, i);
+
+	i = 0;
+	hlist_for_each_entry(entry, &pf->vlan_list, node) {
+		enetc_vfe_to_vaft_data(&entry->vfe, &data);
+		ntmp_vaft_add_entry(&si->ntmp.cbdrs, i++, &data);
+	}
+}
+
+static void enetc_pf_flush_si_vlan_filter(struct enetc_pf *pf, int si_id)
+{
+	enetc_pf_flush_vlan_exact_filter(pf, si_id);
+	if (pf->hw_ops->set_si_vlan_hash_filter) {
+		struct enetc_hw *hw = &pf->si->hw;
+
+		pf->hw_ops->set_si_vlan_hash_filter(hw, si_id, 0);
+	}
+}
+
+static u16 enetc_msg_pf_flush_vf_vlan_entries(struct enetc_pf *pf, int vf_id)
+{
+	struct enetc_si *si = pf->si;
+	union enetc_pf_msg pf_msg;
+
+	if (is_enetc_rev1(si)) {
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT;
+		return pf_msg.code;
+	}
+
+	enetc_pf_flush_si_vlan_filter(pf, vf_id + 1);
+
+	pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_SUCCESS;
+
+	return pf_msg.code;
+}
+
+static u16 enetc_msg_pf_set_vf_vlan_promisc_mode(struct enetc_pf *pf, int vf_id)
+{
+	struct enetc_msg_swbd *msg_swbd = &pf->rxmsg[vf_id];
+	struct enetc_msg_vlan_promsic_mode *msg;
+	union enetc_pf_msg pf_msg;
+	bool promisc_mode = false;
+	int si_id = vf_id + 1;
+
+	if (!enetc_pf_is_vf_trusted(pf, vf_id)) {
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_PERMISSION_DENY;
+		return pf_msg.code;
+	}
+
+	msg = (struct enetc_msg_vlan_promsic_mode *)msg_swbd->vaddr;
+	if (msg->promisc_mode == ENETC_VLAN_PROMISC_MODE_ENABLE)
+		promisc_mode = true;
+
+	if (msg->flush_vlans)
+		enetc_pf_flush_si_vlan_filter(pf, si_id);
+
+	enetc_set_si_vlan_promisc(pf, si_id, promisc_mode);
+
+	pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_SUCCESS;
+
+	return pf_msg.code;
+}
+
+static u16 enetc_msg_handle_vlan_filter(struct enetc_msg_header *msg_hdr,
+					struct enetc_pf *pf, int vf_id)
+{
+	union enetc_pf_msg pf_msg;
+
+	switch (msg_hdr->cmd_id) {
+	case ENETC_MSG_ADD_EXACT_VLAN_ENTRIES:
+		return enetc_msg_pf_add_vf_vlan_entries(pf, vf_id);
+	case ENETC_MSG_DEL_EXACT_VLAN_ENTRIES:
+		return enetc_msg_pf_del_vf_vlan_entries(pf, vf_id);
+	case ENETC_MSG_SET_VLAN_HASH_TABLE:
+		return enetc_msg_pf_set_vf_vlan_hash_filter(pf, vf_id);
+	case ENETC_MSG_FLUSH_VLAN_ENTRIES:
+		return enetc_msg_pf_flush_vf_vlan_entries(pf, vf_id);
+	case ENETC_MSG_SET_VLAN_PROMISC_MODE:
+		return enetc_msg_pf_set_vf_vlan_promisc_mode(pf, vf_id);
+	default:
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT;
+
+		return pf_msg.code;
+	}
+}
+
+static u16 enetc_msg_pf_reply_link_status(struct enetc_pf *pf)
+{
+	struct net_device *ndev = pf->si->ndev;
+	union enetc_pf_msg pf_msg;
+
+	pf_msg.class_id = ENETC_MSG_CLASS_ID_LINK_STATUS;
+	if (netif_carrier_ok(ndev))
+		pf_msg.class_code = ENETC_PF_NC_LINK_STATUS_UP;
+	else
+		pf_msg.class_code = ENETC_PF_NC_LINK_STATUS_DOWN;
+
+	return pf_msg.code;
+}
+
+int enetc_pf_send_msg(struct enetc_pf *pf, u32 msg_code, u16 ms_mask)
+{
+	struct enetc_si *si = pf->si;
+	u32 psimsgsr;
+	int err;
+
+	psimsgsr = PSIMSGSR_SET_MC(msg_code);
+	psimsgsr |= ms_mask;
+
+	guard(mutex)(&si->msg_lock);
+	enetc_wr(&si->hw, ENETC_PSIMSGSR, psimsgsr);
+	err = read_poll_timeout(enetc_rd, psimsgsr,
+				!(psimsgsr & ms_mask),
+				100, 100000, false, &si->hw, ENETC_PSIMSGSR);
+
+	return err;
+}
+
+static void enetc_pf_send_link_status_msg(struct enetc_pf *pf, u16 ms_mask)
+{
+	struct device *dev = &pf->si->pdev->dev;
+	struct net_device *ndev = pf->si->ndev;
+	union enetc_pf_msg pf_msg = { 0 };
+	u32 msg_code;
+	int err;
+
+	if (!ms_mask)
+		return;
+
+	pf_msg.class_id = ENETC_MSG_CLASS_ID_LINK_STATUS;
+	if (netif_carrier_ok(ndev))
+		pf_msg.class_code = ENETC_PF_NC_LINK_STATUS_UP;
+	else
+		pf_msg.class_code = ENETC_PF_NC_LINK_STATUS_DOWN;
+
+	msg_code = pf_msg.code;
+	err = enetc_pf_send_msg(pf, msg_code, ms_mask);
+	if (err)
+		dev_err(dev, "PF notifies link status failed\n");
+}
+
+static u16 enetc_msg_register_link_status_notify(struct enetc_pf *pf, int vf_id,
+						 bool notify)
+{
+	struct enetc_hw *hw = &pf->si->hw;
+	union enetc_pf_msg pf_msg;
+	u32 msg_code, val;
+
+	pf->vf_link_status_notify[vf_id] = notify;
+
+	pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_SUCCESS;
+	msg_code = pf_msg.code;
+
+	/* Reply to VF */
+	val = ENETC_SIMSGSR_SET_MC(msg_code);
+	val |= ENETC_PSIMSGRR_MR(vf_id); /* w1c */
+	enetc_wr(hw, ENETC_PSIMSGRR, val);
+
+	/* Notify VF the current link status */
+	if (notify)
+		enetc_pf_send_link_status_msg(pf, PSIMSGSR_MS(vf_id));
+
+	return 0;
+}
+
+static u16 enetc_msg_handle_link_status(struct enetc_msg_header *msg_hdr,
+					struct enetc_pf *pf, int vf_id)
+{
+	union enetc_pf_msg pf_msg;
+
+	switch (msg_hdr->cmd_id) {
+	case ENETC_MSG_GET_CURRENT_LINK_STATUS:
+		return enetc_msg_pf_reply_link_status(pf);
+	case ENETC_MSG_REGISTER_LINK_CHANGE_NOTIFY:
+		return enetc_msg_register_link_status_notify(pf, vf_id, true);
+	case ENETC_MSG_UNREGISTER_LINK_CHANGE_NOTIFY:
+		return enetc_msg_register_link_status_notify(pf, vf_id, false);
+	default:
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT;
+
+		return pf_msg.code;
+	}
+}
+
+static u16 enetc_msg_pf_reply_link_speed(struct enetc_pf *pf)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(pf->si->ndev);
+	struct ethtool_link_ksettings link_info = {0};
+	union enetc_pf_msg pf_msg;
+
+	rtnl_lock();
+	if (!priv->phylink ||
+	    phylink_ethtool_ksettings_get(priv->phylink, &link_info)) {
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT;
+		rtnl_unlock();
+
+		return pf_msg.code;
+	}
+	rtnl_unlock();
+
+	pf_msg.class_id = ENETC_MSG_CLASS_ID_LINK_SPEED;
+
+	switch (link_info.base.speed) {
+	case SPEED_10:
+		if (link_info.base.duplex == DUPLEX_HALF)
+			pf_msg.class_code = ENETC_MSG_SPEED_10M_HD;
+		else
+			pf_msg.class_code = ENETC_MSG_SPEED_10M_FD;
+		break;
+	case SPEED_100:
+		if (link_info.base.duplex == DUPLEX_HALF)
+			pf_msg.class_code = ENETC_MSG_SPEED_100M_HD;
+		else
+			pf_msg.class_code = ENETC_MSG_SPEED_100M_FD;
+		break;
+	case SPEED_1000:
+		pf_msg.class_code = ENETC_MSG_SPEED_1000M;
+		break;
+	case SPEED_2500:
+		pf_msg.class_code = ENETC_MSG_SPEED_2500M;
+		break;
+	case SPEED_5000:
+		pf_msg.class_code = ENETC_MSG_SPEED_5G;
+		break;
+	case SPEED_10000:
+		pf_msg.class_code = ENETC_MSG_SPEED_10G;
+		break;
+	case SPEED_25000:
+		pf_msg.class_code = ENETC_MSG_SPEED_25G;
+		break;
+	case SPEED_50000:
+		pf_msg.class_code = ENETC_MSG_SPEED_50G;
+		break;
+	case SPEED_100000:
+		pf_msg.class_code = ENETC_MSG_SPEED_100G;
+		break;
+	default:
+		pf_msg.class_code = ENETC_MSG_SPEED_UNKNOWN;
+	}
+
+	return pf_msg.code;
+}
+
+static u16 enetc_msg_handle_link_speed(struct enetc_msg_header *msg_hdr,
+				       struct enetc_pf *pf, int vf_id)
+{
+	union enetc_pf_msg pf_msg;
+
+	switch (msg_hdr->cmd_id) {
+	case ENETC_MSG_GET_CURRENT_LINK_SPEED:
+		return enetc_msg_pf_reply_link_speed(pf);
+	default:
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT;
+
+		return pf_msg.code;
+	}
+}
+
+static bool enetc_msg_check_crc16(void *msg_addr, u32 msg_size)
+{
+	u8 *data_buf = msg_addr + 2;
+	u8 data_size = msg_size - 2;
+	u16 verify_val;
+
+	if (msg_size > ENETC_DEFAULT_MSG_SIZE)
+		return false;
+
+	verify_val = crc_itu_t(ENETC_CRC_INIT, data_buf, data_size);
+	verify_val = crc_itu_t(verify_val, msg_addr, 2);
+	if (verify_val)
+		return false;
+
+	return true;
+}
+
+void enetc_msg_handle_rxmsg(struct enetc_pf *pf, int vf_id, u16 *msg_code)
+{
+	struct enetc_msg_swbd *msg_swbd = &pf->rxmsg[vf_id];
+	struct device *dev = &pf->si->pdev->dev;
+	struct enetc_msg_header *msg_hdr;
+	union enetc_pf_msg pf_msg;
+	u32 msg_size;
+
+	msg_hdr = (struct enetc_msg_header *)msg_swbd->vaddr;
+	msg_size = ENETC_MSG_SIZE(msg_hdr->len);
+	if (!enetc_msg_check_crc16(msg_swbd->vaddr, msg_size)) {
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CRC_ERROR;
+		*msg_code = pf_msg.code;
+
+		dev_err(dev, "VSI to PSI Message CRC16 error\n");
+
+		return;
+	}
+
+	/* Currently, we don't support asynchronous action */
+	if (msg_hdr->cookie) {
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT;
+		*msg_code = pf_msg.code;
+
+		dev_err(dev, "Cookie field is not supported yet\n");
+
+		return;
+	}
+
+	switch (msg_hdr->class_id) {
+	case ENETC_MSG_CLASS_ID_MAC_FILTER:
+		*msg_code = enetc_msg_handle_mac_filter(msg_hdr, pf, vf_id);
+		break;
+	case ENETC_MSG_CLASS_ID_VLAN_FILTER:
+		*msg_code = enetc_msg_handle_vlan_filter(msg_hdr, pf, vf_id);
+		break;
+	case ENETC_MSG_CLASS_ID_LINK_STATUS:
+		*msg_code = enetc_msg_handle_link_status(msg_hdr, pf, vf_id);
+		break;
+	case ENETC_MSG_CLASS_ID_LINK_SPEED:
+		*msg_code = enetc_msg_handle_link_speed(msg_hdr, pf, vf_id);
+		break;
+	default:
+		pf_msg.class_id = ENETC_MSG_CLASS_ID_CMD_NOT_SUPPORT;
+		*msg_code = pf_msg.code;
+	}
+}
+
+#ifdef CONFIG_PCI_IOV
+int enetc_sriov_configure(struct pci_dev *pdev, int num_vfs)
+{
+	struct enetc_si *si;
+	struct enetc_pf *pf;
+	int err;
+
+	if (enetc_pf_is_owned_by_mcore(pdev)) {
+		err = pci_sriov_configure_simple(pdev, num_vfs);
+		if (err < 0)
+			dev_err(&pdev->dev,
+				"pci_sriov_configure_simple err %d\n", err);
+
+		return err;
+	}
+
+	si = pci_get_drvdata(pdev);
+	pf = enetc_si_priv(si);
+
+	if (!num_vfs) {
+		pci_disable_sriov(pdev);
+		enetc_msg_psi_free(pf);
+		pf->num_vfs = 0;
+	} else {
+		pf->num_vfs = num_vfs;
+
+		err = enetc_msg_psi_init(pf);
+		if (err) {
+			dev_err(&pdev->dev, "enetc_msg_psi_init (%d)\n", err);
+			goto err_msg_psi;
+		}
+
+		err = pci_enable_sriov(pdev, num_vfs);
+		if (err) {
+			dev_err(&pdev->dev, "pci_enable_sriov err %d\n", err);
+			goto err_en_sriov;
+		}
+	}
+
+	return num_vfs;
+
+err_en_sriov:
+	enetc_msg_psi_free(pf);
+err_msg_psi:
+	pf->num_vfs = 0;
+
+	return err;
+}
+#else
+int enetc_sriov_configure(struct pci_dev *pdev, int num_vfs)
+{
+	return 0;
+}
+#endif
diff --git a/devices/enetc/enetc_ptp.c b/devices/enetc/enetc_ptp.c
new file mode 100755
index 0000000..7638aab
--- /dev/null
+++ b/devices/enetc/enetc_ptp.c
@@ -0,0 +1,145 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+/* Copyright 2019 NXP */
+
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/fsl/ptp_qoriq.h>
+
+#include "enetc.h"
+
+int enetc_phc_index = -1;
+//EXPORT_SYMBOL_GPL(enetc_phc_index);
+
+static struct ptp_clock_info enetc_ptp_caps = {
+	.owner		= THIS_MODULE,
+	.name		= "ENETC PTP clock",
+	.max_adj	= 512000,
+	.n_alarm	= 0,
+	.n_ext_ts	= 2,
+	.n_per_out	= 0,
+	.n_pins		= 0,
+	.pps		= 1,
+	.adjfine	= ptp_qoriq_adjfine,
+	.adjtime	= ptp_qoriq_adjtime,
+	.gettime64	= ptp_qoriq_gettime,
+	.settime64	= ptp_qoriq_settime,
+	.enable		= ptp_qoriq_enable,
+};
+
+static int enetc_ptp_probe(struct pci_dev *pdev,
+			   const struct pci_device_id *ent)
+{
+	struct ptp_qoriq *ptp_qoriq;
+	void __iomem *base;
+	int err, len, n;
+
+	if (pdev->dev.of_node && !of_device_is_available(pdev->dev.of_node)) {
+		dev_info(&pdev->dev, "device is disabled, skipping\n");
+		return -ENODEV;
+	}
+
+	err = pci_enable_device_mem(pdev);
+	if (err)
+		return dev_err_probe(&pdev->dev, err, "device enable failed\n");
+
+	err = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));
+	if (err) {
+		dev_err(&pdev->dev, "DMA configuration failed: 0x%x\n", err);
+		goto err_dma;
+	}
+
+	err = pci_request_mem_regions(pdev, KBUILD_MODNAME);
+	if (err) {
+		dev_err(&pdev->dev, "pci_request_regions failed err=%d\n", err);
+		goto err_pci_mem_reg;
+	}
+
+	pci_set_master(pdev);
+
+	ptp_qoriq = kzalloc(sizeof(*ptp_qoriq), GFP_KERNEL);
+	if (!ptp_qoriq) {
+		err = -ENOMEM;
+		goto err_alloc_ptp;
+	}
+
+	len = pci_resource_len(pdev, ENETC_BAR_REGS);
+
+	base = ioremap(pci_resource_start(pdev, ENETC_BAR_REGS), len);
+	if (!base) {
+		err = -ENXIO;
+		dev_err(&pdev->dev, "ioremap() failed\n");
+		goto err_ioremap;
+	}
+
+	/* Allocate 1 interrupt */
+	n = pci_alloc_irq_vectors(pdev, 1, 1, PCI_IRQ_MSIX);
+	if (n != 1) {
+		err = -EPERM;
+		goto err_irq_vectors;
+	}
+
+	ptp_qoriq->irq = pci_irq_vector(pdev, 0);
+
+	err = request_irq(ptp_qoriq->irq, ptp_qoriq_isr, 0, DRIVER, ptp_qoriq);
+	if (err) {
+		dev_err(&pdev->dev, "request_irq() failed!\n");
+		goto err_irq;
+	}
+
+	ptp_qoriq->dev = &pdev->dev;
+
+	err = ptp_qoriq_init(ptp_qoriq, base, &enetc_ptp_caps);
+	if (err)
+		goto err_no_clock;
+
+	enetc_phc_index = ptp_qoriq->phc_index;
+	pci_set_drvdata(pdev, ptp_qoriq);
+
+	return 0;
+
+err_no_clock:
+	free_irq(ptp_qoriq->irq, ptp_qoriq);
+err_irq:
+	pci_free_irq_vectors(pdev);
+err_irq_vectors:
+	iounmap(base);
+err_ioremap:
+	kfree(ptp_qoriq);
+err_alloc_ptp:
+	pci_release_mem_regions(pdev);
+err_pci_mem_reg:
+err_dma:
+	pci_disable_device(pdev);
+
+	return err;
+}
+
+static void enetc_ptp_remove(struct pci_dev *pdev)
+{
+	struct ptp_qoriq *ptp_qoriq = pci_get_drvdata(pdev);
+
+	enetc_phc_index = -1;
+	ptp_qoriq_free(ptp_qoriq);
+	pci_free_irq_vectors(pdev);
+	kfree(ptp_qoriq);
+
+	pci_release_mem_regions(pdev);
+	pci_disable_device(pdev);
+}
+
+static const struct pci_device_id enetc_ptp_id_table[] = {
+	{ PCI_DEVICE(PCI_VENDOR_ID_FREESCALE, ENETC_DEV_ID_PTP) },
+	{ 0, } /* End of table. */
+};
+MODULE_DEVICE_TABLE(pci, enetc_ptp_id_table);
+
+static struct pci_driver enetc_ptp_driver = {
+	.name = KBUILD_MODNAME,
+	.id_table = enetc_ptp_id_table,
+	.probe = enetc_ptp_probe,
+	.remove = enetc_ptp_remove,
+};
+module_pci_driver(enetc_ptp_driver);
+
+MODULE_DESCRIPTION("ENETC PTP clock driver");
+MODULE_LICENSE("Dual BSD/GPL");
diff --git a/devices/enetc/enetc_qos.c b/devices/enetc/enetc_qos.c
index 6db778a..dfe4f4c 100644
--- a/devices/enetc/enetc_qos.c
+++ b/devices/enetc/enetc_qos.c
@@ -1,7 +1,7 @@
 // SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
 /* Copyright 2019 NXP */
 
-#include "enetc.h"
+#include "enetc_pf.h"
 
 #include <net/pkt_sched.h>
 #include <linux/math64.h>
@@ -9,68 +9,42 @@
 #include <net/pkt_cls.h>
 #include <net/tc_act/tc_gate.h>
 
+static const struct netc_flower enetc_flower[] = {
+	{
+		BIT_ULL(FLOW_ACTION_GATE),
+		BIT_ULL(FLOW_ACTION_POLICE),
+		BIT_ULL(FLOW_DISSECTOR_KEY_ETH_ADDRS) |
+		BIT_ULL(FLOW_DISSECTOR_KEY_VLAN),
+		FLOWER_TYPE_PSFP
+	},
+};
+
 static u16 enetc_get_max_gcl_len(struct enetc_hw *hw)
 {
-	return enetc_rd(hw, ENETC_QBV_PTGCAPR_OFFSET)
-		& ENETC_QBV_MAX_GCL_LEN_MASK;
+	return enetc_rd(hw, ENETC_PTGCAPR) & ENETC_PTGCAPR_MAX_GCL_LEN_MASK;
 }
 
-void enetc_sched_speed_set(struct enetc_ndev_priv *priv, int speed)
-{
-	u32 old_speed = priv->speed;
-	u32 pspeed;
-
-	if (speed == old_speed)
-		return;
-
-	switch (speed) {
-	case SPEED_1000:
-		pspeed = ENETC_PMR_PSPEED_1000M;
-		break;
-	case SPEED_2500:
-		pspeed = ENETC_PMR_PSPEED_2500M;
-		break;
-	case SPEED_100:
-		pspeed = ENETC_PMR_PSPEED_100M;
-		break;
-	case SPEED_10:
-	default:
-		pspeed = ENETC_PMR_PSPEED_10M;
-	}
-
-	priv->speed = speed;
-	enetc_port_wr(&priv->si->hw, ENETC_PMR,
-		      (enetc_port_rd(&priv->si->hw, ENETC_PMR)
-		      & (~ENETC_PMR_PSPEED_MASK))
-		      | pspeed);
-}
-
-static int enetc_setup_taprio(struct net_device *ndev,
+static int enetc_setup_taprio(struct enetc_ndev_priv *priv,
 			      struct tc_taprio_qopt_offload *admin_conf)
 {
-	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	struct enetc_pf *pf = enetc_si_priv(priv->si);
+	struct enetc_hw *hw = &priv->si->hw;
 	struct enetc_cbd cbd = {.cmd = 0};
 	struct tgs_gcl_conf *gcl_config;
 	struct tgs_gcl_data *gcl_data;
-	struct gce *gce;
 	dma_addr_t dma;
+	struct gce *gce;
 	u16 data_size;
 	u16 gcl_len;
-	u32 tge;
+	void *tmp;
 	int err;
 	int i;
 
-	if (admin_conf->num_entries > enetc_get_max_gcl_len(&priv->si->hw))
-		return -EINVAL;
-	gcl_len = admin_conf->num_entries;
+	if (!pf->hw_ops->set_time_gating || !pf->hw_ops->set_tc_msdu)
+		return -EOPNOTSUPP;
 
-	tge = enetc_rd(&priv->si->hw, ENETC_QBV_PTGCR_OFFSET);
-	if (admin_conf->cmd == TAPRIO_CMD_DESTROY) {
-		enetc_wr(&priv->si->hw,
-			 ENETC_QBV_PTGCR_OFFSET,
-			 tge & (~ENETC_QBV_TGE));
-		return 0;
-	}
+	if (admin_conf->num_entries > enetc_get_max_gcl_len(hw))
+		return -EINVAL;
 
 	if (admin_conf->cycle_time > U32_MAX ||
 	    admin_conf->cycle_time_extension > U32_MAX)
@@ -80,10 +54,12 @@ static int enetc_setup_taprio(struct net_device *ndev,
 	 * control BD descriptor.
 	 */
 	gcl_config = &cbd.gcl_conf;
+	gcl_len = admin_conf->num_entries;
 
 	data_size = struct_size(gcl_data, entry, gcl_len);
-	gcl_data = kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
-	if (!gcl_data)
+	tmp = enetc_cbd_alloc_data_mem(priv->si, &cbd, data_size,
+				       &dma, (void *)&gcl_data);
+	if (!tmp)
 		return -ENOMEM;
 
 	gce = (struct gce *)(gcl_data + 1);
@@ -107,61 +83,172 @@ static int enetc_setup_taprio(struct net_device *ndev,
 		temp_gce->period = cpu_to_le32(temp_entry->interval);
 	}
 
-	cbd.length = cpu_to_le16(data_size);
 	cbd.status_flags = 0;
 
-	dma = dma_map_single(&priv->si->pdev->dev, gcl_data,
-			     data_size, DMA_TO_DEVICE);
-	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
-		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
-		kfree(gcl_data);
-		return -ENOMEM;
-	}
-
-	cbd.addr[0] = lower_32_bits(dma);
-	cbd.addr[1] = upper_32_bits(dma);
 	cbd.cls = BDCR_CMD_PORT_GCL;
 	cbd.status_flags = 0;
 
-	enetc_wr(&priv->si->hw, ENETC_QBV_PTGCR_OFFSET,
-		 tge | ENETC_QBV_TGE);
+	pf->hw_ops->set_time_gating(hw, true);
+
+	err = ec_enetc_send_cmd(priv->si, &cbd);
+	if (err)
+		pf->hw_ops->set_time_gating(hw, false);
+
+	enetc_cbd_free_data_mem(priv->si, data_size, tmp, &dma);
 
-	err = enetc_send_cmd(priv->si, &cbd);
 	if (err)
-		enetc_wr(&priv->si->hw,
-			 ENETC_QBV_PTGCR_OFFSET,
-			 tge & (~ENETC_QBV_TGE));
+		return err;
+
+	pf->hw_ops->set_tc_msdu(hw, admin_conf->max_sdu);
+	priv->active_offloads |= ENETC_F_QBV;
+
+	return 0;
+}
 
-	dma_unmap_single(&priv->si->pdev->dev, dma, data_size, DMA_TO_DEVICE);
-	kfree(gcl_data);
+static int enetc4_setup_taprio(struct enetc_ndev_priv *priv,
+			       struct tc_taprio_qopt_offload *admin_conf)
+{
+	struct enetc_pf *pf = enetc_si_priv(priv->si);
+	struct enetc_si *si = priv->si;
+	struct enetc_hw *hw = &si->hw;
+	bool tge_enable;
+	int port, err;
+
+	if (!pf->hw_ops->set_time_gating || !pf->hw_ops->get_time_gating ||
+	    !pf->hw_ops->set_tc_msdu)
+		return -EINVAL;
+
+	port = enetc4_pf_to_port(si->pdev);
+	if (port < 0)
+		return -EINVAL;
+
+	/* Set the maximum frame size for each traffic class */
+	pf->hw_ops->set_tc_msdu(hw, admin_conf->max_sdu);
+
+	tge_enable = pf->hw_ops->get_time_gating(hw);
+	if (!tge_enable)
+		pf->hw_ops->set_time_gating(hw, true);
+
+	err = netc_setup_taprio(&si->ntmp, port, admin_conf);
+	if (err)
+		goto disable_tge;
+
+	priv->active_offloads |= ENETC_F_QBV;
+
+	return 0;
+
+disable_tge:
+	/* We should disable tge if its initial state is disabled */
+	if (!tge_enable)
+		pf->hw_ops->set_time_gating(hw, false);
+
+	if (pf->hw_ops->reset_tc_msdu)
+		pf->hw_ops->reset_tc_msdu(hw);
 
 	return err;
 }
 
-int enetc_setup_tc_taprio(struct net_device *ndev, void *type_data)
+static void enetc_reset_taprio_stats(struct enetc_ndev_priv *priv)
+{
+	int i;
+
+	for (i = 0; i < priv->num_tx_rings; i++)
+		priv->tx_ring[i]->stats.win_drop = 0;
+}
+
+static void enetc_reset_taprio(struct enetc_ndev_priv *priv)
+{
+	struct enetc_pf *pf = enetc_si_priv(priv->si);
+	struct enetc_hw *hw = &priv->si->hw;
+
+	if (pf->hw_ops->set_time_gating)
+		pf->hw_ops->set_time_gating(hw, false);
+
+	if (pf->hw_ops->reset_tc_msdu)
+		pf->hw_ops->reset_tc_msdu(hw);
+
+	priv->active_offloads &= ~ENETC_F_QBV;
+}
+
+static void enetc_taprio_destroy(struct net_device *ndev)
 {
-	struct tc_taprio_qopt_offload *taprio = type_data;
 	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-	int err;
+
+	enetc_reset_taprio(priv);
+	ec_enetc_reset_tc_mqprio(ndev);
+	enetc_reset_taprio_stats(priv);
+}
+
+static void enetc_taprio_stats(struct net_device *ndev,
+			       struct tc_taprio_qopt_stats *stats)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	u64 window_drops = 0;
 	int i;
 
+	for (i = 0; i < priv->num_tx_rings; i++)
+		window_drops += priv->tx_ring[i]->stats.win_drop;
+
+	stats->window_drops = window_drops;
+}
+
+static void enetc_taprio_queue_stats(struct net_device *ndev,
+				     struct tc_taprio_qopt_queue_stats *queue_stats)
+{
+	struct tc_taprio_qopt_stats *stats = &queue_stats->stats;
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	int queue = queue_stats->queue;
+
+	stats->window_drops = priv->tx_ring[queue]->stats.win_drop;
+}
+
+static int enetc_taprio_replace(struct net_device *ndev,
+				struct tc_taprio_qopt_offload *offload)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	struct enetc_si *si = priv->si;
+	int i, err;
+
+	err = ec_enetc_setup_tc_mqprio(ndev, &offload->mqprio);
+	if (err)
+		return err;
+
 	/* TSD and Qbv are mutually exclusive in hardware */
 	for (i = 0; i < priv->num_tx_rings; i++)
 		if (priv->tx_ring[i]->tsd_enable)
 			return -EBUSY;
 
-	for (i = 0; i < priv->num_tx_rings; i++)
-		enetc_set_bdr_prio(&priv->si->hw,
-				   priv->tx_ring[i]->index,
-				   (taprio->cmd != TAPRIO_CMD_DESTROY) ? i : 0);
+	if (is_enetc_rev1(si))
+		err = enetc_setup_taprio(priv, offload);
+	else
+		err = enetc4_setup_taprio(priv, offload);
+	if (err)
+		ec_enetc_reset_tc_mqprio(ndev);
 
-	err = enetc_setup_taprio(ndev, taprio);
+	return err;
+}
 
-	if (err)
-		for (i = 0; i < priv->num_tx_rings; i++)
-			enetc_set_bdr_prio(&priv->si->hw,
-					   priv->tx_ring[i]->index,
-					   (taprio->cmd == TAPRIO_CMD_DESTROY) ? 0 : i);
+int enetc_setup_tc_taprio(struct net_device *ndev, void *type_data)
+{
+	struct tc_taprio_qopt_offload *offload = type_data;
+	int err = 0;
+
+	switch (offload->cmd) {
+	case TAPRIO_CMD_REPLACE:
+		err = enetc_taprio_replace(ndev, offload);
+		break;
+	case TAPRIO_CMD_DESTROY:
+		enetc_taprio_destroy(ndev);
+		break;
+	case TAPRIO_CMD_STATS:
+		enetc_taprio_stats(ndev, &offload->stats);
+		break;
+	case TAPRIO_CMD_QUEUE_STATS:
+		enetc_taprio_queue_stats(ndev, &offload->queue_stats);
+		break;
+	default:
+		err = -EOPNOTSUPP;
+	}
 
 	return err;
 }
@@ -176,13 +263,13 @@ static u8 enetc_get_cbs_bw(struct enetc_hw *hw, u8 tc)
 	return enetc_port_rd(hw, ENETC_PTCCBSR0(tc)) & ENETC_CBS_BW_MASK;
 }
 
-int enetc_setup_tc_cbs(struct net_device *ndev, void *type_data)
+static int enetc_configure_tc_cbs(struct net_device *ndev, void *type_data)
 {
 	struct enetc_ndev_priv *priv = netdev_priv(ndev);
 	struct tc_cbs_qopt_offload *cbs = type_data;
 	u32 port_transmit_rate = priv->speed;
 	u8 tc_nums = netdev_get_num_tc(ndev);
-	struct enetc_si *si = priv->si;
+	struct enetc_hw *hw = &priv->si->hw;
 	u32 hi_credit_bit, hi_credit_reg;
 	u32 max_interference_size;
 	u32 port_frame_max_size;
@@ -191,8 +278,8 @@ int enetc_setup_tc_cbs(struct net_device *ndev, void *type_data)
 	int bw_sum = 0;
 	u8 bw;
 
-	prio_top = netdev_get_prio_tc_map(ndev, tc_nums - 1);
-	prio_next = netdev_get_prio_tc_map(ndev, tc_nums - 2);
+	prio_top = tc_nums - 1;
+	prio_next = tc_nums - 2;
 
 	/* Support highest prio and second prio tc in cbs mode */
 	if (tc != prio_top && tc != prio_next)
@@ -203,15 +290,15 @@ int enetc_setup_tc_cbs(struct net_device *ndev, void *type_data)
 		 * lower than this TC have been disabled.
 		 */
 		if (tc == prio_top &&
-		    enetc_get_cbs_enable(&si->hw, prio_next)) {
+		    enetc_get_cbs_enable(hw, prio_next)) {
 			dev_err(&ndev->dev,
 				"Disable TC%d before disable TC%d\n",
 				prio_next, tc);
 			return -EINVAL;
 		}
 
-		enetc_port_wr(&si->hw, ENETC_PTCCBSR1(tc), 0);
-		enetc_port_wr(&si->hw, ENETC_PTCCBSR0(tc), 0);
+		enetc_port_wr(hw, ENETC_PTCCBSR1(tc), 0);
+		enetc_port_wr(hw, ENETC_PTCCBSR0(tc), 0);
 
 		return 0;
 	}
@@ -228,13 +315,13 @@ int enetc_setup_tc_cbs(struct net_device *ndev, void *type_data)
 	 * higher than this TC have been enabled.
 	 */
 	if (tc == prio_next) {
-		if (!enetc_get_cbs_enable(&si->hw, prio_top)) {
+		if (!enetc_get_cbs_enable(hw, prio_top)) {
 			dev_err(&ndev->dev,
 				"Enable TC%d first before enable TC%d\n",
 				prio_top, prio_next);
 			return -EINVAL;
 		}
-		bw_sum += enetc_get_cbs_bw(&si->hw, prio_top);
+		bw_sum += enetc_get_cbs_bw(hw, prio_top);
 	}
 
 	if (bw_sum + bw >= 100) {
@@ -243,7 +330,7 @@ int enetc_setup_tc_cbs(struct net_device *ndev, void *type_data)
 		return -EINVAL;
 	}
 
-	enetc_port_rd(&si->hw, ENETC_PTCMSDUR(tc));
+	enetc_port_rd(hw, ENETC_PTCMSDUR(tc));
 
 	/* For top prio TC, the max_interfrence_size is maxSizedFrame.
 	 *
@@ -263,8 +350,8 @@ int enetc_setup_tc_cbs(struct net_device *ndev, void *type_data)
 		u32 m0, ma, r0, ra;
 
 		m0 = port_frame_max_size * 8;
-		ma = enetc_port_rd(&si->hw, ENETC_PTCMSDUR(prio_top)) * 8;
-		ra = enetc_get_cbs_bw(&si->hw, prio_top) *
+		ma = enetc_port_rd(hw, ENETC_PTCMSDUR(prio_top)) * 8;
+		ra = enetc_get_cbs_bw(hw, prio_top) *
 			port_transmit_rate * 10000ULL;
 		r0 = port_transmit_rate * 1000000ULL;
 		max_interference_size = m0 + ma +
@@ -284,40 +371,222 @@ int enetc_setup_tc_cbs(struct net_device *ndev, void *type_data)
 	hi_credit_reg = (u32)div_u64((ENETC_CLK * 100ULL) * hi_credit_bit,
 				     port_transmit_rate * 1000000ULL);
 
-	enetc_port_wr(&si->hw, ENETC_PTCCBSR1(tc), hi_credit_reg);
+	enetc_port_wr(hw, ENETC_PTCCBSR1(tc), hi_credit_reg);
 
 	/* Set bw register and enable this traffic class */
-	enetc_port_wr(&si->hw, ENETC_PTCCBSR0(tc), bw | ENETC_CBSE);
+	enetc_port_wr(hw, ENETC_PTCCBSR0(tc), bw | ENETC_CBSE);
 
 	return 0;
 }
 
-int enetc_setup_tc_txtime(struct net_device *ndev, void *type_data)
+static inline u32 enetc4_get_cbs_enable(struct enetc_hw *hw, int tc)
+{
+	return enetc_port_rd(hw, ENETC4_PTCCBSR0(tc)) & PTCCBSR0_CBSE;
+}
+
+static void enetc4_set_tc_cbs_params(struct enetc_hw *hw, int tc,
+				     bool en, u32 bw, u32 hi_credit)
+{
+	if (en) {
+		u32 val = PTCCBSR0_CBSE;
+
+		val |= (bw / 10) & PTCCBSR0_BW;
+		val |= (bw % 10) << 16;
+
+		enetc_port_wr(hw, ENETC4_PTCCBSR1(tc), hi_credit);
+		enetc_port_wr(hw, ENETC4_PTCCBSR0(tc), val);
+
+	} else {
+		enetc_port_wr(hw, ENETC4_PTCCBSR1(tc), 0);
+		enetc_port_wr(hw, ENETC4_PTCCBSR0(tc), 0);
+	}
+}
+
+static u32 enetc4_get_cbs_bw(struct enetc_hw *hw, int tc)
+{
+	u32 val, bw;
+
+	val = enetc_port_rd(hw, ENETC4_PTCCBSR0(tc));
+	bw = (val & PTCCBSR0_BW) * 10 + PTCCBSR0_GET_FRACT(val);
+
+	return bw;
+}
+
+static inline u32 enetc4_get_tc_msdu(struct enetc_hw *hw, int tc)
+{
+	return enetc_port_rd(hw, ENETC4_PTCTMSDUR(tc)) & PTCTMSDUR_MAXSDU;
+}
+
+static int enetc4_configure_tc_cbs(struct net_device *ndev, void *type_data)
 {
 	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-	struct tc_etf_qopt_offload *qopt = type_data;
+	struct tc_cbs_qopt_offload *cbs = type_data;
+	u32 port_transmit_rate = priv->speed;
 	u8 tc_nums = netdev_get_num_tc(ndev);
-	int tc;
+	u32 hi_credit_bit, hi_credit_reg;
+	u8 high_prio_tc, second_prio_tc;
+	struct enetc_si *si = priv->si;
+	struct enetc_hw *hw = &si->hw;
+	u32 max_interference_size;
+	u32 port_frame_max_size;
+	u32 bw, bw_sum;
+	u8 tc;
 
-	if (!tc_nums)
+	high_prio_tc = tc_nums - 1;
+	second_prio_tc = tc_nums - 2;
+
+	tc = netdev_txq_to_tc(ndev, cbs->queue);
+
+	/* Support highest prio and second prio tc in cbs mode */
+	if (tc != high_prio_tc && tc != second_prio_tc)
 		return -EOPNOTSUPP;
 
-	tc = qopt->queue;
+	if (!cbs->enable) {
+		/* Make sure the other TC that are numerically
+		 * lower than this TC have been disabled.
+		 */
+		if (tc == high_prio_tc &&
+		    enetc4_get_cbs_enable(hw, second_prio_tc)) {
+			dev_err(&ndev->dev,
+				"Disable TC%d before disable TC%d\n",
+				second_prio_tc, tc);
+			return -EINVAL;
+		}
+
+		enetc4_set_tc_cbs_params(hw, tc, false, 0, 0);
+
+		return 0;
+	}
+
+	/* The unit of idleslope and sendslope is kbps. And the sendslope should be
+	 * a negative number, it can be calculated as follows, IEEE 802.1Q-2014
+	 * Section 8.6.8.2 item g):
+	 * sendslope = idleslope - port_transmit_rate
+	 */
+	if (cbs->idleslope - cbs->sendslope != port_transmit_rate * 1000L ||
+	    cbs->idleslope < 0 || cbs->sendslope > 0)
+		return -EOPNOTSUPP;
+
+	port_frame_max_size = ndev->mtu + VLAN_ETH_HLEN + ETH_FCS_LEN;
+
+	/* The unit of port_transmit_rate is Mbps, the unit of bw is 1/1000 */
+	bw = cbs->idleslope / port_transmit_rate;
+	bw_sum = bw;
+
+	/* Make sure the credit-based shaper of highest priority TC has been enabled
+	 * before the secondary priority TC.
+	 */
+	if (tc == second_prio_tc) {
+		if (!enetc4_get_cbs_enable(hw, high_prio_tc)) {
+			dev_err(&ndev->dev,
+				"Enable TC%d first before enable TC%d\n",
+				high_prio_tc, second_prio_tc);
+			return -EINVAL;
+		}
+		bw_sum += enetc4_get_cbs_bw(hw, high_prio_tc);
+	}
 
-	if (tc < 0 || tc >= priv->num_tx_rings)
+	if (bw_sum >= 1000) {
+		dev_err(&ndev->dev,
+			"The sum of all CBS Bandwidth can't exceed 1000\n");
 		return -EINVAL;
+	}
 
-	/* Do not support TXSTART and TX CSUM offload simutaniously */
-	if (ndev->features & NETIF_F_CSUM_MASK)
-		return -EBUSY;
+	/* For the AVB Class A (highest priority TC), the max_interfrence_size is
+	 * maximum sized frame for the port.
+	 * For the AVB Class B (second highest priority TC), the max_interfrence_size
+	 * is calculated as below:
+	 *
+	 *      max_interference_size = (Ra * M0) / (R0 - Ra) + MA + M0
+	 *
+	 *	- RA: idleSlope for AVB Class A
+	 *	- R0: port transmit rate
+	 *	- M0: maximum sized frame for the port
+	 *	- MA: maximum sized frame for AVB Class A
+	 */
+
+	if (tc == high_prio_tc) {
+		max_interference_size = port_frame_max_size * 8;
+	} else {
+		u32 m0, ma;
+		u64 ra, r0;
+
+		m0 = port_frame_max_size * 8;
+		ma = enetc4_get_tc_msdu(hw, high_prio_tc) * 8;
+		ra = enetc4_get_cbs_bw(hw, high_prio_tc) *
+		     port_transmit_rate * 1000ULL;
+		r0 = port_transmit_rate * 1000000ULL;
+		max_interference_size = m0 + ma + (u32)div_u64(ra * m0, r0 - ra);
+	}
+
+	/* hiCredit bits calculate by:
+	 *
+	 * max_interference_size * (idleslope / port_transmit_rate)
+	 */
+	hi_credit_bit = max_interference_size * bw / 1000;
+
+	/* Number of credits per bit is calculated as follows:
+	 *
+	 * (enetClockFrequency / port_transmit_rate) * 100
+	 */
+	hi_credit_reg = (u32)div_u64((ENETC4_CLK * 1000ULL) * hi_credit_bit,
+				     port_transmit_rate * 1000000ULL);
+
+	enetc4_set_tc_cbs_params(hw, tc, true, bw, hi_credit_reg);
+
+	return 0;
+}
+
+int enetc_setup_tc_cbs(struct net_device *ndev, void *type_data)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+
+	if (is_enetc_rev1(priv->si))
+		return enetc_configure_tc_cbs(ndev, type_data);
+	else
+		return enetc4_configure_tc_cbs(ndev, type_data);
+}
+
+int enetc_setup_tc_txtime(struct net_device *ndev, void *type_data)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	struct enetc_pf *pf = enetc_si_priv(priv->si);
+	struct tc_etf_qopt_offload *qopt = type_data;
+	u8 tc_nums = netdev_get_num_tc(ndev);
+	struct enetc_hw *hw = &pf->si->hw;
+	int i, tc;
+
+	if (!tc_nums || !pf->hw_ops->set_tc_tsd)
+		return -EOPNOTSUPP;
+
+	if (qopt->queue < 0 || qopt->queue >= ndev->real_num_tx_queues)
+		return -EINVAL;
 
 	/* TSD and Qbv are mutually exclusive in hardware */
-	if (enetc_rd(&priv->si->hw, ENETC_QBV_PTGCR_OFFSET) & ENETC_QBV_TGE)
+	if (pf->hw_ops->get_time_gating && pf->hw_ops->get_time_gating(hw))
 		return -EBUSY;
 
-	priv->tx_ring[tc]->tsd_enable = qopt->enable;
-	enetc_port_wr(&priv->si->hw, ENETC_PTCTSDR(tc),
-		      qopt->enable ? ENETC_TSDE : 0);
+	tc = netdev_txq_to_tc(ndev, qopt->queue);
+	/* According to the NETC block guide, time specific departure operation
+	 * should only be used on the highest priority traffic class.
+	 */
+	if (tc != tc_nums - 1) {
+		dev_err(&ndev->dev,
+			"TSD should be used on the highest priority TC:%d!\n",
+			tc_nums - 1);
+		return -EINVAL;
+	}
+
+	/* Accordiing to the NETC block guide, all traffic on the traffic class
+	 * should use time specific departure operation.
+	 */
+	for (i = 0; i < ndev->tc_to_txq[tc].count; i++) {
+		u16 offset = ndev->tc_to_txq[tc].offset + i;
+
+		priv->tx_ring[offset]->tsd_enable = qopt->enable;
+	}
+
+	pf->hw_ops->set_tc_tsd(hw, tc, qopt->enable);
 
 	return 0;
 }
@@ -392,7 +661,7 @@ struct enetc_psfp_gate {
 	u32 num_entries;
 	refcount_t refcount;
 	struct hlist_node node;
-	struct action_gate_entry entries[];
+	struct action_gate_entry entries[] __counted_by(num_entries);
 };
 
 /* Only enable the green color frame now
@@ -432,13 +701,13 @@ struct enetc_psfp {
 static struct actions_fwd enetc_act_fwd[] = {
 	{
 		BIT(FLOW_ACTION_GATE),
-		BIT(FLOW_DISSECTOR_KEY_ETH_ADDRS),
+		BIT_ULL(FLOW_DISSECTOR_KEY_ETH_ADDRS),
 		FILTER_ACTION_TYPE_PSFP
 	},
 	{
 		BIT(FLOW_ACTION_POLICE) |
 		BIT(FLOW_ACTION_GATE),
-		BIT(FLOW_DISSECTOR_KEY_ETH_ADDRS),
+		BIT_ULL(FLOW_DISSECTOR_KEY_ETH_ADDRS),
 		FILTER_ACTION_TYPE_PSFP
 	},
 	/* example for ACL actions */
@@ -450,16 +719,12 @@ static struct actions_fwd enetc_act_fwd[] = {
 };
 
 static struct enetc_psfp epsfp = {
+	.dev_bitmap = 0,
 	.psfp_sfi_bitmap = NULL,
 };
 
 static LIST_HEAD(enetc_block_cb_list);
 
-static inline int enetc_get_port(struct enetc_ndev_priv *priv)
-{
-	return priv->si->pdev->devfn & 0x7;
-}
-
 /* Stream Identity Entry Set Descriptor */
 static int enetc_streamid_hw_set(struct enetc_ndev_priv *priv,
 				 struct enetc_streamid *sid,
@@ -468,10 +733,16 @@ static int enetc_streamid_hw_set(struct enetc_ndev_priv *priv,
 	struct enetc_cbd cbd = {.cmd = 0};
 	struct streamid_data *si_data;
 	struct streamid_conf *si_conf;
-	u16 data_size;
 	dma_addr_t dma;
+	u16 data_size;
+	void *tmp;
+	int port;
 	int err;
 
+	port = enetc_pf_to_port(priv->si->pdev);
+	if (port < 0)
+		return -EINVAL;
+
 	if (sid->index >= priv->psfp_cap.max_streamid)
 		return -EINVAL;
 
@@ -485,52 +756,36 @@ static int enetc_streamid_hw_set(struct enetc_ndev_priv *priv,
 	cbd.status_flags = 0;
 
 	data_size = sizeof(struct streamid_data);
-	si_data = kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
-	cbd.length = cpu_to_le16(data_size);
-
-	dma = dma_map_single(&priv->si->pdev->dev, si_data,
-			     data_size, DMA_FROM_DEVICE);
-	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
-		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
-		kfree(si_data);
+	tmp = enetc_cbd_alloc_data_mem(priv->si, &cbd, data_size,
+				       &dma, (void *)&si_data);
+	if (!tmp)
 		return -ENOMEM;
-	}
 
-	cbd.addr[0] = lower_32_bits(dma);
-	cbd.addr[1] = upper_32_bits(dma);
 	eth_broadcast_addr(si_data->dmac);
-	si_data->vid_vidm_tg =
-		cpu_to_le16(ENETC_CBDR_SID_VID_MASK
-			    + ((0x3 << 14) | ENETC_CBDR_SID_VIDM));
+	si_data->vid_vidm_tg = (ENETC_CBDR_SID_VID_MASK
+			       + ((0x3 << 14) | ENETC_CBDR_SID_VIDM));
 
 	si_conf = &cbd.sid_set;
 	/* Only one port supported for one entry, set itself */
-	si_conf->iports = 1 << enetc_get_port(priv);
+	si_conf->iports = cpu_to_le32(1 << port);
 	si_conf->id_type = 1;
 	si_conf->oui[2] = 0x0;
 	si_conf->oui[1] = 0x80;
 	si_conf->oui[0] = 0xC2;
 
-	err = enetc_send_cmd(priv->si, &cbd);
+	err = ec_enetc_send_cmd(priv->si, &cbd);
 	if (err)
-		return -EINVAL;
+		goto out;
 
-	if (!enable) {
-		kfree(si_data);
-		return 0;
-	}
+	if (!enable)
+		goto out;
 
 	/* Enable the entry overwrite again incase space flushed by hardware */
-	memset(&cbd, 0, sizeof(cbd));
-
-	cbd.index = cpu_to_le16((u16)sid->index);
-	cbd.cmd = 0;
-	cbd.cls = BDCR_CMD_STREAM_IDENTIFY;
 	cbd.status_flags = 0;
 
 	si_conf->en = 0x80;
 	si_conf->stream_handle = cpu_to_le32(sid->handle);
-	si_conf->iports = 1 << enetc_get_port(priv);
+	si_conf->iports = cpu_to_le32(1 << port);
 	si_conf->id_type = sid->filtertype;
 	si_conf->oui[2] = 0x0;
 	si_conf->oui[1] = 0x80;
@@ -538,11 +793,6 @@ static int enetc_streamid_hw_set(struct enetc_ndev_priv *priv,
 
 	memset(si_data, 0, data_size);
 
-	cbd.length = cpu_to_le16(data_size);
-
-	cbd.addr[0] = lower_32_bits(dma);
-	cbd.addr[1] = upper_32_bits(dma);
-
 	/* VIDM default to be 1.
 	 * VID Match. If set (b1) then the VID must match, otherwise
 	 * any VID is considered a match. VIDM setting is only used
@@ -550,20 +800,19 @@ static int enetc_streamid_hw_set(struct enetc_ndev_priv *priv,
 	 */
 	if (si_conf->id_type == STREAMID_TYPE_NULL) {
 		ether_addr_copy(si_data->dmac, sid->dst_mac);
-		si_data->vid_vidm_tg =
-		cpu_to_le16((sid->vid & ENETC_CBDR_SID_VID_MASK) +
-			    ((((u16)(sid->tagged) & 0x3) << 14)
-			     | ENETC_CBDR_SID_VIDM));
+		si_data->vid_vidm_tg = (sid->vid & ENETC_CBDR_SID_VID_MASK) +
+				       ((((u16)(sid->tagged) & 0x3) << 14)
+				       | ENETC_CBDR_SID_VIDM);
 	} else if (si_conf->id_type == STREAMID_TYPE_SMAC) {
 		ether_addr_copy(si_data->smac, sid->src_mac);
-		si_data->vid_vidm_tg =
-		cpu_to_le16((sid->vid & ENETC_CBDR_SID_VID_MASK) +
-			    ((((u16)(sid->tagged) & 0x3) << 14)
-			     | ENETC_CBDR_SID_VIDM));
+		si_data->vid_vidm_tg = (sid->vid & ENETC_CBDR_SID_VID_MASK) +
+				       ((((u16)(sid->tagged) & 0x3) << 14)
+				       | ENETC_CBDR_SID_VIDM);
 	}
 
-	err = enetc_send_cmd(priv->si, &cbd);
-	kfree(si_data);
+	err = ec_enetc_send_cmd(priv->si, &cbd);
+out:
+	enetc_cbd_free_data_mem(priv->si, data_size, tmp, &dma);
 
 	return err;
 }
@@ -575,6 +824,11 @@ static int enetc_streamfilter_hw_set(struct enetc_ndev_priv *priv,
 {
 	struct enetc_cbd cbd = {.cmd = 0};
 	struct sfi_conf *sfi_config;
+	int port;
+
+	port = enetc_pf_to_port(priv->si->pdev);
+	if (port < 0)
+		return -EINVAL;
 
 	cbd.index = cpu_to_le16(sfi->index);
 	cbd.cls = BDCR_CMD_STREAM_FILTER;
@@ -594,7 +848,7 @@ static int enetc_streamfilter_hw_set(struct enetc_ndev_priv *priv,
 	}
 
 	sfi_config->sg_inst_table_index = cpu_to_le16(sfi->gate_id);
-	sfi_config->input_ports = 1 << enetc_get_port(priv);
+	sfi_config->input_ports = cpu_to_le32(1 << port);
 
 	/* The priority value which may be matched against the
 	 * frames priority value to determine a match for this entry.
@@ -618,7 +872,7 @@ static int enetc_streamfilter_hw_set(struct enetc_ndev_priv *priv,
 	}
 
 exit:
-	return enetc_send_cmd(priv->si, &cbd);
+	return ec_enetc_send_cmd(priv->si, &cbd);
 }
 
 static int enetc_streamcounter_hw_get(struct enetc_ndev_priv *priv,
@@ -629,6 +883,7 @@ static int enetc_streamcounter_hw_get(struct enetc_ndev_priv *priv,
 	struct sfi_counter_data *data_buf;
 	dma_addr_t dma;
 	u16 data_size;
+	void *tmp;
 	int err;
 
 	cbd.index = cpu_to_le16((u16)index);
@@ -637,51 +892,42 @@ static int enetc_streamcounter_hw_get(struct enetc_ndev_priv *priv,
 	cbd.status_flags = 0;
 
 	data_size = sizeof(struct sfi_counter_data);
-	data_buf = kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
-	if (!data_buf)
-		return -ENOMEM;
 
-	dma = dma_map_single(&priv->si->pdev->dev, data_buf,
-			     data_size, DMA_FROM_DEVICE);
-	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
-		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
-		err = -ENOMEM;
-		goto exit;
-	}
-	cbd.addr[0] = lower_32_bits(dma);
-	cbd.addr[1] = upper_32_bits(dma);
-
-	cbd.length = cpu_to_le16(data_size);
+	tmp = enetc_cbd_alloc_data_mem(priv->si, &cbd, data_size,
+				       &dma, (void *)&data_buf);
+	if (!tmp)
+		return -ENOMEM;
 
-	err = enetc_send_cmd(priv->si, &cbd);
+	err = ec_enetc_send_cmd(priv->si, &cbd);
 	if (err)
 		goto exit;
 
 	cnt->matching_frames_count =
-			((u64)le32_to_cpu(data_buf->matchh) << 32)
-			+ data_buf->matchl;
+		((u64)le32_to_cpu(data_buf->matchh) << 32) +
+		le32_to_cpu(data_buf->matchl);
 
 	cnt->not_passing_sdu_count =
-			((u64)le32_to_cpu(data_buf->msdu_droph) << 32)
-			+ data_buf->msdu_dropl;
+		((u64)le32_to_cpu(data_buf->msdu_droph) << 32) +
+		le32_to_cpu(data_buf->msdu_dropl);
 
-	cnt->passing_sdu_count = cnt->matching_frames_count
-				- cnt->not_passing_sdu_count;
+	cnt->passing_sdu_count =
+		cnt->matching_frames_count - cnt->not_passing_sdu_count;
 
 	cnt->not_passing_frames_count =
-		((u64)le32_to_cpu(data_buf->stream_gate_droph) << 32)
-		+ le32_to_cpu(data_buf->stream_gate_dropl);
+		((u64)le32_to_cpu(data_buf->stream_gate_droph) << 32) +
+		le32_to_cpu(data_buf->stream_gate_dropl);
 
-	cnt->passing_frames_count = cnt->matching_frames_count
-				- cnt->not_passing_sdu_count
-				- cnt->not_passing_frames_count;
+	cnt->passing_frames_count = cnt->matching_frames_count -
+				    cnt->not_passing_sdu_count -
+				    cnt->not_passing_frames_count;
 
 	cnt->red_frames_count =
-		((u64)le32_to_cpu(data_buf->flow_meter_droph) << 32)
-		+ le32_to_cpu(data_buf->flow_meter_dropl);
+		((u64)le32_to_cpu(data_buf->flow_meter_droph) << 32) +
+		le32_to_cpu(data_buf->flow_meter_dropl);
 
 exit:
-	kfree(data_buf);
+	enetc_cbd_free_data_mem(priv->si, data_size, tmp, &dma);
+
 	return err;
 }
 
@@ -723,6 +969,7 @@ static int enetc_streamgate_hw_set(struct enetc_ndev_priv *priv,
 	dma_addr_t dma;
 	u16 data_size;
 	int err, i;
+	void *tmp;
 	u64 now;
 
 	cbd.index = cpu_to_le16(sgi->index);
@@ -732,7 +979,7 @@ static int enetc_streamgate_hw_set(struct enetc_ndev_priv *priv,
 
 	/* disable */
 	if (!enable)
-		return enetc_send_cmd(priv->si, &cbd);
+		return ec_enetc_send_cmd(priv->si, &cbd);
 
 	if (!sgi->num_entries)
 		return 0;
@@ -753,7 +1000,7 @@ static int enetc_streamgate_hw_set(struct enetc_ndev_priv *priv,
 	sgi_config->en = 0x80;
 
 	/* Basic config */
-	err = enetc_send_cmd(priv->si, &cbd);
+	err = ec_enetc_send_cmd(priv->si, &cbd);
 	if (err)
 		return -EINVAL;
 
@@ -769,25 +1016,11 @@ static int enetc_streamgate_hw_set(struct enetc_ndev_priv *priv,
 	sgcl_config->acl_len = (sgi->num_entries - 1) & 0x3;
 
 	data_size = struct_size(sgcl_data, sgcl, sgi->num_entries);
-
-	sgcl_data = kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
-	if (!sgcl_data)
+	tmp = enetc_cbd_alloc_data_mem(priv->si, &cbd, data_size,
+				       &dma, (void *)&sgcl_data);
+	if (!tmp)
 		return -ENOMEM;
 
-	cbd.length = cpu_to_le16(data_size);
-
-	dma = dma_map_single(&priv->si->pdev->dev,
-			     sgcl_data, data_size,
-			     DMA_FROM_DEVICE);
-	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
-		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
-		kfree(sgcl_data);
-		return -ENOMEM;
-	}
-
-	cbd.addr[0] = lower_32_bits(dma);
-	cbd.addr[1] = upper_32_bits(dma);
-
 	sgce = &sgcl_data->sgcl[0];
 
 	sgcl_config->agtst = 0x80;
@@ -838,11 +1071,10 @@ static int enetc_streamgate_hw_set(struct enetc_ndev_priv *priv,
 		sgcl_data->btl = cpu_to_le32(lo);
 	}
 
-	err = enetc_send_cmd(priv->si, &cbd);
+	err = ec_enetc_send_cmd(priv->si, &cbd);
 
 exit:
-	kfree(sgcl_data);
-
+	enetc_cbd_free_data_mem(priv->si, data_size, tmp, &dma);
 	return err;
 }
 
@@ -859,7 +1091,7 @@ static int enetc_flowmeter_hw_set(struct enetc_ndev_priv *priv,
 	cbd.status_flags = 0x80;
 
 	if (!enable)
-		return enetc_send_cmd(priv->si, &cbd);
+		return ec_enetc_send_cmd(priv->si, &cbd);
 
 	fmi_config = &cbd.fmi_conf;
 	fmi_config->en = 0x80;
@@ -884,7 +1116,7 @@ static int enetc_flowmeter_hw_set(struct enetc_ndev_priv *priv,
 	 */
 	fmi_config->conf = 0;
 
-	return enetc_send_cmd(priv->si, &cbd);
+	return ec_enetc_send_cmd(priv->si, &cbd);
 }
 
 static struct enetc_stream_filter *enetc_get_stream_by_index(u32 index)
@@ -1058,8 +1290,8 @@ revert_sid:
 	return err;
 }
 
-static struct actions_fwd *enetc_check_flow_actions(u64 acts,
-						    unsigned int inputkeys)
+static struct actions_fwd *
+enetc_check_flow_actions(u64 acts, unsigned long long inputkeys)
 {
 	int i;
 
@@ -1071,6 +1303,46 @@ static struct actions_fwd *enetc_check_flow_actions(u64 acts,
 	return NULL;
 }
 
+static int enetc_psfp_policer_validate(const struct flow_action *action,
+				       const struct flow_action_entry *act,
+				       struct netlink_ext_ack *extack)
+{
+	if (act->police.exceed.act_id != FLOW_ACTION_DROP) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Offload not supported when exceed action is not drop");
+		return -EOPNOTSUPP;
+	}
+
+	if (act->police.notexceed.act_id != FLOW_ACTION_PIPE &&
+	    act->police.notexceed.act_id != FLOW_ACTION_ACCEPT) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Offload not supported when conform action is not pipe or ok");
+		return -EOPNOTSUPP;
+	}
+
+	if (act->police.notexceed.act_id == FLOW_ACTION_ACCEPT &&
+	    !flow_action_is_last_entry(action, act)) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Offload not supported when conform action is ok, but action is not last");
+		return -EOPNOTSUPP;
+	}
+
+	if (act->police.peakrate_bytes_ps ||
+	    act->police.avrate || act->police.overhead) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Offload not supported when peakrate/avrate/overhead is configured");
+		return -EOPNOTSUPP;
+	}
+
+	if (act->police.rate_pkt_ps) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "QoS offload not support packets per second");
+		return -EOPNOTSUPP;
+	}
+
+	return 0;
+}
+
 static int enetc_psfp_parse_clsflower(struct enetc_ndev_priv *priv,
 				      struct flow_cls_offload *f)
 {
@@ -1227,6 +1499,10 @@ static int enetc_psfp_parse_clsflower(struct enetc_ndev_priv *priv,
 
 	/* Flow meter and max frame size */
 	if (entryp) {
+		err = enetc_psfp_policer_validate(&rule->action, entryp, extack);
+		if (err)
+			goto free_sfi;
+
 		if (entryp->police.burst) {
 			fmi = kzalloc(sizeof(*fmi), GFP_KERNEL);
 			if (!fmi) {
@@ -1257,7 +1533,7 @@ static int enetc_psfp_parse_clsflower(struct enetc_ndev_priv *priv,
 		int index;
 
 		index = enetc_get_free_index(priv);
-		if (sfi->handle < 0) {
+		if (index < 0) {
 			NL_SET_ERR_MSG_MOD(extack, "No Stream Filter resource!");
 			err = -ENOSPC;
 			goto free_fmi;
@@ -1456,6 +1732,153 @@ static int enetc_setup_tc_cls_flower(struct enetc_ndev_priv *priv,
 	}
 }
 
+static const struct netc_flower *enetc4_parse_tc_flower(u64 actions, u64 keys)
+{
+	u64 key_acts, all_acts;
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(enetc_flower); i++) {
+		key_acts = enetc_flower[i].key_acts;
+		all_acts = enetc_flower[i].key_acts |
+			   enetc_flower[i].opt_acts;
+
+		/* key_acts must be matched */
+		if ((actions & key_acts) == key_acts &&
+		    (actions & all_acts) == actions &&
+		    keys & enetc_flower[i].keys)
+			return &enetc_flower[i];
+	}
+
+	return NULL;
+}
+
+static int enetc4_config_clsflower(struct enetc_ndev_priv *priv,
+				   struct flow_cls_offload *f)
+{
+	struct flow_rule *rule = flow_cls_offload_flow_rule(f);
+	struct netlink_ext_ack *extack = f->common.extack;
+	struct flow_action *action = &rule->action;
+	struct flow_dissector *dissector;
+	const struct netc_flower *flower;
+	struct flow_action_entry *entry;
+	u64 actions = 0;
+	int i;
+
+	dissector = rule->match.dissector;
+
+	if (!flow_action_has_entries(action)) {
+		NL_SET_ERR_MSG_MOD(extack, "At least one action is needed");
+		return -EINVAL;
+	}
+
+	if (!flow_action_basic_hw_stats_check(action, extack))
+		return -EOPNOTSUPP;
+
+	flow_action_for_each(i, entry, action)
+		actions |= BIT_ULL(entry->id);
+
+	flower = enetc4_parse_tc_flower(actions, dissector->used_keys);
+	if (!flower) {
+		NL_SET_ERR_MSG_MOD(extack, "Unsupported actions or keys");
+		return -EOPNOTSUPP;
+	}
+
+	switch (flower->type) {
+	case FLOWER_TYPE_PSFP:
+		return netc_setup_psfp(&priv->si->ntmp, 0, f);
+	default:
+		NL_SET_ERR_MSG_MOD(extack, "Unsupported flower type");
+		return -EOPNOTSUPP;
+	}
+}
+
+static void enetc4_destroy_flower_rule(struct ntmp_priv *ntmp,
+				       struct netc_flower_rule *rule)
+{
+	switch (rule->flower_type) {
+	case FLOWER_TYPE_PSFP:
+		netc_delete_psfp_flower_rule(ntmp, rule);
+		break;
+	default:
+		break;
+	}
+}
+
+static int enetc4_destroy_clsflower(struct enetc_ndev_priv *priv,
+				    struct flow_cls_offload *f)
+{
+	struct netlink_ext_ack *extack = f->common.extack;
+	struct ntmp_priv *ntmp = &priv->si->ntmp;
+	unsigned long cookie = f->cookie;
+	struct netc_flower_rule *rule;
+
+	guard(mutex)(&ntmp->flower_lock);
+	rule = netc_find_flower_rule_by_cookie(ntmp, 0, cookie);
+	if (!rule) {
+		NL_SET_ERR_MSG_MOD(extack, "Cannot find the rule");
+		return -EINVAL;
+	}
+
+	enetc4_destroy_flower_rule(ntmp, rule);
+
+	return 0;
+}
+
+static int enetc4_get_cls_flower_stats(struct enetc_ndev_priv *priv,
+				       struct flow_cls_offload *f)
+{
+	struct netlink_ext_ack *extack = f->common.extack;
+	struct ntmp_priv *ntmp = &priv->si->ntmp;
+	unsigned long cookie = f->cookie;
+	struct netc_flower_rule *rule;
+	u64 pkt_cnt, drop_cnt;
+	u64 byte_cnt = 0;
+	int err;
+
+	guard(mutex)(&ntmp->flower_lock);
+	rule = netc_find_flower_rule_by_cookie(ntmp, 0, cookie);
+	if (!rule) {
+		NL_SET_ERR_MSG_MOD(extack, "Cannot find the rule");
+		return -EINVAL;
+	}
+
+	switch (rule->flower_type) {
+	case FLOWER_TYPE_PSFP:
+		err = netc_psfp_flower_stat(ntmp, rule, &byte_cnt,
+					    &pkt_cnt, &drop_cnt);
+		if (err) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "Failed to get statistics of PSFP");
+			return err;
+		}
+		break;
+	default:
+		NL_SET_ERR_MSG_MOD(extack, "Unknown flower type");
+		return -EINVAL;
+	}
+
+	flow_stats_update(&f->stats, byte_cnt, pkt_cnt, drop_cnt,
+			  rule->lastused, FLOW_ACTION_HW_STATS_IMMEDIATE);
+	rule->lastused = jiffies;
+
+	return 0;
+}
+
+static int enetc4_setup_tc_cls_flower(struct enetc_ndev_priv *priv,
+				      struct flow_cls_offload *f)
+{
+	switch (f->command) {
+	case FLOW_CLS_REPLACE:
+		return enetc4_config_clsflower(priv, f);
+	case FLOW_CLS_DESTROY:
+		return enetc4_destroy_clsflower(priv, f);
+	case FLOW_CLS_STATS:
+		return enetc4_get_cls_flower_stats(priv, f);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
 static inline void clean_psfp_sfi_bitmap(void)
 {
 	bitmap_free(epsfp.psfp_sfi_bitmap);
@@ -1505,22 +1928,99 @@ static void clean_psfp_all(void)
 	clean_psfp_sfi_bitmap();
 }
 
-int enetc_setup_tc_block_cb(enum tc_setup_type type, void *type_data,
-			    void *cb_priv)
+static int enetc_setup_tc_block_cb(enum tc_setup_type type, void *type_data,
+				   void *cb_priv)
 {
 	struct net_device *ndev = cb_priv;
+	struct enetc_ndev_priv *priv;
 
 	if (!tc_can_offload(ndev))
 		return -EOPNOTSUPP;
 
+	priv = netdev_priv(ndev);
 	switch (type) {
 	case TC_SETUP_CLSFLOWER:
-		return enetc_setup_tc_cls_flower(netdev_priv(ndev), type_data);
+		if (is_enetc_rev1(priv->si))
+			return enetc_setup_tc_cls_flower(priv, type_data);
+		else
+			return enetc4_setup_tc_cls_flower(priv, type_data);
 	default:
 		return -EOPNOTSUPP;
 	}
 }
 
+void enetc4_clear_flower_list(struct enetc_si *si)
+{
+	struct ntmp_priv *ntmp = &si->ntmp;
+	struct netc_flower_rule *rule;
+	struct hlist_node *tmp;
+
+	guard(mutex)(&ntmp->flower_lock);
+	hlist_for_each_entry_safe(rule, tmp, &ntmp->flower_list, node)
+		enetc4_destroy_flower_rule(ntmp, rule);
+}
+
+static int enetc4_tc_flower_destory(struct enetc_ndev_priv *priv)
+{
+	if (!list_empty(&enetc_block_cb_list))
+		return -EBUSY;
+
+	enetc4_clear_flower_list(priv->si);
+
+	return 0;
+}
+
+static int enetc_tc_flower_enable(struct enetc_ndev_priv *priv)
+{
+	struct enetc_hw *hw = &priv->si->hw;
+	int err;
+
+	if (is_enetc_rev1(priv->si)) {
+		enetc_get_max_cap(priv);
+
+		err = enetc_psfp_init(priv);
+		if (err)
+			return err;
+
+		enetc_wr(hw, ENETC_PPSFPMR, enetc_rd(hw, ENETC_PPSFPMR) |
+			ENETC_PPSFPMR_PSFPEN | ENETC_PPSFPMR_VS |
+			ENETC_PPSFPMR_PVC | ENETC_PPSFPMR_PVZC);
+	}
+
+	return 0;
+}
+
+static int enetc_tc_flower_disable(struct enetc_ndev_priv *priv)
+{
+	if (is_enetc_rev1(priv->si))
+		return enetc_psfp_disable(priv);
+	else
+		return enetc4_tc_flower_destory(priv);
+}
+
+int enetc_set_tc_flower(struct net_device *ndev, bool en)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	int err;
+
+	if (en) {
+		err = enetc_tc_flower_enable(priv);
+		if (err)
+			return err;
+
+		priv->active_offloads |= ENETC_F_QCI;
+		return 0;
+	}
+
+	err = enetc_tc_flower_disable(priv);
+	if (err)
+		return err;
+
+	priv->active_offloads &= ~ENETC_F_QCI;
+
+	return 0;
+}
+
 int enetc_psfp_init(struct enetc_ndev_priv *priv)
 {
 	if (epsfp.psfp_sfi_bitmap)
@@ -1553,7 +2053,7 @@ int enetc_setup_tc_psfp(struct net_device *ndev, void *type_data)
 {
 	struct enetc_ndev_priv *priv = netdev_priv(ndev);
 	struct flow_block_offload *f = type_data;
-	int err;
+	int port, err;
 
 	err = flow_block_cb_setup_simple(f, &enetc_block_cb_list,
 					 enetc_setup_tc_block_cb,
@@ -1561,12 +2061,23 @@ int enetc_setup_tc_psfp(struct net_device *ndev, void *type_data)
 	if (err)
 		return err;
 
+	if (!is_enetc_rev1(priv->si))
+		return 0;
+
 	switch (f->command) {
 	case FLOW_BLOCK_BIND:
-		set_bit(enetc_get_port(priv), &epsfp.dev_bitmap);
+		port = enetc_pf_to_port(priv->si->pdev);
+		if (port < 0)
+			return -EINVAL;
+
+		set_bit(port, &epsfp.dev_bitmap);
 		break;
 	case FLOW_BLOCK_UNBIND:
-		clear_bit(enetc_get_port(priv), &epsfp.dev_bitmap);
+		port = enetc_pf_to_port(priv->si->pdev);
+		if (port < 0)
+			return -EINVAL;
+
+		clear_bit(port, &epsfp.dev_bitmap);
 		if (!epsfp.dev_bitmap)
 			clean_psfp_all();
 		break;
@@ -1574,3 +2085,30 @@ int enetc_setup_tc_psfp(struct net_device *ndev, void *type_data)
 
 	return 0;
 }
+
+int enetc_qos_query_caps(struct net_device *ndev, void *type_data)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	struct tc_query_caps_base *base = type_data;
+	struct enetc_si *si = priv->si;
+
+	switch (base->type) {
+	case TC_SETUP_QDISC_MQPRIO: {
+		struct tc_mqprio_caps *caps = base->caps;
+
+		caps->validate_queue_counts = true;
+
+		return 0;
+	}
+	case TC_SETUP_QDISC_TAPRIO: {
+		struct tc_taprio_caps *caps = base->caps;
+
+		if (si->hw_features & ENETC_SI_F_QBV)
+			caps->supports_queue_max_sdu = true;
+
+		return 0;
+	}
+	default:
+		return -EOPNOTSUPP;
+	}
+}
diff --git a/devices/enetc/enetc_vf.c b/devices/enetc/enetc_vf.c
index d205a8e..a14262f 100644
--- a/devices/enetc/enetc_vf.c
+++ b/devices/enetc/enetc_vf.c
@@ -17,9 +17,9 @@ static const char enetc_drv_name[] = ENETC_DRV_NAME_STR;
 
 /* Probing/ Init */
 static const struct net_device_ops enetc_ndev_ops = {
-	.ndo_open		= enetc_open,
-	.ndo_stop		= enetc_close,
-	.ndo_start_xmit		= enetc_xmit,
+	.ndo_open		= ec_enetc_open,
+	.ndo_stop		= ec_enetc_close,
+	.ndo_start_xmit		= ec_enetc_xmit,
 };
 
 static void enetc_vf_netdev_setup(struct enetc_si *si, struct net_device *ndev,
@@ -35,7 +35,7 @@ static void enetc_vf_netdev_setup(struct enetc_si *si, struct net_device *ndev,
 
 	priv->msg_enable = (NETIF_MSG_IFUP << 1) - 1;
 	ndev->netdev_ops = ndev_ops;
-	enetc_set_ethtool_ops(ndev);
+	ec_enetc_set_ethtool_ops(ndev);
 	ndev->watchdog_timeo = 5 * HZ;
 	ndev->max_mtu = ENETC_MAX_MTU;
 
@@ -75,7 +75,7 @@ static int enetc_vf_probe(struct pci_dev *pdev,
 	struct enetc_si *si;
 	int err;
 
-	err = enetc_pci_probe(pdev, KBUILD_MODNAME, 0);
+	err = ec_enetc_pci_probe(pdev, KBUILD_MODNAME, 0);
 	if (err) {
 		dev_err(&pdev->dev, "PCI probing failed\n");
 		return err;
@@ -83,7 +83,7 @@ static int enetc_vf_probe(struct pci_dev *pdev,
 
 	si = pci_get_drvdata(pdev);
 
-	enetc_get_si_caps(si);
+	ec_enetc_get_si_caps(si);
 
 	ndev = alloc_etherdev(sizeof(*priv));
 	if (!ndev) {
@@ -96,9 +96,9 @@ static int enetc_vf_probe(struct pci_dev *pdev,
 
 	priv = netdev_priv(ndev);
 
-	enetc_init_si_rings_params(priv);
+	ec_enetc_init_si_rings_params(priv);
 
-	err = enetc_alloc_si_resources(priv);
+	err = ec_enetc_alloc_si_resources(priv);
 	if (err) {
 		dev_err(&pdev->dev, "SI resource alloc failed\n");
 		goto err_alloc_si_res;
@@ -134,12 +134,12 @@ static int enetc_vf_probe(struct pci_dev *pdev,
 err_reg_ec_net:
 	enetc_free_rings(priv);
 err_get_ptp:
-	enetc_free_si_resources(priv);
+	ec_enetc_free_si_resources(priv);
 err_alloc_si_res:
 	si->ndev = NULL;
 	free_netdev(ndev);
 err_alloc_netdev:
-	enetc_pci_remove(pdev);
+	ec_enetc_pci_remove(pdev);
 
 	return err;
 }
@@ -156,11 +156,11 @@ static void enetc_vf_remove(struct pci_dev *pdev)
 	ecdev_close(priv->ecdev);
 	ecdev_withdraw(priv->ecdev);
 
-	enetc_free_si_resources(priv);
+	ec_enetc_free_si_resources(priv);
 	enetc_free_rings(priv);
 	free_netdev(si->ndev);
 
-	enetc_pci_remove(pdev);
+	ec_enetc_pci_remove(pdev);
 }
 
 static const struct pci_device_id enetc_vf_id_table[] = {
diff --git a/devices/enetc/netc_blk_ctrl.c b/devices/enetc/netc_blk_ctrl.c
new file mode 100755
index 0000000..c23d2bc
--- /dev/null
+++ b/devices/enetc/netc_blk_ctrl.c
@@ -0,0 +1,892 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+/*
+ * NXP NETC Blocks Control Driver
+ *
+ * Copyright 2024 NXP
+ */
+#include <linux/clk.h>
+#include <linux/debugfs.h>
+#include <linux/delay.h>
+#include <linux/fsl/netc_global.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/of_net.h>
+#include <linux/of_platform.h>
+#include <linux/phy.h>
+#include <linux/seq_file.h>
+
+/* NETCMIX registers */
+#define IMX95_CFG_LINK_IO_VAR		0x0
+#define  IO_VAR_16FF_16G_SERDES		0x1
+#define  IO_VAR(port, var)		(((var) & 0xf) << ((port) << 2))
+
+#define IMX95_CFG_LINK_MII_PROT		0x4
+#define CFG_LINK_MII_PORT_0		GENMASK(3, 0)
+#define CFG_LINK_MII_PORT_1		GENMASK(7, 4)
+#define  MII_PROT_MII			0x0
+#define  MII_PROT_RMII			0x1
+#define  MII_PROT_RGMII			0x2
+#define  MII_PROT_SERIAL		0x3
+#define  MII_PROT(port, prot)		(((prot) & 0xf) << ((port) << 2))
+
+#define IMX95_CFG_LINK_PCS_PROT(a)	(0x8 + (a) * 4)
+#define PCS_PROT_1G_SGMII		BIT(0)
+#define PCS_PROT_2500M_SGMII		BIT(1)
+#define PCS_PROT_XFI			BIT(3)
+#define PCS_PROT_SFI			BIT(4)
+#define PCS_PROT_10G_SXGMII		BIT(6)
+
+#define IMX94_MISC_SOC_CONTROL		0x0
+#define  SEL_XPCS_1			BIT(1)
+#define   IMX94_XPCS_PORT_0		0x0
+#define   IMX94_XPCS_PORT_1		0x1
+
+#define IMX94_EXT_PIN_CONTROL		0x10
+#define  MAC2_MAC3_SEL			BIT(1)
+
+#define IMX94_CFG_LINK_PCS_PROT(a)	(0x14 + (a) * 4)
+#define IMX94_NETC_LINK_CFG(a)		(0x4c + (a) * 4)
+#define  NETC_LINK_CFG_MII_PROT		GENMASK(3, 0)
+#define  NETC_LINK_CFG_IO_VAR		GENMASK(19, 16)
+
+/* NETC privileged register block register */
+#define PRB_NETCRR			0x100
+#define  NETCRR_SR			BIT(0)
+#define  NETCRR_LOCK			BIT(1)
+
+#define PRB_NETCSR			0x104
+#define  NETCSR_ERROR			BIT(0)
+#define  NETCSR_STATE			BIT(1)
+
+/* NETC integrated endpoint register block register */
+#define IERB_EMDIOFAUXR			0x344
+#define IERB_T0FAUXR			0x444
+#define IERB_ETBCR(a)			(0x300c + 0x100 * (a))
+#define IERB_EFAUXR(a)			(0x3044 + 0x100 * (a))
+#define IERB_VFAUXR(a)			(0x4004 + 0x40 * (a))
+#define FAUXR_LDID			GENMASK(3, 0)
+
+/* Platform information */
+#define IMX95_ENETC0_BUS_DEVFN		0x0
+#define IMX95_ENETC1_BUS_DEVFN		0x40
+#define IMX95_ENETC2_BUS_DEVFN		0x80
+#define IMX95_LINK_NUM			3
+
+#define IMX94_ENETC3_BUS_DEVFN		0x0
+#define IMX94_TIMER0_BUS_DEVFN		0x1
+#define IMX94_SWITCH_BUS_DEVFN		0x2
+#define IMX94_ENETC0_BUS_DEVFN		0x100
+#define IMX94_TIMER1_BUS_DEVFN		0x101
+#define IMX94_ENETC1_BUS_DEVFN		0x140
+#define IMX94_ENETC2_BUS_DEVFN		0x180
+#define IMX94_TIMER2_BUS_DEVFN		0x181
+#define IMX94_ENETC0_LINK		3
+#define IMX94_ENETC1_LINK		4
+#define IMX94_ENETC2_LINK		5
+#define IMX94_ENETC0_OFFSET		0
+#define IMX94_ENETC1_OFFSET		1
+#define IMX94_ENETC2_OFFSET		2
+#define IMX94_SWITCH_PORT2		2
+#define IMX94_SWITCH_CPU_PORT		3
+#define IMX94_TIMER0_ID			0
+#define IMX94_TIMER1_ID			1
+#define IMX94_TIMER2_ID			2
+
+/* Flags for different platforms */
+#define NETC_HAS_NETCMIX		BIT(0)
+
+struct netc_blk_ctrl {
+	void __iomem *prb;
+	void __iomem *ierb;
+	void __iomem *netcmix;
+	struct clk *ipg_clk;
+
+	const struct netc_devinfo *devinfo;
+	atomic_t wakeonlan_count;
+	struct platform_device *pdev;
+	struct dentry *debugfs_root;
+};
+
+struct netc_devinfo {
+	u32 flags;
+	int num_link; /* Internal links are not included */
+	int (*netcmix_init)(struct platform_device *pdev);
+	int (*ierb_init)(struct platform_device *pdev);
+	void (*xpcs_port_init)(struct netc_blk_ctrl *priv, int port);
+};
+
+static struct netc_blk_ctrl *netc_bc;
+
+static void netc_reg_write(void __iomem *base, u32 offset, u32 val)
+{
+	iowrite32(val, base + offset);
+}
+
+static u32 netc_reg_read(void __iomem *base, u32 offset)
+{
+	return ioread32(base + offset);
+}
+
+static int netc_of_pci_get_bus_devfn(struct device_node *np)
+{
+	u32 reg[5];
+	int error;
+
+	error = of_property_read_u32_array(np, "reg", reg, ARRAY_SIZE(reg));
+	if (error)
+		return error;
+
+	return (reg[0] >> 8) & 0xffff;
+}
+
+static int netc_get_link_mii_protocol(phy_interface_t interface)
+{
+	switch (interface) {
+	case PHY_INTERFACE_MODE_MII:
+		return MII_PROT_MII;
+	case PHY_INTERFACE_MODE_RMII:
+		return MII_PROT_RMII;
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_RGMII_ID:
+	case PHY_INTERFACE_MODE_RGMII_RXID:
+	case PHY_INTERFACE_MODE_RGMII_TXID:
+		return MII_PROT_RGMII;
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_2500BASEX:
+	case PHY_INTERFACE_MODE_10GBASER:
+	case PHY_INTERFACE_MODE_XGMII:
+	case PHY_INTERFACE_MODE_USXGMII:
+		return MII_PROT_SERIAL;
+	default:
+		return -EINVAL;
+	}
+}
+
+static int imx95_netcmix_init(struct platform_device *pdev)
+{
+	struct netc_blk_ctrl *priv = platform_get_drvdata(pdev);
+	struct device_node *np = pdev->dev.of_node;
+	struct device_node *child, *gchild;
+	phy_interface_t interface;
+	int bus_devfn, mii_proto;
+	u32 val;
+	int err;
+
+	/* Default setting */
+	val = MII_PROT(0, MII_PROT_RGMII) | MII_PROT(1, MII_PROT_RGMII) |
+	      MII_PROT(2, MII_PROT_SERIAL);
+
+	/* Update the link MII protocol through parsing phy-mode */
+	for_each_available_child_of_node(np, child) {
+		for_each_available_child_of_node(child, gchild) {
+			if (!of_device_is_compatible(gchild, "fsl,imx95-enetc"))
+				continue;
+
+			bus_devfn = netc_of_pci_get_bus_devfn(gchild);
+			if (bus_devfn < 0) {
+				err = -EINVAL;
+				goto err_out;
+			}
+
+			if (bus_devfn == IMX95_ENETC2_BUS_DEVFN)
+				continue;
+
+			err = of_get_phy_mode(gchild, &interface);
+			if (err)
+				continue;
+
+			mii_proto = netc_get_link_mii_protocol(interface);
+			if (mii_proto < 0) {
+				err = -EINVAL;
+				goto err_out;
+			}
+
+			switch (bus_devfn) {
+			case IMX95_ENETC0_BUS_DEVFN:
+				val = u32_replace_bits(val, mii_proto,
+						       CFG_LINK_MII_PORT_0);
+				break;
+			case IMX95_ENETC1_BUS_DEVFN:
+				val = u32_replace_bits(val, mii_proto,
+						       CFG_LINK_MII_PORT_1);
+				break;
+			default:
+				err = -EINVAL;
+				goto err_out;
+			}
+		}
+	}
+
+	/* Configure Link I/O variant */
+	netc_reg_write(priv->netcmix, IMX95_CFG_LINK_IO_VAR,
+		       IO_VAR(2, IO_VAR_16FF_16G_SERDES));
+	/* Configure Link 2 PCS protocol */
+	netc_reg_write(priv->netcmix, IMX95_CFG_LINK_PCS_PROT(2),
+		       PCS_PROT_10G_SXGMII);
+	netc_reg_write(priv->netcmix, IMX95_CFG_LINK_MII_PROT, val);
+
+	return 0;
+
+err_out:
+	of_node_put(gchild);
+	of_node_put(child);
+
+	return err;
+}
+
+static int imx94_enetc_get_link_num(struct device_node *np)
+{
+	int bus_devfn;
+
+	bus_devfn = netc_of_pci_get_bus_devfn(np);
+	if (bus_devfn < 0)
+		return -EINVAL;
+
+	/* Parse ENETC link number */
+	switch (bus_devfn) {
+	case IMX94_ENETC0_BUS_DEVFN:
+		return IMX94_ENETC0_LINK;
+	case IMX94_ENETC1_BUS_DEVFN:
+		return IMX94_ENETC1_LINK;
+	case IMX94_ENETC2_BUS_DEVFN:
+		return IMX94_ENETC2_LINK;
+	default:
+		return -EINVAL;
+	}
+}
+
+static int imx94_link_config(struct netc_blk_ctrl *priv,
+			     struct device_node *np, int link_id)
+{
+	phy_interface_t interface;
+	int mii_proto, err;
+	u32 val;
+
+	err = of_get_phy_mode(np, &interface);
+	if (err)
+		return err;
+
+	mii_proto = netc_get_link_mii_protocol(interface);
+	if (mii_proto < 0)
+		return -EINVAL;
+
+	val = mii_proto & NETC_LINK_CFG_MII_PROT;
+	if (mii_proto == MII_PROT_SERIAL) {
+		int pcs_proto = PCS_PROT_1G_SGMII;
+
+		if (pcs_proto == PHY_INTERFACE_MODE_2500BASEX)
+			pcs_proto = PCS_PROT_2500M_SGMII;
+
+		netc_reg_write(priv->netcmix, IMX94_CFG_LINK_PCS_PROT(link_id),
+			       pcs_proto);
+		val = u32_replace_bits(val, IO_VAR_16FF_16G_SERDES,
+				       NETC_LINK_CFG_IO_VAR);
+	}
+
+	netc_reg_write(priv->netcmix, IMX94_NETC_LINK_CFG(link_id), val);
+
+	if (link_id == IMX94_ENETC0_LINK) {
+		val = netc_reg_read(priv->netcmix, IMX94_EXT_PIN_CONTROL);
+		val |= MAC2_MAC3_SEL;
+		netc_reg_write(priv->netcmix, IMX94_EXT_PIN_CONTROL, val);
+	}
+
+	return 0;
+}
+
+static int imx94_enetc_link_config(struct netc_blk_ctrl *priv,
+				   struct device_node *np, bool *enetc0_en)
+{
+	int link_id;
+
+	link_id = imx94_enetc_get_link_num(np);
+	if (link_id < 0)
+		return -EINVAL;
+
+	if (link_id == IMX94_ENETC0_LINK)
+		*enetc0_en = true;
+
+	return imx94_link_config(priv, np, link_id);
+}
+
+static int imx94_switch_link_config(struct netc_blk_ctrl *priv,
+				    struct device_node *np, bool *swp2_en)
+{
+	struct device_node *ports;
+	int port_id, err = 0;
+
+	ports = of_get_child_by_name(np, "ports");
+	if (!ports)
+		ports = of_get_child_by_name(np, "ethernet-ports");
+	if (!ports)
+		return -ENODEV;
+
+	for_each_available_child_of_node_scoped(ports, child) {
+		if (of_property_read_u32(child, "reg", &port_id) < 0) {
+			err = -ENODEV;
+			goto end;
+		}
+
+		if (port_id == IMX94_SWITCH_CPU_PORT)
+			continue;
+
+		if (port_id == IMX94_SWITCH_PORT2)
+			*swp2_en = true;
+
+		err = imx94_link_config(priv, child, port_id);
+		if (err)
+			goto end;
+	}
+
+end:
+	of_node_put(ports);
+
+	return err;
+}
+
+static int imx94_netcmix_init(struct platform_device *pdev)
+{
+	struct netc_blk_ctrl *priv = platform_get_drvdata(pdev);
+	struct device_node *np = pdev->dev.of_node;
+	bool enetc0_en = false, swp2_en = false;
+	int err;
+
+	for_each_available_child_of_node_scoped(np, child) {
+		for_each_available_child_of_node_scoped(child, gchild) {
+			if (of_device_is_compatible(gchild, "pci1131,e101")) {
+				err = imx94_enetc_link_config(priv, gchild, &enetc0_en);
+				if (err)
+					return err;
+			} else if (of_device_is_compatible(gchild, "pci1131,eef2")) {
+				err = imx94_switch_link_config(priv, gchild, &swp2_en);
+				if (err)
+					return err;
+			}
+		}
+	}
+
+	if (enetc0_en && swp2_en) {
+		dev_err(&pdev->dev, "Cannot enable swp2 and enetc0 at the same time\n");
+
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static bool netc_ierb_is_locked(struct netc_blk_ctrl *priv)
+{
+	return !!(netc_reg_read(priv->prb, PRB_NETCRR) & NETCRR_LOCK);
+}
+
+static int netc_lock_ierb(struct netc_blk_ctrl *priv)
+{
+	u32 val;
+
+	netc_reg_write(priv->prb, PRB_NETCRR, NETCRR_LOCK);
+
+	return read_poll_timeout(netc_reg_read, val, !(val & NETCSR_STATE),
+				 100, 2000, false, priv->prb, PRB_NETCSR);
+}
+
+static int netc_unlock_ierb_with_warm_reset(struct netc_blk_ctrl *priv)
+{
+	u32 val;
+
+	netc_reg_write(priv->prb, PRB_NETCRR, 0);
+
+	return read_poll_timeout(netc_reg_read, val, !(val & NETCRR_LOCK),
+				 1000, 100000, true, priv->prb, PRB_NETCRR);
+}
+
+static int imx95_ierb_init(struct platform_device *pdev)
+{
+	struct netc_blk_ctrl *priv = platform_get_drvdata(pdev);
+
+	/* EMDIO : No MSI-X intterupt */
+	netc_reg_write(priv->ierb, IERB_EMDIOFAUXR, 0);
+	/* ENETC0 PF */
+	netc_reg_write(priv->ierb, IERB_EFAUXR(0), 0);
+	/* ENETC0 VF0 */
+	netc_reg_write(priv->ierb, IERB_VFAUXR(0), 1);
+	/* ENETC0 VF1 */
+	netc_reg_write(priv->ierb, IERB_VFAUXR(1), 2);
+	/* ENETC1 PF */
+	netc_reg_write(priv->ierb, IERB_EFAUXR(1), 3);
+	/* ENETC1 VF0 : Disabled on 19x19 board dts */
+	netc_reg_write(priv->ierb, IERB_VFAUXR(2), 5);
+	/* ENETC1 VF1 : Disabled on 19x19 board dts */
+	netc_reg_write(priv->ierb, IERB_VFAUXR(3), 6);
+	/* ENETC2 PF */
+	netc_reg_write(priv->ierb, IERB_EFAUXR(2), 4);
+	/* ENETC2 VF0 : Disabled on 15x15 board dts */
+	netc_reg_write(priv->ierb, IERB_VFAUXR(4), 5);
+	/* ENETC2 VF1 : Disabled on 15x15 board dts */
+	netc_reg_write(priv->ierb, IERB_VFAUXR(5), 6);
+	/* NETC TIMER */
+	netc_reg_write(priv->ierb, IERB_T0FAUXR, 7);
+
+	return 0;
+}
+
+static int imx94_enetc_get_enetc_offset(struct device_node *np)
+{
+	int bus_devfn;
+
+	bus_devfn = netc_of_pci_get_bus_devfn(np);
+	if (bus_devfn < 0)
+		return -EINVAL;
+
+	/* Parse ENETC offset */
+	switch (bus_devfn) {
+	case IMX94_ENETC0_BUS_DEVFN:
+		return IMX94_ENETC0_OFFSET;
+	case IMX94_ENETC1_BUS_DEVFN:
+		return IMX94_ENETC1_OFFSET;
+	case IMX94_ENETC2_BUS_DEVFN:
+		return IMX94_ENETC2_OFFSET;
+	default:
+		return -EINVAL;
+	}
+}
+
+static int imx94_enetc_get_timer_id(struct device_node *np)
+{
+	int bus_devfn;
+
+	bus_devfn = netc_of_pci_get_bus_devfn(np);
+	if (bus_devfn < 0)
+		return -EINVAL;
+
+	/* Parse ENETC PTP timer ID */
+	switch (bus_devfn) {
+	case IMX94_TIMER0_BUS_DEVFN:
+		return IMX94_TIMER0_ID;
+	case IMX94_TIMER1_BUS_DEVFN:
+		return IMX94_TIMER1_ID;
+	case IMX94_TIMER2_BUS_DEVFN:
+		return IMX94_TIMER2_ID;
+	default:
+		return -EINVAL;
+	}
+}
+
+static int imx94_enetc_update_tid(struct netc_blk_ctrl *priv, struct device_node *pf_np)
+{
+	struct device_node *timer_np;
+	int offset, tid;
+
+	offset = imx94_enetc_get_enetc_offset(pf_np);
+	if (offset < 0) {
+		dev_err(&priv->pdev->dev, "Find unknown PF node.\n");
+		return offset;
+	}
+
+	timer_np = of_parse_phandle(pf_np, "nxp,ptp-timer", 0);
+	if (!timer_np) {
+		/*
+		 * If nxp,ptp-timer is not set, the first timer of the bus
+		 * where enetc is located will be used as the default timer.
+		 */
+		tid = IMX94_TIMER1_ID;
+		goto update_reg;
+	}
+
+	tid = imx94_enetc_get_timer_id(timer_np);
+	of_node_put(timer_np);
+	if (tid < 0) {
+		dev_err(&priv->pdev->dev, "Incorrect bus/devfn of ptp-timer.\n");
+		return tid;
+	}
+
+update_reg:
+	netc_reg_write(priv->ierb, IERB_ETBCR(offset), tid);
+
+	return 0;
+}
+
+static int imx94_ierb_init(struct platform_device *pdev)
+{
+	struct netc_blk_ctrl *priv = platform_get_drvdata(pdev);
+	struct device_node *blk_np = pdev->dev.of_node;
+	int ret = 0;
+
+	if (!blk_np)
+		return -ENODEV;
+
+	for_each_available_child_of_node_scoped(blk_np, bus_np)
+		for_each_available_child_of_node_scoped(bus_np, pf_np)
+			if (of_device_is_compatible(pf_np, "pci1131,e101"))
+				ret = imx94_enetc_update_tid(priv, pf_np);
+
+	return ret;
+}
+
+static int netc_ierb_init(struct platform_device *pdev)
+{
+	struct netc_blk_ctrl *priv = platform_get_drvdata(pdev);
+	const struct netc_devinfo *devinfo = priv->devinfo;
+	int err;
+
+	if (netc_ierb_is_locked(priv)) {
+		err = netc_unlock_ierb_with_warm_reset(priv);
+		if (err) {
+			dev_err(&pdev->dev, "Unlock IERB failed.\n");
+			return err;
+		}
+	}
+
+	if (devinfo->ierb_init) {
+		err = devinfo->ierb_init(pdev);
+		if (err)
+			return err;
+	}
+
+	err = netc_lock_ierb(priv);
+	if (err) {
+		dev_err(&pdev->dev, "Lock IERB failed.\n");
+		return err;
+	}
+
+	atomic_set(&priv->wakeonlan_count, 0);
+
+	return 0;
+}
+
+static void imx94_ec_netc_xpcs_port_init(struct netc_blk_ctrl *priv, int port)
+{
+	u32 val;
+
+	val = netc_reg_read(priv->netcmix, IMX94_MISC_SOC_CONTROL);
+	if (port == IMX94_XPCS_PORT_1)
+		val |= SEL_XPCS_1;
+	else
+		val &= ~SEL_XPCS_1;
+	netc_reg_write(priv->netcmix, IMX94_MISC_SOC_CONTROL, val);
+}
+
+void ec_netc_xpcs_port_init(int port)
+{
+	struct netc_blk_ctrl *priv = netc_bc;
+	const struct netc_devinfo *devinfo;
+
+	if (!priv)
+		return;
+	devinfo = priv->devinfo;
+
+	if (devinfo->xpcs_port_init)
+		devinfo->xpcs_port_init(priv, port);
+}
+//EXPORT_SYMBOL_GPL(ec_netc_xpcs_port_init);
+
+void ec_netc_ierb_enable_wakeonlan(void)
+{
+	struct netc_blk_ctrl *priv = netc_bc;
+
+	if (!priv)
+		return;
+
+	atomic_inc(&priv->wakeonlan_count);
+}
+//EXPORT_SYMBOL_GPL(ec_netc_ierb_enable_wakeonlan);
+
+void ec_netc_ierb_disable_wakeonlan(void)
+{
+	struct netc_blk_ctrl *priv = netc_bc;
+
+	if (!priv)
+		return;
+
+	atomic_dec(&priv->wakeonlan_count);
+	if (atomic_read(&priv->wakeonlan_count) < 0) {
+		atomic_set(&priv->wakeonlan_count, 0);
+		dev_warn(&priv->pdev->dev, "Wake-on-LAN count underflow.\n");
+	}
+}
+//EXPORT_SYMBOL_GPL(ec_netc_ierb_disable_wakeonlan);
+
+int ec_netc_ierb_may_wakeonlan(void)
+{
+	struct netc_blk_ctrl *priv = netc_bc;
+
+	if (!priv)
+		return -ENXIO;
+
+	return atomic_read(&priv->wakeonlan_count);
+}
+
+#if IS_ENABLED(CONFIG_DEBUG_FS)
+static int netc_prb_show(struct seq_file *s, void *data)
+{
+	struct netc_blk_ctrl *priv = s->private;
+	u32 val;
+
+	val = netc_reg_read(priv->prb, PRB_NETCRR);
+	seq_printf(s, "[PRB NETCRR] Lock:%d SR:%d\n",
+		   (val & NETCRR_LOCK) ? 1 : 0,
+		   (val & NETCRR_SR) ? 1 : 0);
+
+	val = netc_reg_read(priv->prb, PRB_NETCSR);
+	seq_printf(s, "[PRB NETCSR] State:%d Error:%d\n",
+		   (val & NETCSR_STATE) ? 1 : 0,
+		   (val & NETCSR_ERROR) ? 1 : 0);
+
+	return 0;
+}
+DEFINE_SHOW_ATTRIBUTE(netc_prb);
+
+static void netc_blk_ctrl_create_debugfs(struct netc_blk_ctrl *priv)
+{
+	struct dentry *root;
+
+	root = debugfs_create_dir("netc_blk_ctrl", NULL);
+	if (IS_ERR(root))
+		return;
+
+	priv->debugfs_root = root;
+
+	debugfs_create_file("prb", 0444, root, priv, &netc_prb_fops);
+}
+
+static void netc_blk_ctrl_remove_debugfs(struct netc_blk_ctrl *priv)
+{
+	debugfs_remove_recursive(priv->debugfs_root);
+	priv->debugfs_root = NULL;
+}
+
+#else
+
+static void netc_blk_ctrl_create_debugfs(struct netc_blk_ctrl *priv)
+{
+}
+
+static void netc_blk_ctrl_remove_debugfs(struct netc_blk_ctrl *priv)
+{
+}
+#endif
+
+static int netc_prb_check_error(struct netc_blk_ctrl *priv)
+{
+	u32 val;
+
+	val = netc_reg_read(priv->prb, PRB_NETCSR);
+	if (val & NETCSR_ERROR)
+		return -1;
+
+	return 0;
+}
+
+static const struct netc_devinfo imx95_devinfo = {
+	.flags = NETC_HAS_NETCMIX,
+	.num_link = IMX95_LINK_NUM,
+	.netcmix_init = imx95_netcmix_init,
+	.ierb_init = imx95_ierb_init,
+};
+
+static const struct netc_devinfo imx94_devinfo = {
+	.flags = NETC_HAS_NETCMIX,
+	.netcmix_init = imx94_netcmix_init,
+	.ierb_init = imx94_ierb_init,
+	.xpcs_port_init = imx94_ec_netc_xpcs_port_init,
+};
+
+static const struct of_device_id netc_blk_ctrl_match[] = {
+	{ .compatible = "nxp,imx95-netc-blk-ctrl", .data = &imx95_devinfo },
+	{ .compatible = "nxp,imx94-netc-blk-ctrl", .data = &imx94_devinfo },
+	{},
+};
+MODULE_DEVICE_TABLE(of, netc_blk_ctrl_match);
+
+static int netc_blk_ctrl_probe(struct platform_device *pdev)
+{
+	struct device_node *node = pdev->dev.of_node;
+	const struct netc_devinfo *devinfo;
+	struct device *dev = &pdev->dev;
+	const struct of_device_id *id;
+	struct netc_blk_ctrl *priv;
+	void __iomem *regs;
+	int err;
+
+	priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->pdev = pdev;
+	priv->ipg_clk = devm_clk_get_optional(dev, "ipg_clk");
+	if (IS_ERR(priv->ipg_clk)) {
+		dev_err(dev, "Get ipg_clk failed\n");
+		err = PTR_ERR(priv->ipg_clk);
+		return err;
+	}
+
+	err = clk_prepare_enable(priv->ipg_clk);
+	if (err) {
+		dev_err(dev, "Enable ipg_clk failed\n");
+		goto disable_ipg_clk;
+	}
+
+	id = of_match_device(netc_blk_ctrl_match, dev);
+	if (!id) {
+		dev_err(dev, "Cannot match device\n");
+		err = -EINVAL;
+		goto disable_ipg_clk;
+	}
+
+	devinfo = (struct netc_devinfo *)id->data;
+	if (!devinfo) {
+		dev_err(dev, "No device information\n");
+		err = -EINVAL;
+		goto disable_ipg_clk;
+	}
+	priv->devinfo = devinfo;
+
+	regs = devm_platform_ioremap_resource_byname(pdev, "ierb");
+	if (IS_ERR(regs)) {
+		err = PTR_ERR(regs);
+		dev_err(dev, "Missing IERB resource\n");
+		goto disable_ipg_clk;
+	}
+	priv->ierb = regs;
+
+	regs = devm_platform_ioremap_resource_byname(pdev, "prb");
+	if (IS_ERR(regs)) {
+		err = PTR_ERR(regs);
+		dev_err(dev, "Missing PRB resource\n");
+		goto disable_ipg_clk;
+	}
+	priv->prb = regs;
+
+	if (devinfo->flags & NETC_HAS_NETCMIX) {
+		regs = devm_platform_ioremap_resource_byname(pdev, "netcmix");
+		if (IS_ERR(regs)) {
+			err = PTR_ERR(regs);
+			dev_err(dev, "Missing NETCMIX resource\n");
+			goto disable_ipg_clk;
+		}
+		priv->netcmix = regs;
+	}
+
+	platform_set_drvdata(pdev, priv);
+
+	if (devinfo->netcmix_init) {
+		err = devinfo->netcmix_init(pdev);
+		if (err) {
+			dev_err(dev, "Initializing NETCMIX failed\n");
+			goto disable_ipg_clk;
+		}
+	}
+
+	err = netc_ierb_init(pdev);
+	if (err) {
+		dev_err(dev, "Initializing IERB failed.\n");
+		goto disable_ipg_clk;
+	}
+
+	if (netc_prb_check_error(priv) < 0)
+		dev_warn(dev, "The current IERB configuration is invalid.\n");
+
+	netc_bc = priv;
+	netc_blk_ctrl_create_debugfs(priv);
+
+	err = of_platform_populate(node, NULL, NULL, dev);
+	if (err) {
+		dev_err(dev, "of_platform_populate failed\n");
+		goto remove_debugfs;
+	}
+
+	return 0;
+
+remove_debugfs:
+	netc_blk_ctrl_remove_debugfs(priv);
+	netc_bc = NULL;
+disable_ipg_clk:
+	clk_disable_unprepare(priv->ipg_clk);
+
+	return err;
+}
+
+static void netc_blk_ctrl_remove(struct platform_device *pdev)
+{
+	struct netc_blk_ctrl *priv = platform_get_drvdata(pdev);
+
+	of_platform_depopulate(&pdev->dev);
+	netc_blk_ctrl_remove_debugfs(priv);
+	netc_bc = NULL;
+	clk_disable_unprepare(priv->ipg_clk);
+}
+
+static int netc_blk_ctrl_suspend_noirq(struct device *dev)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	struct netc_blk_ctrl *priv = platform_get_drvdata(pdev);
+
+	if (ec_netc_ierb_may_wakeonlan())
+		return 0;
+
+	clk_disable_unprepare(priv->ipg_clk);
+
+	return 0;
+}
+
+static int netc_blk_ctrl_resume_noirq(struct device *dev)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	struct netc_blk_ctrl *priv = platform_get_drvdata(pdev);
+	const struct netc_devinfo *devinfo = priv->devinfo;
+	int err;
+
+	if (ec_netc_ierb_may_wakeonlan())
+		return 0;
+
+	err = clk_prepare_enable(priv->ipg_clk);
+	if (err) {
+		dev_err(dev, "Enable ipg_clk failed\n");
+		return err;
+	}
+
+	if (devinfo->netcmix_init) {
+		err = devinfo->netcmix_init(pdev);
+		if (err) {
+			dev_err(dev, "Initializing NETCMIX failed\n");
+			goto disable_ipg_clk;
+		}
+	}
+
+	err = netc_ierb_init(pdev);
+	if (err) {
+		dev_err(&pdev->dev, "Initializing IERB failed.\n");
+		goto disable_ipg_clk;
+	}
+
+	if (netc_prb_check_error(priv) < 0)
+		dev_warn(&pdev->dev,
+			 "The current IERB configuration is invalid.\n");
+
+	return 0;
+
+disable_ipg_clk:
+	clk_disable_unprepare(priv->ipg_clk);
+
+	return err;
+}
+
+static const struct dev_pm_ops __maybe_unused netc_blk_ctrl_pm_ops = {
+	SET_NOIRQ_SYSTEM_SLEEP_PM_OPS(netc_blk_ctrl_suspend_noirq,
+				      netc_blk_ctrl_resume_noirq)
+};
+
+static struct platform_driver netc_blk_ctrl_driver = {
+	.driver = {
+		.name = "nxp-netc-blk-ctrl",
+		.of_match_table = netc_blk_ctrl_match,
+		.pm = pm_ptr(&netc_blk_ctrl_pm_ops),
+	},
+	.probe = netc_blk_ctrl_probe,
+	.remove = netc_blk_ctrl_remove,
+};
+
+module_platform_driver(netc_blk_ctrl_driver);
+
+MODULE_DESCRIPTION("NXP NETC Blocks Control Driver");
+MODULE_LICENSE("Dual BSD/GPL");
diff --git a/devices/enetc/netc_debugfs_lib.c b/devices/enetc/netc_debugfs_lib.c
new file mode 100755
index 0000000..0361461
--- /dev/null
+++ b/devices/enetc/netc_debugfs_lib.c
@@ -0,0 +1,616 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+/*
+ * NETC NTMP (NETC Table Management Protocol) 2.0 driver
+ * Copyright 2024 NXP
+ */
+#include <linux/fsl/netc_lib.h>
+
+#include "ntmp_private.h"
+
+int netc_kstrtouint(const char __user *buffer, size_t count,
+		    loff_t *ppos, u32 *val)
+{
+	char cmd_buffer[256];
+	int len, err;
+
+	if (*ppos != 0 || !count)
+		return -EINVAL;
+
+	if (count >= sizeof(cmd_buffer))
+		return -ENOSPC;
+
+	len = simple_write_to_buffer(cmd_buffer, sizeof(cmd_buffer) - 1,
+				     ppos, buffer, count);
+	if (len < 0)
+		return len;
+
+	cmd_buffer[len] = '\0';
+	err = kstrtouint(cmd_buffer, 16, val);
+	if (err)
+		return err;
+
+	return len;
+}
+EXPORT_SYMBOL_GPL(netc_kstrtouint);
+
+void netc_show_psfp_flower(struct seq_file *s, struct netc_flower_rule *rule)
+{
+	struct ntmp_isit_entry *isit_entry = rule->key_tbl->isit_entry;
+	struct ntmp_ist_entry *ist_entry = rule->key_tbl->ist_entry;
+	struct ntmp_isft_entry *isft_entry = rule->isft_entry;
+	u32 rpt_eid, isct_eid;
+
+	seq_printf(s, "ISIT entry ID:0x%x\n", isit_entry->entry_id);
+	seq_printf(s, "IST entry ID: 0x%x\n", ist_entry->entry_id);
+
+	if (isft_entry) {
+		rpt_eid = le32_to_cpu(isft_entry->cfge.rp_eid);
+		isct_eid = le32_to_cpu(isft_entry->cfge.isc_eid);
+		seq_printf(s, "ISFT entry ID: 0x%x\n", isft_entry->entry_id);
+	} else {
+		rpt_eid = le32_to_cpu(ist_entry->cfge.rp_eid);
+		isct_eid = le32_to_cpu(ist_entry->cfge.isc_eid);
+	}
+
+	seq_printf(s, "RPT entry ID: 0x%x\n", rpt_eid);
+	seq_printf(s, "ISCT entry ID: 0x%x\n", isct_eid);
+
+	if (rule->gate_tbl) {
+		seq_printf(s, "SGIT entry ID: 0x%x\n", rule->gate_tbl->sgit_eid);
+		seq_printf(s, "SGCLT entry ID: 0x%x\n", rule->gate_tbl->sgclt_eid);
+	}
+}
+EXPORT_SYMBOL_GPL(netc_show_psfp_flower);
+
+int netc_show_isit_entry(struct ntmp_priv *priv, struct seq_file *s,
+			 u32 entry_id)
+{
+	struct ntmp_isit_entry *isit_entry __free(kfree);
+	struct isit_keye_data *keye;
+	u32 key_aux;
+	int i, err;
+
+	isit_entry = kzalloc(sizeof(*isit_entry), GFP_KERNEL);
+	if (!isit_entry)
+		return -ENOMEM;
+
+	err = ntmp_isit_query_entry(&priv->cbdrs, entry_id, isit_entry);
+	if (err) {
+		seq_printf(s, "Query ISIT entry ID (0x%x) failed\n", entry_id);
+		return err;
+	}
+
+	keye = &isit_entry->keye;
+	key_aux = le32_to_cpu(keye->key_aux);
+	seq_printf(s, "Show ingress stream identification table entry 0x%x\n",
+		   entry_id);
+	seq_printf(s, "Key type: %lu, Source Port ID: %lu, IS_EID: %u\n",
+		   FIELD_GET(ISIT_KEY_TYPE, key_aux),
+		   FIELD_GET(ISIT_SRC_PORT_ID, key_aux),
+		   le32_to_cpu(isit_entry->is_eid));
+	seq_puts(s, "Keys: ");
+	for (i = 0; i < ISIT_FRAME_KEY_LEN; i++)
+		seq_printf(s, "%02x", keye->frame_key[i]);
+	seq_puts(s, "\n\n");
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(netc_show_isit_entry);
+
+int netc_show_ist_entry(struct ntmp_priv *priv, struct seq_file *s,
+			u32 entry_id)
+{
+	struct ist_cfge_data *cfge __free(kfree);
+	u32 bitmap_evmeid, cfg;
+	u16 switch_cfg;
+	int err;
+
+	cfge = kzalloc(sizeof(*cfge), GFP_KERNEL);
+	if (!cfge)
+		return -ENOMEM;
+
+	err = ntmp_ist_query_entry(&priv->cbdrs, entry_id, cfge);
+	if (err) {
+		seq_printf(s, "Query IST entry ID (0x%x) failed\n", entry_id);
+		return err;
+	}
+
+	switch_cfg = le16_to_cpu(cfge->switch_cfg);
+	bitmap_evmeid = le32_to_cpu(cfge->bitmap_evmeid);
+	cfg = le32_to_cpu(cfge->cfg);
+	seq_printf(s, "Show ingress stream table entry 0x%x\n", entry_id);
+	seq_printf(s, "Stream Filtering: %s, Report Receive Timestamp: %s\n",
+		   is_en(cfg & IST_SFE), is_en(cfg & IST_RRT));
+	seq_printf(s, "OIPV: %s, IPV: %lu, ODR: %s, DR: %lu\n",
+		   is_en(cfg & IST_OIPV), FIELD_GET(IST_IPV, cfg),
+		   is_en(cfg & IST_ODR), FIELD_GET(IST_DR, cfg));
+	seq_printf(s, "IMIRE: %s, TIMECAPE: %s, SPPD: %s, ISQGA: %lu\n",
+		   is_en(cfg & IST_IMIRE), is_en(cfg & IST_TIMERCAPE),
+		   is_en(cfg & IST_SPPD), FIELD_GET(IST_ISQGA, cfg));
+	seq_printf(s, "ORP: %s, OSGI: %s, Host Reason:%lu\n",
+		   is_en(cfg & IST_ORP), is_en(cfg & IST_OSGI),
+		   FIELD_GET(IST_HR, cfg));
+
+	switch (priv->cbdrs.tbl.ist_ver) {
+	case NTMP_TBL_VER0:
+		seq_printf(s, "Forwarding Action: %lu, SDU type:%lu\n",
+			   FIELD_GET(IST_V0_FA, cfg),
+			   FIELD_GET(IST_V0_SDU_TYPE, cfg));
+		break;
+	case NTMP_TBL_VER1:
+		seq_printf(s, "Forwarding Action: %lu, SDU type:%lu\n",
+			   FIELD_GET(IST_V1_FA, cfg),
+			   FIELD_GET(IST_V1_SDU_TYPE, cfg));
+		seq_printf(s, "SDFA: %lu, OSDFA: %s\n",
+			   FIELD_GET(IST_SDFA, cfg), is_en(cfg & IST_OSDFA));
+		break;
+	default:
+		break;
+	}
+
+	seq_printf(s, "MSDU :%u\n", le16_to_cpu(cfge->msdu));
+	seq_printf(s, "IFME_LEN_CHANGE: 0x%lx, Egress Port: %lu\n",
+		   FIELD_GET(IST_IFME_LEN_CHANGE, switch_cfg),
+		   FIELD_GET(IST_EPORT, switch_cfg));
+	seq_printf(s, "Override ET_EID: %lu, CTD: %lu\n",
+		   FIELD_GET(IST_OETEID, switch_cfg),
+		   FIELD_GET(IST_CTD, switch_cfg));
+	seq_printf(s, "ISQG_EID: 0x%x, RP_EID: 0x%x\n", le32_to_cpu(cfge->isqg_eid),
+		   le32_to_cpu(cfge->rp_eid));
+	seq_printf(s, "SGI_EID: 0x%x, IFM_EID: 0x%x\n", le32_to_cpu(cfge->sgi_eid),
+		   le32_to_cpu(cfge->ifm_eid));
+	seq_printf(s, "ET_EID: 0x%x, ISC_EID: 0x%x\n", le32_to_cpu(cfge->et_eid),
+		   le32_to_cpu(cfge->isc_eid));
+	seq_printf(s, "Egress Port bitmap: 0x%lx, Event Monitor Event ID: %lu\n",
+		   bitmap_evmeid & IST_EGRESS_PORT_BITMAP,
+		   FIELD_GET(IST_EVMEID, bitmap_evmeid));
+	seq_puts(s, "\n");
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(netc_show_ist_entry);
+
+int netc_show_isft_entry(struct ntmp_priv *priv, struct seq_file *s,
+			 u32 entry_id)
+{
+	struct ntmp_isft_entry *isft_entry __free(kfree);
+	struct isft_cfge_data *cfge;
+	struct isft_keye_data *keye;
+	u16 cfg;
+	int err;
+
+	isft_entry = kzalloc(sizeof(*isft_entry), GFP_KERNEL);
+	if (!isft_entry)
+		return -ENOMEM;
+
+	keye = &isft_entry->keye;
+	cfge = &isft_entry->cfge;
+	err = ntmp_isft_query_entry(&priv->cbdrs, entry_id, isft_entry);
+	if (err) {
+		seq_printf(s, "Query ISFT entry ID (0x%x) failed\n", entry_id);
+		return err;
+	}
+
+	cfg = le16_to_cpu(cfge->cfg);
+	seq_printf(s, "Show ingress stream filter table entry 0x%x\n", entry_id);
+	seq_printf(s, "IS_EID: 0x%x, PCP: %u\n",
+		   le32_to_cpu(keye->is_eid), keye->pcp);
+	seq_printf(s, "OIPV: %s, IPV: %lu, ODR: %s, DR: %lu\n",
+		   is_en(cfg & ISFT_OIPV), FIELD_GET(ISFT_IPV, cfg),
+		   is_en(cfg & ISFT_ODR), FIELD_GET(ISFT_DR, cfg));
+	seq_printf(s, "IMIRE: %s, TIMECAPE:%s, OSGI: %s, CTD: %s\n",
+		   is_en(cfg & ISFT_IMIRE), is_en(cfg & ISFT_TIMECAPE),
+		   is_en(cfg & ISFT_OSGI), is_yes(cfg & ISFT_CTD));
+	seq_printf(s, "ORP: %s, SDU type: %lu, MSDU: %u\n",
+		   is_en(cfg & ISFT_ORP), FIELD_GET(ISFT_SDU_TYPE, cfg),
+		   le16_to_cpu(cfge->msdu));
+	seq_printf(s, "RP_EID: 0x%x, SGI_EID: 0x%x, ISC_EID: 0x%x\n",
+		   le32_to_cpu(cfge->rp_eid), le32_to_cpu(cfge->sgi_eid),
+		   le32_to_cpu(cfge->isc_eid));
+	seq_puts(s, "\n");
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(netc_show_isft_entry);
+
+int netc_show_sgit_entry(struct ntmp_priv *priv, struct seq_file *s,
+			 u32 entry_id)
+{
+	struct ntmp_sgit_entry *sgit_entry __free(kfree);
+	struct sgit_acfge_data *acfge;
+	struct sgit_icfge_data *icfge;
+	struct sgit_sgise_data *sgise;
+	struct sgit_cfge_data *cfge;
+	int err;
+
+	sgit_entry = kzalloc(sizeof(*sgit_entry), GFP_KERNEL);
+	if (!sgit_entry)
+		return -ENOMEM;
+
+	err = ntmp_sgit_query_entry(&priv->cbdrs, entry_id, sgit_entry);
+	if (err) {
+		seq_printf(s, "Query SGIT entry ID (0x%x) failed\n", entry_id);
+		return err;
+	}
+
+	acfge = &sgit_entry->acfge;
+	icfge = &sgit_entry->icfge;
+	sgise = &sgit_entry->sgise;
+	cfge = &sgit_entry->cfge;
+	seq_printf(s, "Show stream gate instance table entry 0x%x\n", entry_id);
+	seq_printf(s, "OPER_SGCL_EID: 0x%x, CONFIG_CHANGE_TIME: %llu\n",
+		   le32_to_cpu(sgise->oper_sgcl_eid),
+		   le64_to_cpu(sgise->config_change_time));
+	seq_printf(s, "OPER_BASE_TIME: %llu, OPER_CYCLE_TIME_EXT: %u\n",
+		   le64_to_cpu(sgise->oper_base_time),
+		   le32_to_cpu(sgise->oper_cycle_time_ext));
+	seq_printf(s, "OEX: %lu, IRX: %lu, state: %lu\n", sgise->info & SGIT_OEX,
+		   FIELD_GET(SGIT_IRX, sgise->info),
+		   FIELD_GET(SGIT_STATE, sgise->info));
+	seq_printf(s, "OEXEN: %s, IRXEN: %s, SDU type:%lu\n",
+		   is_en(cfge->cfg & SGIT_OEXEN),
+		   is_en(cfge->cfg & SGIT_IRXEN),
+		   FIELD_GET(SGIT_SDU_TYPE, cfge->cfg));
+	seq_printf(s, "OIPV: %s, IPV: %lu, GST: %s, CTD: %s\n",
+		   is_en(icfge->icfg & SGIT_OIPV),
+		   FIELD_GET(SGIT_IPV, icfge->icfg),
+		   icfge->icfg & SGIT_GST ? "open" : "closed",
+		   is_yes(icfge->icfg & SGIT_CTD));
+	seq_printf(s, "ADMIN_SGCL_EID: 0x%x, ADMIN_BASE_TIME: %llu\n",
+		   le32_to_cpu(acfge->admin_sgcl_eid),
+		   le64_to_cpu(acfge->admin_base_time));
+	seq_printf(s, "ADMIN_CYCLE_TIME_EXT: %u\n",
+		   le32_to_cpu(acfge->admin_cycle_time_ext));
+	seq_puts(s, "\n");
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(netc_show_sgit_entry);
+
+int netc_show_sgclt_entry(struct ntmp_priv *priv, struct seq_file *s,
+			  u32 entry_id)
+{
+	struct ntmp_sgclt_entry *sgclt_entry __free(kfree);
+	u32 sgclt_data_size, sgclt_cfge_size;
+	struct sgclt_cfge_data *cfge;
+	int i, err;
+
+	sgclt_cfge_size = struct_size_t(struct sgclt_cfge_data, ge,
+					SGCLT_MAX_GE_NUM);
+	sgclt_data_size = struct_size(sgclt_entry, cfge.ge,
+				      SGCLT_MAX_GE_NUM);
+	sgclt_entry = kzalloc(sgclt_data_size, GFP_KERNEL);
+	if (!sgclt_entry)
+		return -ENOMEM;
+
+	err = ntmp_sgclt_query_entry(&priv->cbdrs, entry_id, sgclt_entry,
+				     sgclt_cfge_size);
+	if (err) {
+		seq_printf(s, "Query SGCLT entry ID (0x%x) failed\n", entry_id);
+		return err;
+	}
+
+	cfge = &sgclt_entry->cfge;
+	seq_printf(s, "Show stream gate control list table entry 0x%x\n", entry_id);
+	seq_printf(s, "REF_COUNT: %u, CYCLE_TIME: %u, LIST_LENGTH: %u\n",
+		   sgclt_entry->ref_count, le32_to_cpu(cfge->cycle_time),
+		   cfge->list_length);
+	seq_printf(s, "EXT_OIPV: %s, EXT_IPV: %lu, EXT_CTD: %s, EXT_GTST: %s\n",
+		   is_en(cfge->ext_cfg & SGCLT_EXT_OIPV),
+		   FIELD_GET(SGCLT_EXT_IPV, cfge->ext_cfg),
+		   is_yes(cfge->ext_cfg & SGCLT_EXT_CTD),
+		   cfge->ext_cfg & SGCLT_EXT_GTST ? "open" : "closed");
+
+	for (i = 0; i < cfge->list_length + 1; i++) {
+		u32 cfg = le32_to_cpu(cfge->ge[i].cfg);
+
+		seq_printf(s, "Gate Entry: %d, Time Interval: %u\n",
+			   i, le32_to_cpu(cfge->ge[i].interval));
+		seq_printf(s, "IOMEN: %s, IOM: %lu\n",
+			   is_en(cfg & SGCLT_IOMEN),
+			   FIELD_GET(SGCLT_IOM, cfg));
+		seq_printf(s, "OIPV: %s, IPV: %lu, CTD: %s, GTST: %s\n",
+			   is_en(cfg & SGCLT_OIPV), FIELD_GET(SGCLT_IPV, cfg),
+			   is_yes(cfg & SGCLT_CTD),
+			   cfg & SGCLT_GTST ? "open" : "closed");
+	}
+	seq_puts(s, "\n");
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(netc_show_sgclt_entry);
+
+int netc_show_isct_entry(struct ntmp_priv *priv, struct seq_file *s,
+			 u32 entry_id)
+{
+	struct isct_stse_data *stse __free(kfree);
+	u32 sg_drop_cnt;
+	int err;
+
+	stse = kzalloc(sizeof(*stse), GFP_KERNEL);
+	if (!stse)
+		return -ENOMEM;
+
+	err = ntmp_isct_operate_entry(&priv->cbdrs, entry_id,
+				      NTMP_CMD_QUERY, stse);
+	if (err) {
+		seq_printf(s, "Query ISCT entry ID (0x%x) failed\n", entry_id);
+		return err;
+	}
+
+	sg_drop_cnt = le32_to_cpu(stse->sg_drop_count);
+	/* Workaround for ERR052134 on i.MX95 platform */
+	if (priv->errata & NTMP_ERR052134) {
+		u32 tmp;
+
+		sg_drop_cnt >>= 9;
+
+		tmp = le32_to_cpu(stse->resv3) & 0x1ff;
+		sg_drop_cnt |= (tmp << 23);
+	}
+
+	seq_printf(s, "Show ingress stream count table entry 0x%x\n", entry_id);
+	seq_printf(s, "RX_COUNT: %u, MSDU_DROP_COUNT: %u\n",
+		   le32_to_cpu(stse->rx_count), le32_to_cpu(stse->msdu_drop_count));
+	seq_printf(s, "POLICER_DROP_COUNT: %u, SG_DROP_COUNT: %u\n",
+		   le32_to_cpu(stse->policer_drop_count), sg_drop_cnt);
+	seq_puts(s, "\n");
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(netc_show_isct_entry);
+
+int netc_show_rpt_entry(struct ntmp_priv *priv, struct seq_file *s,
+			u32 entry_id)
+{
+	struct ntmp_rpt_entry *rpt_entry __free(kfree);
+	struct rpt_cfge_data *cfge;
+	struct rpt_stse_data *stse;
+	u32 bcf_bcs, bef_bes;
+	u16 cfg;
+	int err;
+
+	rpt_entry = kzalloc(sizeof(*rpt_entry), GFP_KERNEL);
+	if (!rpt_entry)
+		return -ENOMEM;
+
+	err = ntmp_rpt_query_entry(&priv->cbdrs, entry_id, rpt_entry);
+	if (err) {
+		seq_printf(s, "Query RPT entry ID (0x%x) failed\n", entry_id);
+		return err;
+	}
+
+	cfge = &rpt_entry->cfge;
+	stse = &rpt_entry->stse;
+	bcf_bcs = le32_to_cpu(stse->bcf_bcs);
+	bef_bes = le32_to_cpu(stse->bef_bes);
+	cfg = le16_to_cpu(cfge->cfg);
+	seq_printf(s, "Show rate policer table entry 0x%x\n", entry_id);
+	seq_printf(s, "BYTE_COUNT: %llu, DROP_FRAMES: %u\n",
+		   le64_to_cpu(stse->byte_count), le32_to_cpu(stse->drop_frames));
+	seq_printf(s, "DR0_GRN_FRAMES: %u, DR1_GRN_FRAMES: %u\n",
+		   le32_to_cpu(stse->dr0_grn_frames),
+		   le32_to_cpu(stse->dr1_grn_frames));
+	seq_printf(s, "DR2_YLW_FRAMES: %u, REMARK_YLW_FRAMES: %u\n",
+		   le32_to_cpu(stse->dr2_ylw_frames),
+		   le32_to_cpu(stse->remark_ylw_frames));
+	seq_printf(s, "DR3_RED_FRAMES: %u, REMARK_RED_FRAMES: %u\n",
+		   le32_to_cpu(stse->dr3_red_frames),
+		   le32_to_cpu(stse->remark_red_frames));
+	seq_printf(s, "LTS: 0x%x, BCI: %u, BEI: %u\n", le32_to_cpu(stse->lts),
+		   le32_to_cpu(stse->bci), le32_to_cpu(stse->bei));
+	seq_printf(s, "BCS: %lu, BCF: 0x%lx\n", FIELD_GET(RPT_BCS, bcf_bcs),
+		   FIELD_GET(RPT_BCF, bcf_bcs));
+	seq_printf(s, "BEF: %lu, BEI: 0x%lx\n", FIELD_GET(RPT_BES, bef_bes),
+		   FIELD_GET(RPT_BEF, bef_bes));
+	seq_printf(s, "CIR: %u, CBS: %u, EIR: %u, EBS: %u\n",
+		   le32_to_cpu(cfge->cir), le32_to_cpu(cfge->cbs),
+		   le32_to_cpu(cfge->eir), le32_to_cpu(cfge->ebs));
+	seq_printf(s, "MREN: %s, DOY: %s, CM: %s, CF: %lu\n",
+		   is_en(cfg & RPT_MREN), is_en(cfg & RPT_DOY),
+		   cfg & RPT_DOY ? "aware" : "blind",
+		   FIELD_GET(RPT_CF, cfg));
+	seq_printf(s, "NDOR: %s, SDU type:%lu, FEN: %s, MR: %u\n",
+		   is_en(cfg & RPT_NDOR), FIELD_GET(RPT_SDU_TYPE, cfg),
+		   is_en(rpt_entry->fee.fen & RPT_FEN),
+		   rpt_entry->pse.mr);
+	seq_puts(s, "\n");
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(netc_show_rpt_entry);
+
+int netc_show_ipft_entry(struct ntmp_priv *priv, struct seq_file *s,
+			 u32 entry_id)
+{
+	struct ntmp_ipft_entry *ipft_entry __free(kfree);
+	struct ipft_keye_data *keye;
+	struct ipft_cfge_data *cfge;
+	u16 dscp, src_port;
+	int i, err;
+	u32 cfg;
+
+	ipft_entry = kzalloc(sizeof(*ipft_entry), GFP_KERNEL);
+	if (!ipft_entry)
+		return -ENOMEM;
+
+	err = ntmp_ipft_query_entry(&priv->cbdrs, entry_id, false, ipft_entry);
+	if (err)
+		return err;
+
+	keye = &ipft_entry->keye;
+	cfge = &ipft_entry->cfge;
+
+	cfg = le32_to_cpu(cfge->cfg);
+	dscp = le16_to_cpu(keye->dscp);
+
+	seq_printf(s, "Show ingress port filter table entry:%u\n", entry_id);
+
+	/* KEYE_DATA */
+	seq_printf(s, "Precedence:%u, Frame attribute flags:0x%04x, mask:0x%04x\n",
+		   keye->precedence, keye->frm_attr_flags, keye->frm_attr_flags_mask);
+	seq_printf(s, "DSCP:0x%lx, mask:0x%lx\n", FIELD_GET(IPFT_DSCP, dscp),
+		   FIELD_GET(IPFT_DSCP_MASK, dscp));
+
+	if (priv->dev_type == NETC_DEV_SWITCH) {
+		u8 port_id, port_mask;
+
+		src_port = le16_to_cpu(keye->src_port);
+		port_id = FIELD_GET(IPFT_SRC_PORT, src_port);
+		port_mask = FIELD_GET(IPFT_SRC_PORT_MASK, src_port);
+		seq_printf(s, "Switch Source Port ID:%d, mask:0x%02x\n",
+			   port_id, port_mask);
+	}
+
+	seq_printf(s, "Outer VLAN TCI:0x%04x, mask:0x%04x\n",
+		   ntohs(keye->outer_vlan_tci), ntohs(keye->outer_vlan_tci_mask));
+	seq_printf(s, "Inner VLAN TCI:0x%04x, mask:0x%04x\n",
+		   ntohs(keye->inner_vlan_tci), ntohs(keye->inner_vlan_tci_mask));
+	seq_printf(s, "Destination MAC:%pM\n", keye->dmac);
+	seq_printf(s, "Destination MAC mask:%pM\n", keye->dmac_mask);
+	seq_printf(s, "Source MAC:%pM\n", keye->smac);
+	seq_printf(s, "Source MAC mask:%pM\n", keye->smac_mask);
+	seq_printf(s, "Ether Type:0x%04x, mask:0x%04x\n", ntohs(keye->ethertype),
+		   ntohs(keye->ethertype_mask));
+	seq_printf(s, "IP protocol:%u, mask:0x%02x\n",  keye->ip_protocol,
+		   keye->ip_protocol_mask);
+	seq_printf(s, "IP Source Address:%08x:%08x:%08x:%08x\n",
+		   ntohl(keye->ip_src[0]), ntohl(keye->ip_src[1]),
+		   ntohl(keye->ip_src[2]), ntohl(keye->ip_src[3]));
+	seq_printf(s, "IP Source Address mask:%08x:%08x:%08x:%08x\n",
+		   ntohl(keye->ip_src_mask[0]), ntohl(keye->ip_src_mask[1]),
+		   ntohl(keye->ip_src_mask[2]), ntohl(keye->ip_src_mask[3]));
+	seq_printf(s, "IP Destination Address:%08x:%08x:%08x:%08x\n",
+		   ntohl(keye->ip_dst[0]), ntohl(keye->ip_dst[1]),
+		   ntohl(keye->ip_dst[2]), ntohl(keye->ip_dst[3]));
+	seq_printf(s, "IP Destination Address mask:%08x:%08x:%08x:%08x\n",
+		   ntohl(keye->ip_dst_mask[0]), ntohl(keye->ip_dst_mask[1]),
+		   ntohl(keye->ip_dst_mask[2]), ntohl(keye->ip_dst_mask[3]));
+	seq_printf(s, "L4 Source Port:%x, mask:0x%04x\n",
+		   ntohs(keye->l4_src_port), ntohs(keye->l4_src_port_mask));
+	seq_printf(s, "L4 Destination Port:%x, mask:0x%04x\n",
+		   ntohs(keye->l4_dst_port), ntohs(keye->l4_dst_port_mask));
+	for (i = 0; i < IPFT_MAX_PLD_LEN; i = i + 6) {
+		seq_printf(s, "Payload %d~%d: %02x %02x %02x %02x %02x %02x\n",
+			   i, i + 5, keye->byte[i].data, keye->byte[i + 1].data,
+			   keye->byte[i + 2].data, keye->byte[i + 3].data,
+			   keye->byte[i + 4].data, keye->byte[i + 5].data);
+		seq_printf(s, "Payload Mask %d~%d: %02x %02x %02x %02x %02x %02x\n",
+			   i, i + 5, keye->byte[i].mask, keye->byte[i + 1].mask,
+			   keye->byte[i + 2].mask, keye->byte[i + 3].mask,
+			   keye->byte[i + 4].mask, keye->byte[i + 5].mask);
+	}
+
+	/* STSE_DATA */
+	seq_printf(s, "Match Count:%llu\n", le64_to_cpu(ipft_entry->match_count));
+
+	/* CFGE_DATA */
+	seq_printf(s, "Override internal Priority %s: %lu\n",
+		   is_en(cfg & IPFT_OIPV), FIELD_GET(IPFT_IPV, cfg));
+	seq_printf(s, "Override Drop Resilience %s: %lu\n",
+		   is_en(IPFT_ODR & cfg), FIELD_GET(IPFT_DR, cfg));
+	seq_printf(s, "Filter Forwarding Action: %lu\n",
+		   FIELD_GET(IPFT_FLTFA, cfg));
+	seq_printf(s, "Filter Action: %lu\n", FIELD_GET(IPFT_FLTA, cfg));
+	seq_printf(s, "Relative Precedent Resolution: %lu\n",
+		   FIELD_GET(IPFT_RPR, cfg));
+	seq_printf(s, "Target For Selected Filter Action: 0x%x\n",
+		   le32_to_cpu(cfge->flta_tgt));
+
+	if (priv->dev_type == NETC_DEV_SWITCH) {
+		seq_printf(s, "Ingress Mirroring %s, Cut through disable: %s\n",
+			   is_en(cfg & IPFT_IMIRE), is_yes(cfg & IPFT_CTD));
+		seq_printf(s, "Host Reason: %lu, Timestamp Capture %s\n",
+			   FIELD_GET(IPFT_HR, cfg), is_en(cfg & IPFT_TIMECAPE));
+		seq_printf(s, "Report Receive Timestamp: %s\n",
+			   is_yes(cfg & IPFT_RRT));
+		seq_printf(s, "Event monitor event ID: %lu\n",
+			   FIELD_GET(IPFT_EVMEID, cfg));
+	} else {
+		seq_printf(s, "Wake-on-LAN Trigger %s\n",
+			   is_en(cfg & IPFT_WOLTE));
+		seq_printf(s, "Bypass L2 Filtering: %s\n",
+			   is_yes(cfg & IPFT_BL2F));
+	}
+
+	seq_puts(s, "\n");
+
+	return err;
+}
+EXPORT_SYMBOL_GPL(netc_show_ipft_entry);
+
+int netc_show_tgst_entry(struct ntmp_priv *priv, struct seq_file *s,
+			 u32 entry_id)
+{
+	struct tgst_query_data *qdata __free(kfree);
+	int i, err;
+
+	qdata = kzalloc(sizeof(*qdata), GFP_KERNEL);
+	if (!qdata)
+		return -ENOMEM;
+
+	err = ntmp_tgst_query_entry(&priv->cbdrs, entry_id, qdata);
+	if (err)
+		return err;
+
+	seq_puts(s, "Dump Time Gate Scheduling Table Entry:\n");
+	seq_printf(s, "Entry ID:%d\n", entry_id);
+	seq_printf(s, "Admin Base Time:%llu\n", le64_to_cpu(qdata->admin_bt));
+	seq_printf(s, "Admin Cycle Time:%u\n", le32_to_cpu(qdata->admin_ct));
+	seq_printf(s, "Admin Cycle Extend Time:%u\n",
+		   le32_to_cpu(qdata->admin_ct_ext));
+	seq_printf(s, "Admin Control List Length:%u\n",
+		   le16_to_cpu(qdata->admin_cl_len));
+	for (i = 0; i < le16_to_cpu(qdata->admin_cl_len); i++) {
+		seq_printf(s, "Gate Entry %d info:\n", i);
+		seq_printf(s, "\tAdmin time interval:%u\n",
+			   le32_to_cpu(qdata->cfge_ge[i].interval));
+		seq_printf(s, "\tAdmin Traffic Class states:%02x\n",
+			   qdata->cfge_ge[i].tc_state);
+		seq_printf(s, "\tAdministrative gate operation type:%u\n",
+			   qdata->cfge_ge[i].hr_cb);
+	}
+
+	seq_printf(s, "Config Change Time:%llu\n", le64_to_cpu(qdata->oper_cfg_ct));
+	seq_printf(s, "Config Change Error:%llu\n", le64_to_cpu(qdata->oper_cfg_ce));
+	seq_printf(s, "Operation Base Time:%llu\n", le64_to_cpu(qdata->oper_bt));
+	seq_printf(s, "Operation Cycle Time:%u\n", le32_to_cpu(qdata->oper_ct));
+	seq_printf(s, "Operation Cycle Extend Time:%u\n",
+		   le32_to_cpu(qdata->oper_ct_ext));
+	seq_printf(s, "Operation Control List Length:%u\n",
+		   le16_to_cpu(qdata->oper_cl_len));
+	for (i = 0; i < le16_to_cpu(qdata->oper_cl_len); i++) {
+		seq_printf(s, "Gate Entry %d info:\n", i);
+		seq_printf(s, "\tOperation time interval:%u\n",
+			   le32_to_cpu(qdata->olse_ge[i].interval));
+		seq_printf(s, "\tOperation Traffic Class states:%02x\n",
+			   qdata->olse_ge[i].tc_state);
+		seq_printf(s, "\tOperation gate operation type:%u\n",
+			   qdata->olse_ge[i].hr_cb);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(netc_show_tgst_entry);
+
+void netc_show_ipft_flower(struct seq_file *s, struct netc_flower_rule *rule)
+{
+	struct ntmp_ipft_entry *ipft_entry = rule->key_tbl->ipft_entry;
+	struct ntmp_ist_entry *ist_entry = rule->key_tbl->ist_entry;
+	u32 ipft_cfg = le32_to_cpu(ipft_entry->cfge.cfg);
+	u32 rpt_eid = NTMP_NULL_ENTRY_ID;
+
+	seq_printf(s, "IPFT entry ID:0x%x\n", ipft_entry->entry_id);
+	if (ist_entry) {
+		seq_printf(s, "IST entry ID: 0x%x\n", ist_entry->entry_id);
+		seq_printf(s, "ISCT entry ID: 0x%x\n", ist_entry->cfge.isc_eid);
+		rpt_eid = le32_to_cpu(ist_entry->cfge.rp_eid);
+	}
+
+	if (FIELD_GET(IPFT_FLTA, ipft_cfg) == IPFT_FLTA_RP)
+		rpt_eid = le32_to_cpu(ipft_entry->cfge.flta_tgt);
+
+	if (rpt_eid != NTMP_NULL_ENTRY_ID)
+		seq_printf(s, "RPT entry ID: 0x%x\n", rpt_eid);
+}
+EXPORT_SYMBOL_GPL(netc_show_ipft_flower);
diff --git a/devices/enetc/netc_tc_lib.c b/devices/enetc/netc_tc_lib.c
new file mode 100755
index 0000000..af6483f
--- /dev/null
+++ b/devices/enetc/netc_tc_lib.c
@@ -0,0 +1,1480 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+/*
+ * NETC NTMP (NETC Table Management Protocol) 2.0 driver
+ * Copyright 2024 NXP
+ */
+#include <linux/fsl/netc_lib.h>
+
+#include "ntmp_private.h"
+
+#define SDU_TYPE_MPDU				1
+
+struct netc_flower_rule *
+netc_find_flower_rule_by_cookie(struct ntmp_priv *priv, int port_id,
+				unsigned long cookie)
+{
+	struct netc_flower_rule *rule;
+
+	hlist_for_each_entry(rule, &priv->flower_list, node) {
+		if (priv->dev_type == NETC_DEV_SWITCH) {
+			if (rule->port_id == port_id && rule->cookie == cookie)
+				return rule;
+		} else {
+			if (rule->cookie == cookie)
+				return rule;
+		}
+	}
+
+	return NULL;
+}
+EXPORT_SYMBOL_GPL(netc_find_flower_rule_by_cookie);
+
+static bool netc_flower_isit_key_matched(const struct isit_keye_data *key1,
+					 const struct isit_keye_data *key2)
+{
+	if (memcmp(key1, key2, sizeof(*key1)))
+		return false;
+
+	return true;
+}
+
+static bool netc_flower_ipft_key_matched(const struct ipft_keye_data *key1,
+					 const struct ipft_keye_data *key2)
+{
+	const void *key1_start = &key1->frm_attr_flags;
+	const void *key2_start = &key2->frm_attr_flags;
+	u32 size = sizeof(*key1) - 8;
+
+	if (memcmp(key1_start, key2_start, size))
+		return false;
+
+	return true;
+}
+
+struct netc_flower_rule *
+netc_find_flower_rule_by_key(struct ntmp_priv *priv,
+			     enum netc_key_tbl_type tbl_type,
+			     void *key)
+{
+	static struct netc_flower_key_tbl *key_tbl;
+	struct netc_flower_rule *rule;
+
+	hlist_for_each_entry(rule, &priv->flower_list, node) {
+		key_tbl = rule->key_tbl;
+		if (key_tbl->tbl_type != tbl_type)
+			continue;
+
+		if (tbl_type == FLOWER_KEY_TBL_ISIT &&
+		    netc_flower_isit_key_matched(key, &key_tbl->isit_entry->keye))
+			return rule;
+		else if (tbl_type == FLOWER_KEY_TBL_IPFT &&
+			 netc_flower_ipft_key_matched(key, &key_tbl->ipft_entry->keye))
+			return rule;
+	}
+
+	return NULL;
+}
+EXPORT_SYMBOL_GPL(netc_find_flower_rule_by_key);
+
+static int netc_psfp_flower_key_validate(struct ntmp_priv *priv,
+					 struct isit_keye_data *keye, int prio,
+					 struct netc_flower_key_tbl **key_tbl,
+					 struct netlink_ext_ack *extack)
+{
+	struct netc_flower_rule *rule, *tmp_rule;
+	struct netc_flower_key_tbl *tmp_tbl;
+
+	/* Find the first rule with the same ISIT key */
+	rule = netc_find_flower_rule_by_key(priv, FLOWER_KEY_TBL_ISIT, keye);
+	if (!rule)
+		return 0;
+
+	if (rule->flower_type != FLOWER_TYPE_PSFP) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Cannot add new rule with different flower type");
+		return -EINVAL;
+	}
+
+	if (prio < 0) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Rule conflicts with existing rules");
+		return -EINVAL;
+	}
+
+	/* Unsupport if existing rule does not have ISFT entry */
+	if (!rule->isft_entry) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "VLAN pbit in rule conflicts with existing rule");
+		return -EINVAL;
+	}
+
+	/* If there are other rules using the same key, an error is returned */
+	hlist_for_each_entry(tmp_rule, &priv->flower_list, node) {
+		tmp_tbl = tmp_rule->key_tbl;
+		if (tmp_tbl->tbl_type != FLOWER_KEY_TBL_ISIT)
+			continue;
+
+		if (!netc_flower_isit_key_matched(keye, &tmp_tbl->isit_entry->keye))
+			continue;
+
+		if (tmp_rule->isft_entry &&
+		    FIELD_GET(ISFT_PCP, tmp_rule->isft_entry->keye.pcp) == prio) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "The same key has been used by existing rule");
+			return -EINVAL;
+		}
+	}
+
+	*key_tbl = rule->key_tbl;
+
+	return 0;
+}
+
+static struct netc_gate_tbl *
+netc_find_flower_gate_table(struct ntmp_priv *priv, u32 index)
+{
+	struct netc_flower_rule *rule;
+
+	hlist_for_each_entry(rule, &priv->flower_list, node) {
+		struct netc_gate_tbl *gate_tbl = rule->gate_tbl;
+
+		if (!gate_tbl)
+			continue;
+
+		if (gate_tbl->sgit_eid == index)
+			return gate_tbl;
+	}
+
+	return NULL;
+}
+
+static int netc_psfp_gate_entry_validate(struct ntmp_priv *priv,
+					 struct flow_action_entry *gate_entry,
+					 struct netc_gate_tbl **gate_tbl,
+					 struct netlink_ext_ack *extack)
+{
+	u64 max_cycle_time;
+	u32 num_gates;
+
+	if (!gate_entry) {
+		NL_SET_ERR_MSG_MOD(extack, "No gate entries");
+		return -EINVAL;
+	}
+
+	num_gates = gate_entry->gate.num_entries;
+	if (num_gates > SGCLT_MAX_GE_NUM) {
+		NL_SET_ERR_MSG_MOD(extack, "Gate number exceeds 256");
+		return -EINVAL;
+	}
+
+	max_cycle_time = gate_entry->gate.cycletime + gate_entry->gate.cycletimeext;
+	if (max_cycle_time > SGIT_MAX_CT_PLUS_CT_EXT) {
+		NL_SET_ERR_MSG_MOD(extack, "Max cycle time exceeds 0x3ffffff ns");
+		return -EINVAL;
+	}
+
+	if (gate_entry->hw_index >= priv->caps.sgit_num_entries) {
+		NL_SET_ERR_MSG_FMT_MOD(extack, "Gate hw index cannot exceed %u",
+				       priv->caps.sgit_num_entries - 1);
+		return -EINVAL;
+	}
+
+	if (test_and_set_bit(gate_entry->hw_index, priv->sgit_eid_bitmap))
+		*gate_tbl = netc_find_flower_gate_table(priv,
+							gate_entry->hw_index);
+
+	return 0;
+}
+
+static struct netc_police_tbl *
+netc_find_flower_police_table(struct ntmp_priv *priv, u32 index)
+{
+	struct netc_flower_rule *rule;
+
+	hlist_for_each_entry(rule, &priv->flower_list, node) {
+		struct netc_police_tbl *police_tbl = rule->police_tbl;
+
+		if (!police_tbl)
+			continue;
+
+		if (police_tbl->rpt_eid == index)
+			return police_tbl;
+	}
+
+	return NULL;
+}
+
+int netc_police_entry_validate(struct ntmp_priv *priv,
+			       const struct flow_action *action,
+			       const struct flow_action_entry *police_entry,
+			       struct netc_police_tbl **police_tbl,
+			       struct netlink_ext_ack *extack)
+{
+	if (police_entry->police.exceed.act_id != FLOW_ACTION_DROP) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Offload not supported when exceed action is not drop");
+		return -EOPNOTSUPP;
+	}
+
+	if (police_entry->police.notexceed.act_id != FLOW_ACTION_PIPE &&
+	    police_entry->police.notexceed.act_id != FLOW_ACTION_ACCEPT) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Offload not supported when conform action is not pipe or ok");
+		return -EOPNOTSUPP;
+	}
+
+	if (police_entry->police.notexceed.act_id == FLOW_ACTION_ACCEPT &&
+	    !flow_action_is_last_entry(action, police_entry)) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Offload not supported when conform action is ok, but action is not last");
+		return -EOPNOTSUPP;
+	}
+
+	if (police_entry->police.peakrate_bytes_ps ||
+	    police_entry->police.avrate || police_entry->police.overhead) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Offload not supported when peakrate/avrate/overhead is configured");
+		return -EOPNOTSUPP;
+	}
+
+	if (police_entry->police.rate_pkt_ps) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "QoS offload not support packets per second");
+		return -EOPNOTSUPP;
+	}
+
+	if (!police_entry->police.rate_bytes_ps && !police_entry->police.burst) {
+		NL_SET_ERR_MSG_MOD(extack, "Burst and rate cannot be all 0");
+		return -EINVAL;
+	}
+
+	if (police_entry->hw_index >= priv->caps.rpt_num_entries) {
+		NL_SET_ERR_MSG_FMT_MOD(extack, "Police index cannot exceed %u",
+				       priv->caps.rpt_num_entries - 1);
+		return -EINVAL;
+	}
+
+	if (test_and_set_bit(police_entry->hw_index, priv->rpt_eid_bitmap))
+		*police_tbl = netc_find_flower_police_table(priv,
+							    police_entry->hw_index);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(netc_police_entry_validate);
+
+static int netc_psfp_isit_keye_construct(struct flow_rule *rule, int port_index,
+					 struct isit_keye_data *keye, int *prio,
+					 struct netlink_ext_ack *extack)
+{
+	struct flow_match_eth_addrs addr_match = {0};
+	struct flow_match_vlan vlan_match = {0};
+	struct isit_psfp_frame_key *frame_key;
+	u32 key_aux;
+	u16 vlan;
+
+	frame_key = (struct isit_psfp_frame_key *)keye->frame_key;
+	/* For ENETC, the port_index should be 0 */
+	key_aux = FIELD_PREP(ISIT_SRC_PORT_ID, port_index);
+
+	if (!flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ETH_ADDRS)) {
+		NL_SET_ERR_MSG_MOD(extack, "Unsupported, must include ETH_ADDRS");
+		return -EINVAL;
+	}
+
+	flow_rule_match_eth_addrs(rule, &addr_match);
+	if (!is_zero_ether_addr(addr_match.mask->dst) &&
+	    !is_zero_ether_addr(addr_match.mask->src)) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Cannot match on both source and destination MAC");
+		return -EINVAL;
+	}
+
+	if (!is_zero_ether_addr(addr_match.mask->dst)) {
+		if (!is_broadcast_ether_addr(addr_match.mask->dst)) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "Masked matching on destination MAC not supported");
+			return -EINVAL;
+		}
+
+		ether_addr_copy(frame_key->mac, addr_match.key->dst);
+		key_aux |= FIELD_PREP(ISIT_KEY_TYPE, ISIT_KEY_TYPE1_DMAC_VLAN);
+	}
+
+	if (!is_zero_ether_addr(addr_match.mask->src)) {
+		if (!is_broadcast_ether_addr(addr_match.mask->src)) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "Masked matching on source MAC not supported");
+			return -EINVAL;
+		}
+
+		ether_addr_copy(frame_key->mac, addr_match.key->src);
+		key_aux |= FIELD_PREP(ISIT_KEY_TYPE, ISIT_KEY_TYPE0_SMAC_VLAN);
+	}
+
+	keye->key_aux = cpu_to_le32(key_aux);
+
+	if (!flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_VLAN))
+		return 0;
+
+	flow_rule_match_vlan(rule, &vlan_match);
+	if (vlan_match.mask->vlan_id) {
+		if (vlan_match.mask->vlan_id != VLAN_VID_MASK) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "Only full mask is supported for VLAN ID");
+			return -EINVAL;
+		}
+
+		vlan = vlan_match.key->vlan_id;
+		vlan |= BIT(15);
+		frame_key->vlan_h = (vlan >> 8) & 0xff;
+		frame_key->vlan_l = vlan & 0xff;
+	}
+
+	if (vlan_match.mask->vlan_priority) {
+		if (vlan_match.mask->vlan_priority !=
+		    (VLAN_PRIO_MASK >> VLAN_PRIO_SHIFT)) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "Only full mask is supported for VLAN priority");
+			return -EINVAL;
+		}
+
+		*prio = vlan_match.key->vlan_priority;
+	}
+
+	return 0;
+}
+
+static void netc_psfp_gate_entry_config(struct ntmp_priv *priv,
+					struct flow_action_entry *gate_entry,
+					struct ntmp_sgit_entry *sgit_entry,
+					struct ntmp_sgclt_entry *sgclt_entry)
+{
+	u32 cycle_time, cycle_time_ext, num_gates;
+	u64 base_time = gate_entry->gate.basetime;
+	u8 sgit_cfg, sgit_icfg = SGIT_GST;
+	u8 sgclt_extcfg = SGCLT_EXT_GTST;
+	int i;
+
+	num_gates = gate_entry->gate.num_entries;
+	cycle_time = gate_entry->gate.cycletime;
+	cycle_time_ext = gate_entry->gate.cycletimeext;
+
+	if (gate_entry->gate.prio >= 0) {
+		sgit_icfg |= FIELD_PREP(SGIT_IPV, gate_entry->gate.prio);
+		sgit_icfg |= SGIT_OIPV;
+	}
+
+	if (priv->adjust_base_time)
+		base_time = priv->adjust_base_time(priv, base_time,
+						   gate_entry->gate.cycletime);
+
+	sgit_cfg = FIELD_PREP(SGIT_SDU_TYPE, SDU_TYPE_MPDU);
+	sgit_entry->acfge.admin_base_time = cpu_to_le64(base_time);
+	sgit_entry->acfge.admin_sgcl_eid = cpu_to_le32(sgclt_entry->entry_id);
+	sgit_entry->acfge.admin_cycle_time_ext = cpu_to_le32(cycle_time_ext);
+	sgit_entry->cfge.cfg = sgit_cfg;
+	sgit_entry->icfge.icfg = sgit_icfg;
+
+	sgclt_entry->cfge.cycle_time = cpu_to_le32(cycle_time);
+	sgclt_entry->cfge.list_length = num_gates - 1;
+	if (gate_entry->gate.prio >= 0) {
+		sgclt_extcfg |= FIELD_PREP(SGCLT_EXT_IPV, gate_entry->gate.prio);
+		sgclt_extcfg |= SGCLT_EXT_OIPV;
+	}
+	sgclt_entry->cfge.ext_cfg = sgclt_extcfg;
+
+	for (i = 0; i < num_gates; i++) {
+		struct action_gate_entry *from = &gate_entry->gate.entries[i];
+		struct sgclt_ge *to = &sgclt_entry->cfge.ge[i];
+		u32 sgclt_cfg = 0;
+
+		if (from->gate_state)
+			sgclt_cfg |= SGCLT_GTST;
+
+		if (from->ipv >= 0) {
+			sgclt_cfg |= FIELD_PREP(SGCLT_IPV, from->ipv);
+			sgclt_cfg |= SGCLT_OIPV;
+		}
+
+		if (from->maxoctets >= 0) {
+			sgclt_cfg |= FIELD_PREP(SGCLT_IOM, from->maxoctets);
+			sgclt_cfg |= SGCLT_IOMEN;
+		}
+
+		to->interval = cpu_to_le32(from->interval);
+		to->cfg = cpu_to_le32(sgclt_cfg);
+	}
+}
+
+void netc_rpt_entry_config(struct flow_action_entry *police_entry,
+			   struct ntmp_rpt_entry *rpt_entry)
+{
+	u64 rate_bps;
+	u32 cir, cbs;
+	u16 cfg;
+
+	rpt_entry->entry_id = police_entry->hw_index;
+
+	/* The unit of rate_bytes_ps is 1Bps, the uint of cir is 3.725bps,
+	 * so convert it.
+	 */
+	rate_bps = police_entry->police.rate_bytes_ps * 8;
+	cir = div_u64(rate_bps * 1000, 3725);
+	cbs = police_entry->police.burst;
+	cfg = FIELD_PREP(RPT_SDU_TYPE, SDU_TYPE_MPDU);
+	rpt_entry->cfge.cir = cpu_to_le32(cir);
+	rpt_entry->cfge.cbs = cpu_to_le32(cbs);
+	rpt_entry->cfge.cfg = cpu_to_le16(cfg);
+	rpt_entry->fee.fen = RPT_FEN;
+}
+EXPORT_SYMBOL_GPL(netc_rpt_entry_config);
+
+static int netc_delete_sgclt_entry(struct ntmp_priv *priv, u32 entry_id)
+{
+	struct ntmp_sgclt_entry *sgclt_entry __free(kfree);
+	struct netc_cbdrs *cbdrs = &priv->cbdrs;
+	u32 max_data_size, max_cfge_size;
+	u32 num_gates, entry_size;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return 0;
+
+	max_cfge_size = struct_size_t(struct sgclt_cfge_data, ge,
+				      SGCLT_MAX_GE_NUM);
+	max_data_size = struct_size(sgclt_entry, cfge.ge, SGCLT_MAX_GE_NUM);
+	sgclt_entry = kzalloc(max_data_size, GFP_KERNEL);
+	if (!sgclt_entry)
+		return -ENOMEM;
+
+	err = ntmp_sgclt_query_entry(cbdrs, entry_id, sgclt_entry, max_cfge_size);
+	if (err)
+		return err;
+
+	/* entry_size equals to 1 + ROUNDUP(N / 2) where N is number of gates */
+	num_gates = sgclt_entry->cfge.list_length + 1;
+	entry_size = 1 + DIV_ROUND_UP(num_gates, 2);
+	err = ntmp_sgclt_delete_entry(cbdrs, entry_id);
+	if (err)
+		return err;
+
+	ntmp_clear_words_bitmap(priv->sgclt_word_bitmap, entry_id, entry_size);
+
+	return 0;
+}
+
+static int netc_delete_sgit_entry(struct ntmp_priv *priv, u32 entry_id)
+{
+	struct ntmp_sgit_entry *entry __free(kfree);
+	struct netc_cbdrs *cbdrs = &priv->cbdrs;
+	struct ntmp_sgit_entry new_entry = {0};
+	u32 sgcl_eid;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return 0;
+
+	entry = kzalloc(sizeof(*entry), GFP_KERNEL);
+	if (!entry)
+		return -ENOMEM;
+
+	/* Step1: Query the stream gate instance table entry to retrieve
+	 * the entry id of the administrative gate control list and the
+	 * opertational gate control list.
+	 */
+	err = ntmp_sgit_query_entry(cbdrs, entry_id, entry);
+	if (err)
+		return err;
+
+	/* Step2: Update the stream gate instance table entry to set
+	 * the entry id of the administrative gate control list to NULL.
+	 */
+	new_entry.acfge.admin_sgcl_eid = cpu_to_le32(NTMP_NULL_ENTRY_ID);
+	new_entry.entry_id = entry_id;
+	err = ntmp_sgit_add_or_update_entry(cbdrs, &new_entry);
+	if (err)
+		return err;
+
+	/* Step3: Delete the stream gate instance table entry. */
+	err = ntmp_sgit_delete_entry(cbdrs, entry_id);
+	if (err)
+		return err;
+
+	ntmp_clear_eid_bitmap(priv->sgit_eid_bitmap, entry_id);
+
+	/* Step4: Delete the administrative gate control list
+	 * and the operational gate control list.
+	 */
+	sgcl_eid = le32_to_cpu(entry->acfge.admin_sgcl_eid);
+	err = netc_delete_sgclt_entry(priv, sgcl_eid);
+	if (err)
+		return err;
+
+	sgcl_eid = le32_to_cpu(entry->sgise.oper_sgcl_eid);
+	err = netc_delete_sgclt_entry(priv, sgcl_eid);
+
+	return err;
+}
+
+static int netc_psfp_set_related_tables(struct ntmp_priv *priv,
+					struct netc_psfp_tbl_entries *tbl)
+{
+	struct ntmp_sgclt_entry *sgclt_entry = tbl->sgclt_entry;
+	struct ntmp_isit_entry *isit_entry = tbl->isit_entry;
+	struct ntmp_isft_entry *isft_entry = tbl->isft_entry;
+	struct ntmp_sgit_entry *sgit_entry = tbl->sgit_entry;
+	struct ntmp_isct_entry *isct_entry = tbl->isct_entry;
+	struct ntmp_ist_entry *ist_entry = tbl->ist_entry;
+	struct ntmp_rpt_entry *rpt_entry = tbl->rpt_entry;
+	struct netc_cbdrs *cbdrs = &priv->cbdrs;
+	int err;
+
+	err = ntmp_isct_operate_entry(cbdrs, isct_entry->entry_id,
+				      NTMP_CMD_ADD, NULL);
+	if (err)
+		return err;
+
+	if (sgclt_entry) {
+		err = ntmp_sgclt_add_entry(cbdrs, sgclt_entry);
+		if (err)
+			goto delete_isct_entry;
+	}
+
+	if (sgit_entry) {
+		err = ntmp_sgit_add_or_update_entry(cbdrs, sgit_entry);
+		if (err) {
+			if (sgclt_entry)
+				ntmp_sgclt_delete_entry(cbdrs,
+							sgclt_entry->entry_id);
+			goto delete_isct_entry;
+		}
+	}
+
+	if (rpt_entry) {
+		err = ntmp_rpt_add_or_update_entry(cbdrs, rpt_entry);
+		if (err)
+			goto delete_sgit_entry;
+	}
+
+	if (ist_entry) {
+		err = ntmp_ist_add_or_update_entry(cbdrs, ist_entry);
+		if (err)
+			goto delete_rpt_entry;
+	}
+
+	if (isft_entry) {
+		err = ntmp_isft_add_or_update_entry(cbdrs, true, isft_entry);
+		if (err)
+			goto delete_ist_entry;
+	}
+
+	if (isit_entry) {
+		err = ntmp_isit_add_or_update_entry(cbdrs, true, isit_entry);
+		if (err)
+			goto delete_isft_entry;
+	}
+
+	return 0;
+
+delete_isft_entry:
+	if (isft_entry)
+		ntmp_isft_delete_entry(cbdrs, isft_entry->entry_id);
+
+delete_ist_entry:
+	if (ist_entry)
+		ntmp_ist_delete_entry(cbdrs, ist_entry->entry_id);
+
+delete_rpt_entry:
+	if (rpt_entry)
+		ntmp_rpt_delete_entry(cbdrs, rpt_entry->entry_id);
+
+delete_sgit_entry:
+	if (sgit_entry)
+		netc_delete_sgit_entry(priv, sgit_entry->entry_id);
+
+delete_isct_entry:
+	ntmp_isct_operate_entry(cbdrs, ist_entry->entry_id, NTMP_CMD_DELETE, NULL);
+
+	return err;
+}
+
+void netc_init_ist_entry_eids(struct ntmp_priv *priv,
+			      struct ntmp_ist_entry *ist_entry)
+{
+	struct ist_cfge_data *cfge = &ist_entry->cfge;
+
+	cfge->rp_eid = cpu_to_le32(NTMP_NULL_ENTRY_ID);
+	cfge->sgi_eid = cpu_to_le32(NTMP_NULL_ENTRY_ID);
+	cfge->isc_eid = cpu_to_le32(NTMP_NULL_ENTRY_ID);
+
+	if (priv->dev_type == NETC_DEV_SWITCH) {
+		cfge->isqg_eid = cpu_to_le32(NTMP_NULL_ENTRY_ID);
+		cfge->ifm_eid = cpu_to_le32(NTMP_NULL_ENTRY_ID);
+		cfge->et_eid = cpu_to_le32(NTMP_NULL_ENTRY_ID);
+	}
+}
+EXPORT_SYMBOL_GPL(netc_init_ist_entry_eids);
+
+static int netc_add_psfp_key_tbl(struct ntmp_priv *priv,
+				 struct netc_flower_key_tbl **key_tbl,
+				 struct isit_keye_data *isit_key,
+				 struct netlink_ext_ack *extack)
+{
+	struct netc_flower_key_tbl *new_tbl __free(kfree);
+	struct ntmp_isit_entry *isit_entry __free(kfree);
+	struct ntmp_ist_entry *ist_entry __free(kfree);
+	u32 ist_cfg = 0;
+
+	new_tbl = kzalloc(sizeof(*new_tbl), GFP_KERNEL);
+	if (!new_tbl)
+		return -ENOMEM;
+
+	isit_entry = kzalloc(sizeof(*isit_entry), GFP_KERNEL);
+	if (!isit_entry)
+		return -ENOMEM;
+
+	ist_entry = kzalloc(sizeof(*ist_entry), GFP_KERNEL);
+	if (!ist_entry)
+		return -ENOMEM;
+
+	new_tbl->tbl_type = FLOWER_KEY_TBL_ISIT;
+	refcount_set(&new_tbl->refcount, 1);
+
+	ist_entry->entry_id = ntmp_lookup_free_eid(priv->ist_eid_bitmap,
+						   priv->caps.ist_num_entries);
+	if (ist_entry->entry_id == NTMP_NULL_ENTRY_ID) {
+		NL_SET_ERR_MSG_MOD(extack, "No available IST entry is found");
+		return -ENOSPC;
+	}
+
+	switch (priv->cbdrs.tbl.ist_ver) {
+	case NTMP_TBL_VER0:
+		if (priv->dev_type == NETC_DEV_SWITCH)
+			ist_cfg |= FIELD_PREP(IST_V0_FA, IST_SWITCH_FA_BF);
+		else
+			ist_cfg |= FIELD_PREP(IST_V0_FA, IST_FA_NO_SI_BITMAP);
+
+		ist_cfg |= FIELD_PREP(IST_V0_SDU_TYPE, SDU_TYPE_MPDU);
+		break;
+	case NTMP_TBL_VER1:
+		if (priv->dev_type == NETC_DEV_SWITCH)
+			ist_cfg |= FIELD_PREP(IST_V1_FA, IST_SWITCH_FA_BF);
+		else
+			ist_cfg |= FIELD_PREP(IST_V1_FA, IST_FA_NO_SI_BITMAP);
+
+		ist_cfg |= FIELD_PREP(IST_V1_SDU_TYPE, SDU_TYPE_MPDU);
+		break;
+	default:
+		NL_SET_ERR_MSG_MOD(extack, "Unknown IST version");
+		ntmp_clear_eid_bitmap(priv->ist_eid_bitmap,
+				      ist_entry->entry_id);
+
+		return -EINVAL;
+	}
+
+	ist_entry->cfge.cfg = cpu_to_le32(ist_cfg);
+	netc_init_ist_entry_eids(priv, ist_entry);
+
+	isit_entry->is_eid = cpu_to_le32(ist_entry->entry_id);
+	isit_entry->keye = *isit_key;
+
+	new_tbl->isit_entry = no_free_ptr(isit_entry);
+	new_tbl->ist_entry = no_free_ptr(ist_entry);
+	*key_tbl = no_free_ptr(new_tbl);
+
+	return 0;
+}
+
+void netc_free_flower_key_tbl(struct ntmp_priv *priv,
+			      struct netc_flower_key_tbl *key_tbl)
+{
+	struct ntmp_ist_entry *ist_entry;
+
+	if (!key_tbl)
+		return;
+
+	ist_entry = key_tbl->ist_entry;
+	if (ist_entry) {
+		ntmp_clear_eid_bitmap(priv->ist_eid_bitmap, ist_entry->entry_id);
+		kfree(key_tbl->ist_entry);
+	}
+
+	switch (key_tbl->tbl_type) {
+	case FLOWER_KEY_TBL_ISIT:
+		kfree(key_tbl->isit_entry);
+		break;
+	case FLOWER_KEY_TBL_IPFT:
+		kfree(key_tbl->ipft_entry);
+		break;
+	}
+
+	kfree(key_tbl);
+}
+EXPORT_SYMBOL_GPL(netc_free_flower_key_tbl);
+
+int netc_setup_psfp(struct ntmp_priv *priv, int port_id,
+		    struct flow_cls_offload *f)
+{
+	struct flow_action_entry *gate_entry = NULL, *police_entry = NULL;
+	struct flow_rule *cls_rule = flow_cls_offload_flow_rule(f);
+	struct ntmp_sgclt_entry *sgclt_entry __free(kfree) = NULL;
+	struct netc_police_tbl *police_tbl __free(kfree) = NULL;
+	struct ntmp_isft_entry *isft_entry __free(kfree) = NULL;
+	struct ntmp_sgit_entry *sgit_entry __free(kfree) = NULL;
+	struct ntmp_rpt_entry *rpt_entry __free(kfree) = NULL;
+	struct netc_gate_tbl *gate_tbl __free(kfree) = NULL;
+	struct netc_flower_rule *rule __free(kfree) = NULL;
+	struct netc_flower_key_tbl *reused_key_tbl = NULL;
+	struct netlink_ext_ack *extack = f->common.extack;
+	struct ntmp_isct_entry *isct_entry __free(kfree);
+	struct netc_police_tbl *reused_police_tbl = NULL;
+	struct netc_gate_tbl *reused_gate_tbl = NULL;
+	struct netc_flower_key_tbl *key_tbl = NULL;
+	u32 ist_eid, sgclt_eid, isct_eid, sgit_eid;
+	u32 sgclt_entry_size = 0, sgclt_data_size;
+	struct ntmp_ist_entry *ist_entry = NULL;
+	struct flow_action_entry *action_entry;
+	struct netc_psfp_tbl_entries psfp_tbl;
+	struct isit_keye_data isit_keye = {0};
+	struct ntmp_isit_entry *isit_entry;
+	unsigned long cookie = f->cookie;
+	u32 ist_cfg = 0, num_gates;
+	int i, err, priority = -1;
+	u16 msdu = 0;
+
+	guard(mutex)(&priv->flower_lock);
+	if (netc_find_flower_rule_by_cookie(priv, port_id, cookie)) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Cannot add new rule with same cookie");
+		return -EINVAL;
+	}
+
+	rule = kzalloc(sizeof(*rule), GFP_KERNEL);
+	if (!rule)
+		return -ENOMEM;
+
+	rule->port_id = port_id;
+	rule->cookie = cookie;
+	rule->flower_type = FLOWER_TYPE_PSFP;
+
+	/* Find gate action entry and police action entry*/
+	flow_action_for_each(i, action_entry, &cls_rule->action)
+		if (action_entry->id == FLOW_ACTION_GATE)
+			gate_entry = action_entry;
+		else if (action_entry->id == FLOW_ACTION_POLICE)
+			police_entry = action_entry;
+
+	err = netc_psfp_gate_entry_validate(priv, gate_entry,
+					    &reused_gate_tbl, extack);
+	if (err)
+		return err;
+
+	if (police_entry) {
+		msdu = police_entry->police.mtu;
+		err = netc_police_entry_validate(priv, &cls_rule->action,
+						 police_entry, &reused_police_tbl,
+						 extack);
+		if (err)
+			goto clear_sgit_eid_bit;
+	}
+
+	err = netc_psfp_isit_keye_construct(cls_rule, port_id, &isit_keye,
+					    &priority, extack);
+	if (err)
+		goto clear_rpt_eid_bit;
+
+	err = netc_psfp_flower_key_validate(priv, &isit_keye, priority,
+					    &reused_key_tbl, extack);
+	if (err)
+		goto clear_rpt_eid_bit;
+
+	if (!reused_key_tbl) {
+		err = netc_add_psfp_key_tbl(priv, &key_tbl, &isit_keye, extack);
+		if (err)
+			goto clear_rpt_eid_bit;
+
+		isit_entry = key_tbl->isit_entry;
+		ist_entry = key_tbl->ist_entry;
+		ist_eid = ist_entry->entry_id;
+		ist_cfg = le32_to_cpu(ist_entry->cfge.cfg);
+	} else {
+		ist_eid = reused_key_tbl->ist_entry->entry_id;
+	}
+
+	if (!reused_police_tbl && police_entry) {
+		police_tbl = kzalloc(sizeof(*police_tbl), GFP_KERNEL);
+		if (!police_tbl) {
+			err = -ENOMEM;
+			goto free_psfp_key_tbl;
+		}
+
+		rpt_entry = kzalloc(sizeof(*rpt_entry), GFP_KERNEL);
+		if (!rpt_entry) {
+			err = -ENOMEM;
+			goto free_psfp_key_tbl;
+		}
+
+		netc_rpt_entry_config(police_entry, rpt_entry);
+		police_tbl->rpt_eid = police_entry->hw_index;
+		refcount_set(&police_tbl->refcount, 1);
+	}
+
+	sgit_eid = gate_entry->hw_index;
+	if (reused_gate_tbl)
+		goto config_isct;
+
+	gate_tbl = kzalloc(sizeof(*gate_tbl), GFP_KERNEL);
+	if (!gate_tbl) {
+		err = -ENOMEM;
+		goto free_psfp_key_tbl;
+	}
+
+	sgit_entry = kzalloc(sizeof(*sgit_entry), GFP_KERNEL);
+	if (!sgit_entry) {
+		err = -ENOMEM;
+		goto free_psfp_key_tbl;
+	}
+
+	sgit_entry->entry_id = sgit_eid;
+	num_gates = gate_entry->gate.num_entries;
+	sgclt_entry_size = 1 + DIV_ROUND_UP(num_gates, 2);
+	sgclt_eid = ntmp_lookup_free_words(priv->sgclt_word_bitmap,
+					   priv->caps.sgclt_num_words,
+					   sgclt_entry_size);
+	if (sgclt_eid == NTMP_NULL_ENTRY_ID) {
+		NL_SET_ERR_MSG_MOD(extack, "No Stream Gate Control List resource");
+		err = -ENOSPC;
+		goto free_psfp_key_tbl;
+	}
+
+	sgclt_data_size = struct_size(sgclt_entry, cfge.ge, num_gates);
+	sgclt_entry = kzalloc(sgclt_data_size, GFP_KERNEL);
+	if (!sgclt_entry) {
+		err = -ENOMEM;
+		goto clear_sgclt_eid_words;
+	}
+
+	sgclt_entry->entry_id = sgclt_eid;
+	gate_tbl->sgclt_eid = sgclt_eid;
+	gate_tbl->sgit_eid = sgit_eid;
+	refcount_set(&gate_tbl->refcount, 1);
+	netc_psfp_gate_entry_config(priv, gate_entry, sgit_entry, sgclt_entry);
+
+config_isct:
+	isct_eid = ntmp_lookup_free_eid(priv->isct_eid_bitmap,
+					priv->caps.isct_num_entries);
+	if (isct_eid == NTMP_NULL_ENTRY_ID) {
+		NL_SET_ERR_MSG_MOD(extack, "No available ISCT entry is found");
+		err = -ENOSPC;
+		goto clear_sgclt_eid_words;
+	}
+
+	isct_entry = kzalloc(sizeof(*isct_entry), GFP_KERNEL);
+	if (!isct_entry) {
+		err = -ENOMEM;
+		goto clear_isct_eid_bit;
+	}
+
+	isct_entry->entry_id = isct_eid;
+
+	/* Determine if an ingress stream filter entry is required */
+	if (priority >= 0) {
+		u16 isft_cfg = FIELD_PREP(ISFT_SDU_TYPE, SDU_TYPE_MPDU);
+
+		isft_entry = kzalloc(sizeof(*isft_entry), GFP_KERNEL);
+		if (!isft_entry) {
+			err = -ENOMEM;
+			goto clear_isct_eid_bit;
+		}
+
+		isft_entry->keye.is_eid = cpu_to_le32(ist_eid);
+		isft_entry->keye.pcp = FIELD_PREP(ISFT_PCP, priority);
+		isft_entry->cfge.msdu = cpu_to_le16(msdu);
+		isft_entry->cfge.isc_eid = cpu_to_le32(isct_eid);
+		isft_entry->cfge.sgi_eid = cpu_to_le32(sgit_eid);
+		isft_cfg |= ISFT_OSGI;
+
+		if (police_entry) {
+			isft_cfg |= ISFT_ORP;
+			isft_entry->cfge.rp_eid = cpu_to_le32(police_entry->hw_index);
+		}
+
+		isft_entry->cfge.cfg = cpu_to_le16(isft_cfg);
+
+		if (ist_entry)
+			ist_cfg |= IST_SFE; /* Enable stream filter */
+	} else if (ist_entry) {
+		ist_cfg |= IST_OSGI;
+		ist_entry->cfge.msdu = cpu_to_le16(msdu);
+		ist_entry->cfge.isc_eid = cpu_to_le32(isct_eid);
+		ist_entry->cfge.sgi_eid = cpu_to_le32(sgit_eid);
+
+		if (police_entry) {
+			ist_cfg |= IST_ORP;
+			ist_entry->cfge.rp_eid = cpu_to_le32(police_entry->hw_index);
+		}
+	}
+
+	if (ist_entry)
+		ist_entry->cfge.cfg = cpu_to_le32(ist_cfg);
+
+	psfp_tbl.ist_entry = ist_entry;
+	psfp_tbl.rpt_entry = rpt_entry;
+	psfp_tbl.isit_entry = isit_entry;
+	psfp_tbl.isft_entry = isft_entry;
+	psfp_tbl.sgit_entry = sgit_entry;
+	psfp_tbl.isct_entry = isct_entry;
+	psfp_tbl.sgclt_entry = sgclt_entry;
+	err = netc_psfp_set_related_tables(priv, &psfp_tbl);
+	if (err)
+		goto clear_isct_eid_bit;
+
+	rule->lastused = jiffies;
+	rule->isft_entry = no_free_ptr(isft_entry);
+
+	if (reused_key_tbl) {
+		rule->key_tbl = reused_key_tbl;
+		refcount_inc(&reused_key_tbl->refcount);
+	} else {
+		rule->key_tbl = key_tbl;
+	}
+
+	if (reused_gate_tbl) {
+		rule->gate_tbl = reused_gate_tbl;
+		refcount_inc(&reused_gate_tbl->refcount);
+	} else {
+		rule->gate_tbl = no_free_ptr(gate_tbl);
+	}
+
+	if (reused_police_tbl) {
+		rule->police_tbl = reused_police_tbl;
+		refcount_inc(&reused_police_tbl->refcount);
+	} else if (police_tbl) {
+		rule->police_tbl = no_free_ptr(police_tbl);
+	}
+
+	hlist_add_head(&no_free_ptr(rule)->node, &priv->flower_list);
+
+	return 0;
+
+clear_isct_eid_bit:
+	ntmp_clear_eid_bitmap(priv->isct_eid_bitmap, isct_eid);
+
+clear_sgclt_eid_words:
+	if (sgclt_entry_size)
+		ntmp_clear_words_bitmap(priv->sgclt_word_bitmap, sgclt_eid,
+					sgclt_entry_size);
+
+free_psfp_key_tbl:
+	netc_free_flower_key_tbl(priv, key_tbl);
+
+clear_rpt_eid_bit:
+	if (police_entry && !reused_police_tbl)
+		ntmp_clear_eid_bitmap(priv->rpt_eid_bitmap,
+				      police_entry->hw_index);
+
+clear_sgit_eid_bit:
+	if (!reused_gate_tbl)
+		ntmp_clear_eid_bitmap(priv->sgit_eid_bitmap,
+				      gate_entry->hw_index);
+
+	return err;
+}
+EXPORT_SYMBOL_GPL(netc_setup_psfp);
+
+void netc_free_flower_police_tbl(struct ntmp_priv *priv,
+				 struct netc_police_tbl *police_tbl)
+{
+	struct netc_cbdrs *cbdrs = &priv->cbdrs;
+
+	if (!police_tbl)
+		return;
+
+	if (refcount_dec_and_test(&police_tbl->refcount)) {
+		ntmp_rpt_delete_entry(cbdrs, police_tbl->rpt_eid);
+		ntmp_clear_eid_bitmap(priv->rpt_eid_bitmap,
+				      police_tbl->rpt_eid);
+		kfree(police_tbl);
+	}
+}
+EXPORT_SYMBOL_GPL(netc_free_flower_police_tbl);
+
+void netc_delete_psfp_flower_rule(struct ntmp_priv *priv,
+				  struct netc_flower_rule *rule)
+{
+	struct ntmp_isft_entry *isft_entry = rule->isft_entry;
+	struct netc_flower_key_tbl *key_tbl = rule->key_tbl;
+	struct netc_gate_tbl *gate_tbl = rule->gate_tbl;
+	struct netc_cbdrs *cbdrs = &priv->cbdrs;
+	struct ntmp_isit_entry *isit_entry;
+	struct ntmp_ist_entry *ist_entry;
+	u32 isct_eid;
+
+	if (refcount_dec_and_test(&key_tbl->refcount)) {
+		isit_entry = key_tbl->isit_entry;
+		ist_entry = key_tbl->ist_entry;
+		ntmp_isit_delete_entry(cbdrs, isit_entry->entry_id);
+		ntmp_ist_delete_entry(cbdrs, ist_entry->entry_id);
+
+		isct_eid = le32_to_cpu(ist_entry->cfge.isc_eid);
+		netc_free_flower_key_tbl(priv, key_tbl);
+	}
+
+	if (isft_entry) {
+		isct_eid = le32_to_cpu(isft_entry->cfge.isc_eid);
+		ntmp_isft_delete_entry(cbdrs, isft_entry->entry_id);
+		kfree(isft_entry);
+	}
+
+	ntmp_isct_operate_entry(cbdrs, isct_eid, NTMP_CMD_DELETE, NULL);
+	ntmp_clear_eid_bitmap(priv->isct_eid_bitmap, isct_eid);
+
+	if (gate_tbl && refcount_dec_and_test(&gate_tbl->refcount)) {
+		netc_delete_sgit_entry(priv, gate_tbl->sgit_eid);
+		ntmp_clear_eid_bitmap(priv->sgit_eid_bitmap, gate_tbl->sgit_eid);
+		kfree(gate_tbl);
+	}
+
+	netc_free_flower_police_tbl(priv, rule->police_tbl);
+
+	hlist_del(&rule->node);
+	kfree(rule);
+}
+EXPORT_SYMBOL_GPL(netc_delete_psfp_flower_rule);
+
+int netc_psfp_flower_stat(struct ntmp_priv *priv, struct netc_flower_rule *rule,
+			  u64 *byte_cnt, u64 *pkt_cnt, u64 *drop_cnt)
+{
+	struct ntmp_ist_entry *ist_entry = rule->key_tbl->ist_entry;
+	struct ntmp_isft_entry *isft_entry = rule->isft_entry;
+	struct isct_stse_data stse = {0};
+	u32 isct_eid, sg_drop_cnt;
+	int err;
+
+	if (isft_entry)
+		isct_eid = le32_to_cpu(isft_entry->cfge.isc_eid);
+	else
+		isct_eid = le32_to_cpu(ist_entry->cfge.isc_eid);
+
+	/* Query, followed by update will reset statistics */
+	err = ntmp_isct_operate_entry(&priv->cbdrs, isct_eid,
+				      NTMP_CMD_QU, &stse);
+	if (err)
+		return err;
+
+	sg_drop_cnt = le32_to_cpu(stse.sg_drop_count);
+	/* Workaround for ERR052134 on i.MX95 platform */
+	if (priv->errata & NTMP_ERR052134) {
+		u32 tmp;
+
+		sg_drop_cnt >>= 9;
+
+		tmp = le32_to_cpu(stse.resv3) & 0x1ff;
+		sg_drop_cnt |= (tmp << 23);
+	}
+
+	*pkt_cnt = le32_to_cpu(stse.rx_count);
+	*drop_cnt = le32_to_cpu(stse.msdu_drop_count) + sg_drop_cnt +
+		    le32_to_cpu(stse.policer_drop_count);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(netc_psfp_flower_stat);
+
+int netc_setup_taprio(struct ntmp_priv *priv, u32 entry_id,
+		      struct tc_taprio_qopt_offload *f)
+{
+	struct tgst_cfge_data *cfge __free(kfree) = NULL;
+	struct netlink_ext_ack *extack = f->extack;
+	u64 base_time = f->base_time;
+	u64 max_cycle_time;
+	int i, err;
+	u32 size;
+
+	if (!priv->get_tgst_free_words) {
+		NL_SET_ERR_MSG_MOD(extack, "get_tgst_free_words() is undefined");
+		return -EINVAL;
+	}
+
+	max_cycle_time = f->cycle_time + f->cycle_time_extension;
+	if (max_cycle_time > U32_MAX) {
+		NL_SET_ERR_MSG_MOD(extack, "Max cycle time exceeds U32_MAX");
+		return -EINVAL;
+	}
+
+	/* Delete the pending administrative control list if it exists */
+	err = ntmp_tgst_delete_admin_gate_list(&priv->cbdrs, entry_id);
+	if (err)
+		return err;
+
+	if (f->num_entries > priv->get_tgst_free_words(priv)) {
+		NL_SET_ERR_MSG_MOD(extack, "TGST doesn't have enough free words");
+		return -EINVAL;
+	}
+
+	size = struct_size(cfge, ge, f->num_entries);
+	cfge = kzalloc(size, GFP_KERNEL);
+	if (!cfge)
+		return -ENOMEM;
+
+	if (priv->adjust_base_time)
+		base_time = priv->adjust_base_time(priv, base_time, f->cycle_time);
+
+	cfge->admin_bt = cpu_to_le64(base_time);
+	cfge->admin_ct = cpu_to_le32(f->cycle_time);
+	cfge->admin_ct_ext = cpu_to_le32(f->cycle_time_extension);
+	cfge->admin_cl_len = cpu_to_le16(f->num_entries);
+	for (i = 0; i < f->num_entries; i++) {
+		struct tc_taprio_sched_entry *temp_entry = &f->entries[i];
+
+		cfge->ge[i].tc_state = temp_entry->gate_mask;
+		cfge->ge[i].interval = cpu_to_le32(temp_entry->interval);
+	}
+
+	err = ntmp_tgst_update_admin_gate_list(&priv->cbdrs, entry_id, cfge);
+	if (err) {
+		NL_SET_ERR_MSG_MOD(extack, "Update control list failed");
+		return err;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(netc_setup_taprio);
+
+int netc_ipft_keye_construct(struct flow_rule *rule, int port_id,
+			     u16 prio, struct ipft_keye_data *keye,
+			     struct netlink_ext_ack *extack)
+{
+	u16 frm_attr_flags = 0, src_port = 0;
+	u16 vlan_tci, vlan_tci_mask;
+	__be16 eth_type = 0;
+
+	keye->precedence = cpu_to_le16(prio);
+
+	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ETH_ADDRS)) {
+		struct flow_match_eth_addrs match = {0};
+
+		flow_rule_match_eth_addrs(rule, &match);
+		ether_addr_copy(keye->dmac, match.key->dst);
+		ether_addr_copy(keye->dmac_mask, match.mask->dst);
+		ether_addr_copy(keye->smac, match.key->src);
+		ether_addr_copy(keye->smac_mask, match.mask->src);
+	}
+
+	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_VLAN)) {
+		struct flow_match_vlan match = {0};
+
+		flow_rule_match_vlan(rule, &match);
+		vlan_tci = match.key->vlan_id | match.key->vlan_dei << 12 |
+			   match.key->vlan_priority << VLAN_PRIO_SHIFT;
+		vlan_tci_mask = match.mask->vlan_id | match.mask->vlan_dei << 12 |
+				match.mask->vlan_priority << VLAN_PRIO_SHIFT;
+		keye->outer_vlan_tci = htons(vlan_tci);
+		keye->outer_vlan_tci_mask = htons(vlan_tci_mask);
+		frm_attr_flags |= IPFT_FAF_OVLAN;
+	}
+
+	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_CVLAN)) {
+		struct flow_match_vlan match = {0};
+
+		flow_rule_match_vlan(rule, &match);
+		vlan_tci = match.key->vlan_id | match.key->vlan_dei << 12 |
+			   match.key->vlan_priority << VLAN_PRIO_SHIFT;
+		vlan_tci_mask = match.mask->vlan_id | match.mask->vlan_dei << 12 |
+				match.mask->vlan_priority << VLAN_PRIO_SHIFT;
+		keye->inner_vlan_tci = htons(vlan_tci);
+		keye->inner_vlan_tci_mask = htons(vlan_tci_mask);
+		frm_attr_flags |= IPFT_FAF_IVLAN;
+	}
+
+	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_BASIC)) {
+		struct flow_match_basic match = {0};
+
+		flow_rule_match_basic(rule, &match);
+		if (match.mask->n_proto && ntohs(match.mask->n_proto) != 0xffff) {
+			NL_SET_ERR_MSG_MOD(extack, "Ether type mask must be 0xFFFF");
+			return -EINVAL;
+		}
+
+		eth_type = match.key->n_proto;
+		keye->ethertype = match.key->n_proto;
+		keye->ethertype_mask = match.mask->n_proto;
+		keye->ip_protocol = match.key->ip_proto;
+		keye->ip_protocol_mask = match.mask->ip_proto;
+		if (match.mask->ip_proto == 0xff) {
+			if (match.key->ip_proto == IPPROTO_TCP)
+				frm_attr_flags |= FIELD_PREP(IPFT_FAF_L4_CODE,
+							     IPFT_FAF_TCP_HDR);
+			else if (match.key->ip_proto == IPPROTO_UDP)
+				frm_attr_flags |= FIELD_PREP(IPFT_FAF_L4_CODE,
+							     IPFT_FAF_UDP_HDR);
+		}
+	}
+
+	if (ntohs(eth_type) == ETH_P_IP &&
+	    flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_IPV4_ADDRS)) {
+		struct flow_match_ipv4_addrs match = {0};
+
+		flow_rule_match_ipv4_addrs(rule, &match);
+		keye->ip_dst[3] = match.key->dst;
+		keye->ip_dst_mask[3] = match.mask->dst;
+		keye->ip_src[3] = match.key->src;
+		keye->ip_src_mask[3] = match.mask->src;
+		frm_attr_flags |= IPFT_FAF_IP_HDR;
+	}
+
+	if (ntohs(eth_type) == ETH_P_IPV6 &&
+	    flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_IPV6_ADDRS)) {
+		struct flow_match_ipv6_addrs match = {0};
+
+		flow_rule_match_ipv6_addrs(rule, &match);
+		memcpy(keye->ip_dst, &match.key->dst, sizeof(keye->ip_dst));
+		memcpy(keye->ip_dst_mask, &match.mask->dst, sizeof(keye->ip_dst_mask));
+		memcpy(keye->ip_src, &match.key->src, sizeof(keye->ip_src));
+		memcpy(keye->ip_src_mask, &match.mask->src, sizeof(keye->ip_src_mask));
+		frm_attr_flags |= IPFT_FAF_IP_HDR | IPFT_FAF_IP_VER6;
+	}
+
+	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_PORTS)) {
+		struct flow_match_ports match = {0};
+
+		flow_rule_match_ports(rule, &match);
+		keye->l4_src_port = match.key->src;
+		keye->l4_src_port_mask = match.mask->src;
+		keye->l4_dst_port = match.key->dst;
+		keye->l4_dst_port_mask = match.mask->dst;
+	}
+
+	keye->frm_attr_flags = cpu_to_le16(frm_attr_flags);
+	keye->frm_attr_flags_mask = keye->frm_attr_flags;
+
+	/* For ENETC, the port_id must be less than 0 */
+	if (port_id >= 0) {
+		src_port |= FIELD_PREP(IPFT_SRC_PORT, port_id);
+		src_port |= IPFT_SRC_PORT_MASK;
+		keye->src_port = cpu_to_le16(src_port);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(netc_ipft_keye_construct);
+
+static int netc_add_police_key_tbl(struct ntmp_priv *priv, u32 rpt_eid,
+				   struct netc_flower_key_tbl **key_tbl,
+				   struct ipft_keye_data *ipft_key)
+{
+	struct netc_flower_key_tbl *new_tbl __free(kfree);
+	struct ntmp_ipft_entry *ipft_entry __free(kfree);
+	struct ipft_cfge_data *ipft_cfge;
+	u32 cfg;
+
+	new_tbl = kzalloc(sizeof(*new_tbl), GFP_KERNEL);
+	if (!new_tbl)
+		return -ENOMEM;
+
+	ipft_entry = kzalloc(sizeof(*ipft_entry), GFP_KERNEL);
+	if (!ipft_entry)
+		return -ENOMEM;
+
+	ipft_cfge = &ipft_entry->cfge;
+	ipft_entry->keye = *ipft_key;
+
+	cfg = FIELD_PREP(IPFT_FLTFA, IPFT_FLTFA_PERMIT);
+	cfg |= FIELD_PREP(IPFT_FLTA, IPFT_FLTA_RP);
+	ipft_cfge->cfg = cpu_to_le32(cfg);
+	ipft_cfge->flta_tgt = cpu_to_le32(rpt_eid);
+
+	new_tbl->tbl_type = FLOWER_KEY_TBL_IPFT;
+	refcount_set(&new_tbl->refcount, 1);
+	new_tbl->ipft_entry = no_free_ptr(ipft_entry);
+	*key_tbl = no_free_ptr(new_tbl);
+
+	return 0;
+}
+
+static int netc_set_police_tables(struct ntmp_priv *priv,
+				  struct ntmp_ipft_entry *ipft_entry,
+				  struct ntmp_rpt_entry *rpt_entry)
+{
+	struct netc_cbdrs *cbdrs = &priv->cbdrs;
+	int err;
+
+	if (rpt_entry) {
+		err = ntmp_rpt_add_or_update_entry(cbdrs, rpt_entry);
+		if (err)
+			return err;
+	}
+
+	err = ntmp_ipft_add_entry(cbdrs, &ipft_entry->entry_id, ipft_entry);
+	if (err)
+		goto delete_rpt_entry;
+
+	return 0;
+
+delete_rpt_entry:
+	if (rpt_entry)
+		ntmp_rpt_delete_entry(cbdrs, rpt_entry->entry_id);
+
+	return err;
+}
+
+int netc_setup_police(struct ntmp_priv *priv, int port_id,
+		      struct flow_cls_offload *f)
+{
+	struct flow_rule *cls_rule = flow_cls_offload_flow_rule(f);
+	struct netc_police_tbl *police_tbl __free(kfree) = NULL;
+	struct ntmp_rpt_entry *rpt_entry __free(kfree) = NULL;
+	struct netc_flower_rule *rule __free(kfree) = NULL;
+	struct netlink_ext_ack *extack = f->common.extack;
+	struct netc_police_tbl *reused_police_tbl = NULL;
+	struct ipft_keye_data *ipft_keye __free(kfree);
+	struct flow_action_entry *police_act = NULL;
+	struct netc_flower_key_tbl *key_tbl = NULL;
+	struct flow_action_entry *action_entry;
+	struct ntmp_ipft_entry *ipft_entry;
+	struct netc_flower_rule *old_rule;
+	unsigned long cookie = f->cookie;
+	u16 prio = f->common.prio;
+	int i, err;
+
+	guard(mutex)(&priv->flower_lock);
+	if (netc_find_flower_rule_by_cookie(priv, port_id, cookie)) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Cannot add new rule with same cookie");
+		return -EINVAL;
+	}
+
+	rule = kzalloc(sizeof(*rule), GFP_KERNEL);
+	if (!rule)
+		return -ENOMEM;
+
+	rule->port_id = port_id;
+	rule->cookie = cookie;
+	rule->flower_type = FLOWER_TYPE_POLICE;
+
+	flow_action_for_each(i, action_entry, &cls_rule->action)
+		if (action_entry->id == FLOW_ACTION_POLICE)
+			police_act = action_entry;
+
+	if (!police_act) {
+		NL_SET_ERR_MSG_MOD(extack, "No police action");
+		return -EINVAL;
+	}
+
+	ipft_keye = kzalloc(sizeof(*ipft_keye), GFP_KERNEL);
+	if (!ipft_keye)
+		return -ENOMEM;
+
+	err = netc_ipft_keye_construct(cls_rule, port_id, prio,
+				       ipft_keye, extack);
+	if (err)
+		return err;
+
+	old_rule = netc_find_flower_rule_by_key(priv, FLOWER_KEY_TBL_IPFT,
+						ipft_keye);
+	if (old_rule) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "The IPFT key has been used by existing rule");
+		return -EINVAL;
+	}
+
+	err = netc_police_entry_validate(priv, &cls_rule->action, police_act,
+					 &reused_police_tbl, extack);
+	if (err)
+		return err;
+
+	if (!reused_police_tbl) {
+		police_tbl = kzalloc(sizeof(*police_tbl), GFP_KERNEL);
+		if (!police_tbl) {
+			err = -ENOMEM;
+			goto clear_rpt_eid_bit;
+		}
+
+		rpt_entry = kzalloc(sizeof(*rpt_entry), GFP_KERNEL);
+		if (!rpt_entry) {
+			err = -ENOMEM;
+			goto clear_rpt_eid_bit;
+		}
+
+		police_tbl->rpt_eid = police_act->hw_index;
+		refcount_set(&police_tbl->refcount, 1);
+		netc_rpt_entry_config(police_act, rpt_entry);
+	}
+
+	err = netc_add_police_key_tbl(priv, police_act->hw_index, &key_tbl,
+				      ipft_keye);
+	if (err) {
+		NL_SET_ERR_MSG_MOD(extack, "Failed to add police key table");
+		goto clear_rpt_eid_bit;
+	}
+
+	ipft_entry = key_tbl->ipft_entry;
+	err = netc_set_police_tables(priv, ipft_entry, rpt_entry);
+	if (err) {
+		NL_SET_ERR_MSG_MOD(extack, "Failed to add police table entries");
+		goto clear_rpt_eid_bit;
+	}
+
+	rule->lastused = jiffies;
+	rule->key_tbl = key_tbl;
+
+	if (reused_police_tbl) {
+		rule->police_tbl = reused_police_tbl;
+		refcount_inc(&reused_police_tbl->refcount);
+	} else if (police_tbl) {
+		rule->police_tbl = no_free_ptr(police_tbl);
+	}
+
+	hlist_add_head(&no_free_ptr(rule)->node, &priv->flower_list);
+
+	return 0;
+
+clear_rpt_eid_bit:
+	if (!reused_police_tbl)
+		ntmp_clear_eid_bitmap(priv->rpt_eid_bitmap, police_act->hw_index);
+
+	return err;
+}
+EXPORT_SYMBOL_GPL(netc_setup_police);
+
+void netc_delete_police_flower_rule(struct ntmp_priv *priv,
+				    struct netc_flower_rule *rule)
+{
+	struct netc_police_tbl *police_tbl = rule->police_tbl;
+	struct netc_flower_key_tbl *key_tbl = rule->key_tbl;
+	struct netc_cbdrs *cbdrs = &priv->cbdrs;
+	struct ntmp_ipft_entry *ipft_entry;
+
+	ipft_entry = key_tbl->ipft_entry;
+	ntmp_ipft_delete_entry(cbdrs, ipft_entry->entry_id);
+
+	netc_free_flower_police_tbl(priv, police_tbl);
+	netc_free_flower_key_tbl(priv, key_tbl);
+
+	hlist_del(&rule->node);
+	kfree(rule);
+}
+EXPORT_SYMBOL_GPL(netc_delete_police_flower_rule);
+
+int netc_police_flower_stat(struct ntmp_priv *priv,
+			    struct netc_flower_rule *rule,
+			    u64 *pkt_cnt)
+{
+	struct ntmp_ipft_entry *ipft_entry = rule->key_tbl->ipft_entry;
+	struct ntmp_ipft_entry *ipft_query __free(kfree) = NULL;
+	int err;
+
+	ipft_query = kzalloc(sizeof(*ipft_query), GFP_KERNEL);
+	if (!ipft_query)
+		return -ENOMEM;
+
+	err = ntmp_ipft_query_entry(&priv->cbdrs, ipft_entry->entry_id,
+				    true, ipft_query);
+	if (err)
+		return err;
+
+	*pkt_cnt = le64_to_cpu(ipft_query->match_count);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(netc_police_flower_stat);
diff --git a/devices/enetc/ntmp.c b/devices/enetc/ntmp.c
new file mode 100755
index 0000000..cff9269
--- /dev/null
+++ b/devices/enetc/ntmp.c
@@ -0,0 +1,2690 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
+/*
+ * NETC NTMP (NETC Table Management Protocol) 2.0 driver
+ * Copyright 2023 NXP
+ */
+#include <linux/iopoll.h>
+#include <linux/fsl/netc_global.h>
+#include <linux/fsl/netc_lib.h>
+
+#include "ntmp_private.h"
+
+#define NETC_CBDR_TIMEOUT		1000 /* us */
+#define NETC_CBDR_MR_EN			BIT(31)
+
+#define NTMP_BASE_ADDR_ALIGN		128
+#define NTMP_DATA_ADDR_ALIGN		32
+
+/* Define NTMP Table ID */
+#define NTMP_MAFT_ID			1
+#define NTMP_VAFT_ID			2
+#define NTMP_RSST_ID			3
+#define NTMP_RFST_ID			4
+#define NTMP_TGST_ID			5
+#define NTMP_RPT_ID			10
+#define NTMP_IPFT_ID			13
+#define NTMP_FDBT_ID			15
+#define NTMP_VFT_ID			18
+#define NTMP_ISIT_ID			30
+#define NTMP_IST_ID			31
+#define NTMP_ISFT_ID			32
+#define NTMP_ETT_ID			33
+#define NTMP_ESRT_ID			35
+#define NTMP_SGIT_ID			36
+#define NTMP_SGCLT_ID			37
+#define NTMP_ISCT_ID			38
+#define NTMP_ECT_ID			39
+#define NTMP_FMT_ID			40
+#define NTMP_BPT_ID			41
+#define NTMP_SBPT_ID			42
+#define NTMP_FMDT_ID			44
+
+/* Generic Update Actions for most tables */
+#define NTMP_GEN_UA_CFGEU		BIT(0)
+#define NTMP_GEN_UA_STSEU		BIT(1)
+
+/* Update Actions for specific tables */
+#define SGIT_UA_ACFGEU			BIT(0)
+#define SGIT_UA_CFGEU			BIT(1)
+#define SGIT_UA_SGISEU			BIT(2)
+#define RPT_UA_FEEU			BIT(1)
+#define RPT_UA_PSEU			BIT(2)
+#define RPT_UA_STSEU			BIT(3)
+#define FDBT_UA_ACTEU			BIT(1)
+#define ESRT_UA_SRSEU			BIT(2)
+#define ECT_UA_STSEU			BIT(0)
+#define BPT_UA_BPSEU			BIT(1)
+#define SBPT_UA_BPSEU			BIT(1)
+
+/* Quary Action: 0: Full query, 1: Only query entry ID */
+#define NTMP_QA_ENTRY_ID		1
+
+#define NTMP_ENTRY_ID_SIZE		4
+#define RSST_ENTRY_NUM			64
+#define RSST_STSE_DATA_SIZE(n)		((n) * 8)
+#define RSST_CFGE_DATA_SIZE(n)		(n)
+#define FMDT_DATA_LEN_ALIGN		4
+
+int netc_setup_cbdr(struct device *dev, int cbd_num,
+		    struct netc_cbdr_regs *regs,
+		    struct netc_cbdr *cbdr)
+{
+	int size;
+
+	size = cbd_num * sizeof(union netc_cbd) +
+	       NTMP_BASE_ADDR_ALIGN;
+
+	cbdr->addr_base = dma_alloc_coherent(dev, size, &cbdr->dma_base,
+					     GFP_KERNEL);
+	if (!cbdr->addr_base)
+		return -ENOMEM;
+
+	cbdr->dma_size = size;
+	cbdr->bd_num = cbd_num;
+	cbdr->regs = *regs;
+
+	/* The base address of the Control BD Ring must be 128 bytes aligned */
+	cbdr->dma_base_align =  ALIGN(cbdr->dma_base,
+				      NTMP_BASE_ADDR_ALIGN);
+	cbdr->addr_base_align = PTR_ALIGN(cbdr->addr_base,
+					  NTMP_BASE_ADDR_ALIGN);
+
+	cbdr->next_to_clean = 0;
+	cbdr->next_to_use = 0;
+	spin_lock_init(&cbdr->ring_lock);
+
+	/* Step 1: Configure the base address of the Control BD Ring */
+	netc_write(cbdr->regs.bar0, lower_32_bits(cbdr->dma_base_align));
+	netc_write(cbdr->regs.bar1, upper_32_bits(cbdr->dma_base_align));
+
+	/* Step 2: Configure the producer index register */
+	netc_write(cbdr->regs.pir, cbdr->next_to_clean);
+
+	/* Step 3: Configure the consumer index register */
+	netc_write(cbdr->regs.cir, cbdr->next_to_use);
+
+	/* Step4: Configure the number of BDs of the Control BD Ring */
+	netc_write(cbdr->regs.lenr, cbdr->bd_num);
+
+	/* Step 5: Enable the Control BD Ring */
+	netc_write(cbdr->regs.mr, NETC_CBDR_MR_EN);
+
+	return 0;
+}
+
+void netc_teardown_cbdr(struct device *dev, struct netc_cbdr *cbdr)
+{
+	/* Disable the Control BD Ring */
+	netc_write(cbdr->regs.mr, 0);
+
+	dma_free_coherent(dev, cbdr->dma_size, cbdr->addr_base, cbdr->dma_base);
+
+	memset(cbdr, 0, sizeof(*cbdr));
+}
+
+static int netc_get_free_cbd_num(struct netc_cbdr *cbdr)
+{
+	return (cbdr->next_to_clean - cbdr->next_to_use - 1 + cbdr->bd_num) %
+		cbdr->bd_num;
+}
+
+static union netc_cbd *netc_get_cbd(struct netc_cbdr *cbdr, int index)
+{
+	return &((union netc_cbd *)(cbdr->addr_base_align))[index];
+}
+
+static void netc_clean_cbdr(struct netc_cbdr *cbdr)
+{
+	union netc_cbd *cbd;
+	int i;
+
+	i = cbdr->next_to_clean;
+	while (netc_read(cbdr->regs.cir) != i) {
+		cbd = netc_get_cbd(cbdr, i);
+		memset(cbd, 0, sizeof(*cbd));
+		i = (i + 1) % cbdr->bd_num;
+	}
+
+	cbdr->next_to_clean = i;
+}
+
+static struct netc_cbdr *netc_select_cbdr(struct netc_cbdrs *cbdrs)
+{
+	int cpu, i;
+
+	for (i = 0; i < cbdrs->cbdr_num; i++) {
+		if (spin_is_locked(&cbdrs->ring[i].ring_lock))
+			continue;
+
+		return &cbdrs->ring[i];
+	}
+
+	/* If all the command BDRs are busy now, we select
+	 * one of them, but need to wait for a while to use.
+	 */
+	cpu = smp_processor_id();
+
+	return &cbdrs->ring[cpu % cbdrs->cbdr_num];
+}
+
+static int netc_xmit_ntmp_cmd(struct netc_cbdrs *cbdrs, union netc_cbd *cbd)
+{
+	union netc_cbd *ring_cbd;
+	struct netc_cbdr *cbdr;
+	int i, err;
+	u16 status;
+	u32 val;
+
+	if (cbdrs->cbdr_num == 1)
+		cbdr = cbdrs->ring;
+	else
+		cbdr = netc_select_cbdr(cbdrs);
+
+	if (unlikely(!cbdr->addr_base))
+		return -EFAULT;
+
+	spin_lock_bh(&cbdr->ring_lock);
+
+	if (unlikely(!netc_get_free_cbd_num(cbdr)))
+		netc_clean_cbdr(cbdr);
+
+	i = cbdr->next_to_use;
+	ring_cbd = netc_get_cbd(cbdr, i);
+
+	/* Copy command BD to the ring */
+	*ring_cbd = *cbd;
+	/* Update producer index of both software and hardware */
+	i = (i + 1) % cbdr->bd_num;
+	cbdr->next_to_use = i;
+	dma_wmb();
+	netc_write(cbdr->regs.pir, i);
+
+	err = read_poll_timeout_atomic(netc_read, val, val == i,
+				       10, NETC_CBDR_TIMEOUT, true,
+				       cbdr->regs.cir);
+	if (unlikely(err)) {
+		err = -EBUSY;
+		goto err_unlock;
+	}
+
+	dma_rmb();
+	/* Get the writeback Command BD, because the caller may need
+	 * to check some other fileds of the response header.
+	 */
+	*cbd = *ring_cbd;
+
+	/* Check the writeback error status */
+	status = le16_to_cpu(cbd->resp_hdr.error_rr) & NTMP_RESP_ERROR;
+	if (unlikely(status)) {
+		dev_err(cbdrs->dma_dev, "Command BD error: 0x%04x\n", status);
+		err = -EIO;
+	}
+
+	netc_clean_cbdr(cbdr);
+	dma_wmb();
+
+err_unlock:
+	spin_unlock_bh(&cbdr->ring_lock);
+
+	return err;
+}
+
+static void *ntmp_alloc_data_mem(struct device *dev, int size,
+				 dma_addr_t *dma, void **data_align)
+{
+	void *data;
+
+	data = dma_alloc_coherent(dev, size + NTMP_DATA_ADDR_ALIGN,
+				  dma, GFP_ATOMIC);
+	if (!data) {
+		dev_err(dev, "NTMP alloc data memory failed!\n");
+		return NULL;
+	}
+
+	*data_align = PTR_ALIGN(data, NTMP_DATA_ADDR_ALIGN);
+
+	return data;
+}
+
+static void ntmp_free_data_mem(struct device *dev, int size,
+			       void *data, dma_addr_t dma)
+{
+	dma_free_coherent(dev, size + NTMP_DATA_ADDR_ALIGN, data, dma);
+}
+
+static void ntmp_fill_request_headr(union netc_cbd *cbd, dma_addr_t dma,
+				    int len, int table_id, int cmd,
+				    int access_method)
+{
+	dma_addr_t dma_align;
+
+	memset(cbd, 0, sizeof(*cbd));
+	dma_align = ALIGN(dma, NTMP_DATA_ADDR_ALIGN);
+	cbd->req_hdr.addr = cpu_to_le64(dma_align);
+	cbd->req_hdr.len = cpu_to_le32(len);
+	cbd->req_hdr.cmd = cmd;
+	cbd->req_hdr.access_method = FIELD_PREP(NTMP_ACCESS_METHOD,
+						access_method);
+	cbd->req_hdr.table_id = table_id;
+	cbd->req_hdr.ver_cci_rr = FIELD_PREP(NTMP_HDR_VERSION,
+					     NTMP_HDR_VER2);
+	/* For NTMP version 2.0 or later version */
+	cbd->req_hdr.npf = cpu_to_le32(NTMP_NPF);
+}
+
+static void ntmp_fill_crd(struct common_req_data *crd,
+			  u8 tblv, u8 qa, u16 ua)
+{
+	crd->update_act = cpu_to_le16(ua);
+	crd->tblv_qact = NTMP_TBLV_QACT(tblv, qa);
+}
+
+static void ntmp_fill_crd_eid(struct ntmp_req_by_eid *rbe, u8 tblv,
+			      u8 qa, u16 ua, u32 entry_id)
+{
+	ntmp_fill_crd(&rbe->crd, tblv, qa, ua);
+	rbe->entry_id = cpu_to_le32(entry_id);
+}
+
+u32 ntmp_lookup_free_eid(unsigned long *bitmap, u32 bitmap_size)
+{
+	u32 entry_id;
+
+	if (!bitmap)
+		return NTMP_NULL_ENTRY_ID;
+
+	entry_id = find_first_zero_bit(bitmap, bitmap_size);
+	if (entry_id == bitmap_size)
+		return NTMP_NULL_ENTRY_ID;
+
+	/* Set the bit once we found it */
+	set_bit(entry_id, bitmap);
+
+	return entry_id;
+}
+
+void ntmp_clear_eid_bitmap(unsigned long *bitmap, u32 entry_id)
+{
+	if (!bitmap || entry_id == NTMP_NULL_ENTRY_ID)
+		return;
+
+	clear_bit(entry_id, bitmap);
+}
+
+u32 ntmp_lookup_free_words(unsigned long *bitmap, u32 bitmap_size,
+			   u32 num_words)
+{
+	u32 entry_id, next_eid, size;
+
+	if (!bitmap)
+		return NTMP_NULL_ENTRY_ID;
+
+	do {
+		entry_id = find_first_zero_bit(bitmap, bitmap_size);
+		if (entry_id == bitmap_size)
+			return NTMP_NULL_ENTRY_ID;
+
+		next_eid = find_next_bit(bitmap, bitmap_size, entry_id + 1);
+		size = next_eid - entry_id;
+	} while (size < num_words && next_eid != bitmap_size);
+
+	if (size < num_words)
+		return NTMP_NULL_ENTRY_ID;
+
+	bitmap_set(bitmap, entry_id, num_words);
+
+	return entry_id;
+}
+
+void ntmp_clear_words_bitmap(unsigned long *bitmap, u32 entry_id,
+			     u32 num_words)
+{
+	if (!bitmap || entry_id == NTMP_NULL_ENTRY_ID)
+		return;
+
+	bitmap_clear(bitmap, entry_id, num_words);
+}
+
+static int ntmp_delete_entry_by_id(struct netc_cbdrs *cbdrs, int tbl_id, u8 tbl_ver,
+				   u32 entry_id, u32 req_len, u32 resp_len)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct ntmp_req_by_eid *req;
+	union netc_cbd cbd;
+	u32 len, dma_len;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return 0;
+
+	/* If the req_len is 0, indicates the requested length it the
+	 * standard length.
+	 */
+	if (!req_len)
+		req_len = sizeof(*req);
+
+	dma_len = req_len >= resp_len ? req_len : resp_len;
+	tmp = ntmp_alloc_data_mem(dev, dma_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(req, tbl_ver, 0, 0, entry_id);
+	len = NTMP_LEN(req_len, resp_len);
+	ntmp_fill_request_headr(&cbd, dma, len, tbl_id,
+				NTMP_CMD_DELETE, NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Delete table (id: %d) entry failed: %d!",
+			tbl_id, err);
+
+	ntmp_free_data_mem(dev, dma_len, tmp, dma);
+
+	return err;
+}
+
+static int ntmp_query_entry_by_id(struct netc_cbdrs *cbdrs, int tbl_id,
+				  u32 len, struct ntmp_req_by_eid *req,
+				  dma_addr_t *dma, bool compare_eid)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct common_resp_query *resp;
+	int cmd = NTMP_CMD_QUERY;
+	union netc_cbd cbd;
+	u32 entry_id;
+	int err;
+
+	entry_id = le32_to_cpu(req->entry_id);
+	if (le16_to_cpu(req->crd.update_act))
+		cmd = NTMP_CMD_QU;
+
+	/* Request header */
+	ntmp_fill_request_headr(&cbd, *dma, len, tbl_id,
+				cmd, NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err) {
+		dev_err(dev, "Query table (id: %d) entry failed: %d\n",
+			tbl_id, err);
+		return err;
+	}
+
+	/* For a few tables, the first field of its response data
+	 * is not entry_id or not the entry_id of current table.
+	 * So return directly here.
+	 */
+	if (!compare_eid)
+		return 0;
+
+	resp = (struct common_resp_query *)req;
+	if (unlikely(le32_to_cpu(resp->entry_id) != entry_id)) {
+		dev_err(dev, "Table (id: %d) query EID:0x%0x, response EID:0x%x\n",
+			tbl_id, entry_id, le32_to_cpu(resp->entry_id));
+		return -EIO;
+	}
+
+	return 0;
+}
+
+int ntmp_maft_add_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			struct maft_entry_data *data)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct maft_req_add *req;
+	union netc_cbd cbd;
+	u32 len, req_len;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	req_len = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, req_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Set mac address filter table request data buffer */
+	ntmp_fill_crd_eid(&req->rbe, cbdrs->tbl.maft_ver, 0, 0, entry_id);
+	req->keye = data->keye;
+	req->cfge = data->cfge;
+
+	len = NTMP_LEN(req_len, 0);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_MAFT_ID,
+				NTMP_CMD_ADD, NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Add MAFT entry failed (%d)!", err);
+
+	ntmp_free_data_mem(dev, req_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_maft_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			  struct maft_entry_data *data)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct maft_resp_query *resp;
+	u32 resp_len = sizeof(*resp);
+	struct ntmp_req_by_eid *req;
+	u32 req_len = sizeof(*req);
+	void *tmp = NULL;
+	dma_addr_t dma;
+	u32 dma_len;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return -EINVAL;
+
+	dma_len = max_t(u32, req_len, resp_len);
+	tmp = ntmp_alloc_data_mem(dev, dma_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(req, cbdrs->tbl.maft_ver, 0, 0, entry_id);
+	err = ntmp_query_entry_by_id(cbdrs, NTMP_MAFT_ID,
+				     NTMP_LEN(req_len, resp_len),
+				     req, &dma, true);
+	if (err)
+		goto end;
+
+	resp = (struct maft_resp_query *)req;
+	data->keye = resp->keye;
+	data->cfge = resp->cfge;
+
+end:
+	ntmp_free_data_mem(dev, dma_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_maft_delete_entry(struct netc_cbdrs *cbdrs, u32 entry_id)
+{
+	return ntmp_delete_entry_by_id(cbdrs, NTMP_MAFT_ID, cbdrs->tbl.maft_ver,
+				       entry_id, 0, 0);
+}
+
+int ntmp_vaft_add_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			struct vaft_entry_data *data)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct vaft_req_add *req;
+	union netc_cbd cbd;
+	u32 len, data_size;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	data_size = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Set VLAN address filter table request data buffer */
+	ntmp_fill_crd_eid(&req->rbe, cbdrs->tbl.vaft_ver, 0, 0, entry_id);
+	req->keye = data->keye;
+	req->cfge = data->cfge;
+
+	len = NTMP_LEN(data_size, 0);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_VAFT_ID,
+				NTMP_CMD_ADD, NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Add VAFT entry failed (%d)!", err);
+
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+int ntmp_vaft_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			  struct vaft_entry_data *data)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct vaft_resp_query *resp;
+	u32 resp_len = sizeof(*resp);
+	struct ntmp_req_by_eid *req;
+	u32 req_len = sizeof(*req);
+	void *tmp = NULL;
+	dma_addr_t dma;
+	u32 dma_len;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return -EINVAL;
+
+	dma_len = max_t(u32, req_len, resp_len);
+	tmp = ntmp_alloc_data_mem(dev, dma_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(req, cbdrs->tbl.vaft_ver, 0, 0, entry_id);
+	err = ntmp_query_entry_by_id(cbdrs, NTMP_VAFT_ID,
+				     NTMP_LEN(req_len, resp_len),
+				     req, &dma, true);
+	if (err)
+		goto end;
+
+	resp = (struct vaft_resp_query *)req;
+	data->keye = resp->keye;
+	data->cfge = resp->cfge;
+
+end:
+	ntmp_free_data_mem(dev, dma_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_vaft_delete_entry(struct netc_cbdrs *cbdrs, u32 entry_id)
+{
+	return ntmp_delete_entry_by_id(cbdrs, NTMP_VAFT_ID, cbdrs->tbl.maft_ver,
+				       entry_id, 0, 0);
+}
+
+int ntmp_rsst_query_or_update_entry(struct netc_cbdrs *cbdrs, u32 *table,
+				    int count, bool query)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct rsst_req_update *requ;
+	struct ntmp_req_by_eid *req;
+	union netc_cbd cbd;
+	u32 len, data_size;
+	dma_addr_t dma;
+	int err, i;
+	void *tmp;
+
+	if (count != RSST_ENTRY_NUM)
+		/* HW only takes in a full 64 entry table */
+		return -EINVAL;
+
+	if (query)
+		data_size = NTMP_ENTRY_ID_SIZE + RSST_STSE_DATA_SIZE(count) +
+			    RSST_CFGE_DATA_SIZE(count);
+	else
+		data_size = struct_size(requ, groups, count);
+
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Set the request data buffer */
+	if (query) {
+		ntmp_fill_crd_eid(req, cbdrs->tbl.rsst_ver, 0, 0, 0);
+		len = NTMP_LEN(sizeof(*req), data_size);
+		ntmp_fill_request_headr(&cbd, dma, len, NTMP_RSST_ID,
+					NTMP_CMD_QUERY, NTMP_AM_ENTRY_ID);
+	} else {
+		requ = (struct rsst_req_update *)req;
+		ntmp_fill_crd_eid(&requ->rbe, cbdrs->tbl.rsst_ver, 0,
+				  NTMP_GEN_UA_CFGEU | NTMP_GEN_UA_STSEU, 0);
+		for (i = 0; i < count; i++)
+			requ->groups[i] = (u8)(table[i]);
+
+		len = NTMP_LEN(data_size, 0);
+		ntmp_fill_request_headr(&cbd, dma, len, NTMP_RSST_ID,
+					NTMP_CMD_UPDATE, NTMP_AM_ENTRY_ID);
+	}
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err) {
+		dev_err(dev, "%s RSS table entry failed (%d)!",
+			query ? "Query" : "Update", err);
+		goto end;
+	}
+
+	if (query) {
+		u8 *group = (u8 *)req;
+
+		group += NTMP_ENTRY_ID_SIZE + RSST_STSE_DATA_SIZE(count);
+		for (i = 0; i < count; i++)
+			table[i] = group[i];
+	}
+
+end:
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+int ntmp_rfst_add_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			struct rfst_entry_data *data)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct rfst_req_add *req;
+	union netc_cbd cbd;
+	u32 len, req_len;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	req_len = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, req_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(&req->rbe, cbdrs->tbl.rfst_ver, 0, 0, entry_id);
+	req->keye = data->keye;
+	req->cfge = data->cfge;
+
+	len = NTMP_LEN(req_len, 0);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_RFST_ID,
+				NTMP_CMD_ADD, NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Add RFS table entry failed (%d)!", err);
+
+	ntmp_free_data_mem(dev, req_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_rfst_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			  struct rfst_entry_data *data)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct rfst_resp_query *resp;
+	u32 resp_len = sizeof(*resp);
+	struct ntmp_req_by_eid *req;
+	u32 req_len = sizeof(*req);
+	void *tmp = NULL;
+	dma_addr_t dma;
+	u32 dma_len;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return -EINVAL;
+
+	dma_len = max_t(u32, req_len, resp_len);
+	tmp = ntmp_alloc_data_mem(dev, dma_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(req, cbdrs->tbl.rfst_ver, 0, 0, entry_id);
+	err = ntmp_query_entry_by_id(cbdrs, NTMP_RFST_ID,
+				     NTMP_LEN(req_len, resp_len),
+				     req, &dma, true);
+	if (err)
+		goto end;
+
+	resp = (struct rfst_resp_query *)req;
+	data->keye = resp->keye;
+	data->cfge = resp->cfge;
+	data->matched_frames = resp->matched_frames;
+
+end:
+	ntmp_free_data_mem(dev, dma_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_rfst_delete_entry(struct netc_cbdrs *cbdrs, u32 entry_id)
+{
+	return ntmp_delete_entry_by_id(cbdrs, NTMP_RFST_ID, cbdrs->tbl.rfst_ver,
+				       entry_id, 0, 0);
+}
+
+/* Test codes for Time gate scheduling table */
+int ntmp_tgst_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			  struct tgst_query_data *data)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct tgst_resp_query *resp;
+	u32 resp_len = sizeof(*resp);
+	struct tgst_cfge_data *cfge;
+	struct tgst_olse_data *olse;
+	struct ntmp_req_by_eid *req;
+	u32 req_len = sizeof(*req);
+	void *tmp = NULL;
+	dma_addr_t dma;
+	u32 dma_len;
+	int i, err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return -EINVAL;
+
+	resp_len += struct_size(cfge, ge, TGST_MAX_ENTRY_NUM) +
+		    struct_size(olse, ge, TGST_MAX_ENTRY_NUM);
+	dma_len = max_t(u32, req_len, resp_len);
+	tmp = ntmp_alloc_data_mem(dev, dma_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(req, cbdrs->tbl.tgst_ver, 0, 0, entry_id);
+	err = ntmp_query_entry_by_id(cbdrs, NTMP_TGST_ID,
+				     NTMP_LEN(req_len, resp_len),
+				     req, &dma, false);
+	if (err)
+		goto end;
+
+	resp = (struct tgst_resp_query *)req;
+	cfge = (struct tgst_cfge_data *)resp->data;
+
+	data->config_change_time = resp->status.cfg_ct;
+	data->admin_bt = cfge->admin_bt;
+	data->admin_ct = cfge->admin_ct;
+	data->admin_ct_ext = cfge->admin_ct_ext;
+	data->admin_cl_len = cfge->admin_cl_len;
+	for (i = 0; i < le16_to_cpu(cfge->admin_cl_len); i++) {
+		data->cfge_ge[i].interval = cfge->ge[i].interval;
+		data->cfge_ge[i].tc_state = cfge->ge[i].tc_state;
+		data->cfge_ge[i].hr_cb = cfge->ge[i].hr_cb;
+	}
+
+	olse = (struct tgst_olse_data *)&cfge->ge[i];
+	data->oper_cfg_ct = olse->oper_cfg_ct;
+	data->oper_cfg_ce = olse->oper_cfg_ce;
+	data->oper_bt = olse->oper_bt;
+	data->oper_ct = olse->oper_ct;
+	data->oper_ct_ext = olse->oper_ct_ext;
+	data->oper_cl_len = olse->oper_cl_len;
+	for (i = 0; i < le16_to_cpu(olse->oper_cl_len); i++) {
+		data->olse_ge[i].interval = olse->ge[i].interval;
+		data->olse_ge[i].tc_state = olse->ge[i].tc_state;
+		data->olse_ge[i].hr_cb = olse->ge[i].hr_cb;
+	}
+
+end:
+	ntmp_free_data_mem(dev, dma_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_tgst_delete_admin_gate_list(struct netc_cbdrs *cbdrs, u32 entry_id)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct tgst_req_update *req;
+	struct tgst_cfge_data *cfge;
+	union netc_cbd cbd;
+	u32 len, data_size;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	data_size = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	cfge = &req->cfge;
+
+	/* Set the request data buffer and set the admin control list len
+	 * to zero to delete the existing admin control list.
+	 */
+	ntmp_fill_crd_eid(&req->rbe, cbdrs->tbl.tgst_ver, 0,
+			  NTMP_GEN_UA_CFGEU, entry_id);
+	cfge->admin_cl_len = 0;
+
+	/* Request header */
+	len = NTMP_LEN(data_size, sizeof(struct tgst_resp_status));
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_TGST_ID,
+				NTMP_CMD_UPDATE, NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Delete TGST entry failed (%d)!", err);
+
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+int ntmp_tgst_update_admin_gate_list(struct netc_cbdrs *cbdrs, u32 entry_id,
+				     struct tgst_cfge_data *cfge)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct tgst_req_update *req;
+	u32 len, req_len, cfge_len;
+	union netc_cbd cbd;
+	dma_addr_t dma;
+	u16 list_len;
+	void *tmp;
+	int err;
+
+	list_len = le16_to_cpu(cfge->admin_cl_len);
+	cfge_len = struct_size(cfge, ge, list_len);
+
+	/* Calculate the size of request data buffer */
+	req_len = struct_size(req, cfge.ge, list_len);
+	tmp = ntmp_alloc_data_mem(dev, req_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Set the request data buffer */
+	ntmp_fill_crd_eid(&req->rbe, cbdrs->tbl.tgst_ver, 0,
+			  NTMP_GEN_UA_CFGEU, entry_id);
+	memcpy(&req->cfge, cfge, cfge_len);
+
+	/* Request header */
+	len = NTMP_LEN(req_len, sizeof(struct tgst_resp_status));
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_TGST_ID,
+				NTMP_CMD_UPDATE, NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Update TGST entry failed (%d)!", err);
+
+	ntmp_free_data_mem(dev, req_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_rpt_add_or_update_entry(struct netc_cbdrs *cbdrs,
+				 struct ntmp_rpt_entry *entry)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct rpt_req_ua *req;
+	union netc_cbd cbd;
+	u32 data_size, len;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	data_size = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(&req->rbe, cbdrs->tbl.rpt_ver, 0, NTMP_GEN_UA_CFGEU |
+			  RPT_UA_FEEU | RPT_UA_PSEU | RPT_UA_STSEU,
+			  entry->entry_id);
+	req->cfge = entry->cfge;
+	req->fee = entry->fee;
+
+	/* Request header */
+	len = NTMP_LEN(data_size, 0);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_RPT_ID,
+				NTMP_CMD_AU, NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Add/Update RPT entry failed (%d)!", err);
+
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+int ntmp_rpt_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			 struct ntmp_rpt_entry *entry)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct rpt_resp_query *resp;
+	u32 resp_len = sizeof(*resp);
+	struct ntmp_req_by_eid *req;
+	u32 req_len = sizeof(*req);
+	void *tmp = NULL;
+	dma_addr_t dma;
+	u32 dma_len;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return -EINVAL;
+
+	dma_len = max_t(u32, req_len, resp_len);
+	tmp = ntmp_alloc_data_mem(dev, dma_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(req, cbdrs->tbl.rpt_ver, 0, 0, entry_id);
+	err = ntmp_query_entry_by_id(cbdrs, NTMP_RPT_ID,
+				     NTMP_LEN(req_len, resp_len),
+				     req, &dma, true);
+	if (err)
+		goto end;
+
+	resp = (struct rpt_resp_query *)req;
+	entry->stse = resp->stse;
+	entry->cfge = resp->cfge;
+	entry->fee = resp->fee;
+	entry->pse = resp->pse;
+
+end:
+	ntmp_free_data_mem(dev, dma_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_rpt_delete_entry(struct netc_cbdrs *cbdrs, u32 entry_id)
+{
+	return ntmp_delete_entry_by_id(cbdrs, NTMP_RPT_ID, cbdrs->tbl.rpt_ver,
+				       entry_id, 0, 0);
+}
+
+int ntmp_isit_add_or_update_entry(struct netc_cbdrs *cbdrs, bool add,
+				  struct ntmp_isit_entry *entry)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct isit_resp_query *resp;
+	struct isit_req_ua *req;
+	union netc_cbd cbd;
+	u32 data_size, len;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	data_size = add ? sizeof(*resp) : sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	if (!add)
+		ntmp_fill_crd(&req->crd, cbdrs->tbl.isit_ver, 0,
+			      NTMP_GEN_UA_CFGEU);
+	else
+		/* Query ENTRY_ID only */
+		ntmp_fill_crd(&req->crd, cbdrs->tbl.isit_ver,
+			      NTMP_GEN_UA_CFGEU, 0);
+
+	req->ak.keye = entry->keye;
+	req->is_eid = entry->is_eid;
+
+	/* Request header */
+	if (add) {
+		len = NTMP_LEN(sizeof(*req), sizeof(*resp));
+		/* Must be EXACT MATCH and the command must be
+		 * add, followed by a query. So that we can get
+		 * the entry id from HW.
+		 */
+		ntmp_fill_request_headr(&cbd, dma, len, NTMP_ISIT_ID,
+					NTMP_CMD_AQ, NTMP_AM_EXACT_KEY);
+	} else {
+		len = NTMP_LEN(sizeof(*req), sizeof(struct common_resp_nq));
+		ntmp_fill_request_headr(&cbd, dma, len, NTMP_ISIT_ID,
+					NTMP_CMD_UPDATE, NTMP_AM_EXACT_KEY);
+	}
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err) {
+		dev_err(dev, "%s ISIT entry failed (%d)!",
+			add ? "Add" : "Update", err);
+		goto end;
+	}
+
+	if (add) {
+		resp = (struct isit_resp_query *)req;
+		entry->entry_id = le32_to_cpu(resp->entry_id);
+	}
+
+end:
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+int ntmp_isit_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			  struct ntmp_isit_entry *entry)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct isit_resp_query *resp;
+	u32 resp_len = sizeof(*resp);
+	struct isit_req_qd *req;
+	u32 req_len, dma_len;
+	void *tmp = NULL;
+	dma_addr_t dma;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return -EINVAL;
+
+	req_len = sizeof(*req);
+	dma_len = max_t(u32, req_len, resp_len);
+	tmp = ntmp_alloc_data_mem(dev, dma_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd(&req->crd, cbdrs->tbl.isit_ver, 0, 0);
+	req->ak.eid.entry_id = cpu_to_le32(entry_id);
+	err = ntmp_query_entry_by_id(cbdrs, NTMP_ISIT_ID,
+				     NTMP_LEN(req_len, resp_len),
+				     (struct ntmp_req_by_eid *)req, &dma, false);
+	if (err)
+		goto end;
+
+	resp = (struct isit_resp_query *)req;
+	if (unlikely(le32_to_cpu(resp->entry_id) != entry_id)) {
+		dev_err(dev, "ISIT Query EID:0x%0x, Response EID:0x%x\n",
+			entry_id, le32_to_cpu(resp->entry_id));
+		err = -EIO;
+		goto end;
+	}
+
+	entry->keye = resp->keye;
+	entry->is_eid = resp->is_eid;
+
+end:
+	ntmp_free_data_mem(dev, dma_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_isit_delete_entry(struct netc_cbdrs *cbdrs, u32 entry_id)
+{
+	u32 resp_len = sizeof(struct common_resp_nq);
+	u32 req_len = sizeof(struct isit_req_qd);
+
+	return ntmp_delete_entry_by_id(cbdrs, NTMP_ISIT_ID, cbdrs->tbl.isit_ver,
+				       entry_id, req_len, resp_len);
+}
+
+int ntmp_ist_add_or_update_entry(struct netc_cbdrs *cbdrs,
+				 struct ntmp_ist_entry *entry)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct ist_req_ua *req;
+	union netc_cbd cbd;
+	u32 data_size, len;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	data_size = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Fill up NTMP request data buffer */
+	ntmp_fill_crd_eid(&req->rbe, cbdrs->tbl.ist_ver, 0,
+			  NTMP_GEN_UA_CFGEU, entry->entry_id);
+	req->cfge = entry->cfge;
+
+	/* Request header */
+	len = NTMP_LEN(data_size, 0);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_IST_ID,
+				NTMP_CMD_AU, NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Add/Update IST entry failed (%d)!", err);
+
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+int ntmp_ist_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			 struct ist_cfge_data *cfge)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct ist_resp_query *resp;
+	struct ntmp_req_by_eid *req;
+	u32 req_len = sizeof(*req);
+	u32 resp_len, dma_len;
+	void *tmp = NULL;
+	dma_addr_t dma;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return -EINVAL;
+
+	resp_len = sizeof(*resp);
+	dma_len = max_t(u32, req_len, resp_len);
+	tmp = ntmp_alloc_data_mem(dev, dma_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(req, cbdrs->tbl.ist_ver, 0, 0, entry_id);
+	err = ntmp_query_entry_by_id(cbdrs, NTMP_IST_ID,
+				     NTMP_LEN(req_len, resp_len),
+				     req, &dma, true);
+	if (err)
+		goto end;
+
+	resp = (struct ist_resp_query *)req;
+	*cfge = resp->cfge;
+
+end:
+	ntmp_free_data_mem(dev, dma_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_ist_delete_entry(struct netc_cbdrs *cbdrs, u32 entry_id)
+{
+	return ntmp_delete_entry_by_id(cbdrs, NTMP_IST_ID, cbdrs->tbl.ist_ver,
+				       entry_id, 0, 0);
+}
+
+int ntmp_isft_add_or_update_entry(struct netc_cbdrs *cbdrs, bool add,
+				  struct ntmp_isft_entry *entry)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct isft_resp_query *resp;
+	struct isft_req_ua *req;
+	union netc_cbd cbd;
+	u32 data_size, len;
+	dma_addr_t dma;
+	void *tmp;
+	u8 qa = 0;
+	int err;
+
+	data_size = add ? sizeof(*resp) : sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	if (add)
+		qa = NTMP_QA_ENTRY_ID;
+
+	ntmp_fill_crd(&req->crd, cbdrs->tbl.isft_ver, qa, NTMP_GEN_UA_CFGEU);
+	req->ak.keye = entry->keye;
+	req->cfge = entry->cfge;
+
+	/* Request header */
+	if (add) {
+		len = NTMP_LEN(sizeof(*req), sizeof(*resp));
+		/* Must be exact match, and command must be add,
+		 * followed by a query. So that we can get entry
+		 * ID from hardware.
+		 */
+		ntmp_fill_request_headr(&cbd, dma, len, NTMP_ISFT_ID,
+					NTMP_CMD_AQ, NTMP_AM_EXACT_KEY);
+	} else {
+		len = NTMP_LEN(sizeof(*req), sizeof(struct common_resp_nq));
+		ntmp_fill_request_headr(&cbd, dma, len, NTMP_ISFT_ID,
+					NTMP_CMD_UPDATE, NTMP_AM_EXACT_KEY);
+	}
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err) {
+		dev_err(dev, "%s ISFT entry failed (%d)!",
+			add ? "Add" : "Update", err);
+		goto end;
+	}
+
+	if (add) {
+		resp = (struct isft_resp_query *)req;
+		entry->entry_id = le32_to_cpu(resp->entry_id);
+	}
+
+end:
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+int ntmp_isft_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			  struct ntmp_isft_entry *entry)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct isft_resp_query *resp;
+	u32 resp_len = sizeof(*resp);
+	struct isft_req_qd *req;
+	u32 req_len, dma_len;
+	void *tmp = NULL;
+	dma_addr_t dma;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return -EINVAL;
+
+	req_len = sizeof(*req);
+	dma_len = max_t(u32, req_len, resp_len);
+	tmp = ntmp_alloc_data_mem(dev, dma_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd(&req->crd, cbdrs->tbl.isft_ver, 0, 0);
+	req->ak.eid.entry_id = cpu_to_le32(entry_id);
+	err = ntmp_query_entry_by_id(cbdrs, NTMP_ISFT_ID,
+				     NTMP_LEN(req_len, resp_len),
+				     (struct ntmp_req_by_eid *)req, &dma, false);
+	if (err)
+		goto end;
+
+	resp = (struct isft_resp_query *)req;
+	if (unlikely(le32_to_cpu(resp->entry_id) != entry_id)) {
+		dev_err(dev, "ISFT Query EID:0x%0x, Response EID:0x%x\n",
+			entry_id, le32_to_cpu(resp->entry_id));
+		err = -EIO;
+		goto end;
+	}
+
+	entry->keye = resp->keye;
+	entry->cfge = resp->cfge;
+
+end:
+	ntmp_free_data_mem(dev, dma_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_isft_delete_entry(struct netc_cbdrs *cbdrs, u32 entry_id)
+{
+	u32 resp_len = sizeof(struct common_resp_nq);
+	u32 req_len = sizeof(struct isft_req_qd);
+
+	return ntmp_delete_entry_by_id(cbdrs, NTMP_ISFT_ID, cbdrs->tbl.isft_ver,
+				       entry_id, req_len, resp_len);
+}
+
+int ntmp_sgclt_add_entry(struct netc_cbdrs *cbdrs,
+			 struct ntmp_sgclt_entry *entry)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct sgclt_req_add *req;
+	u32 num_gates, cfge_len;
+	union netc_cbd cbd;
+	u32 data_size, len;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	num_gates = entry->cfge.list_length + 1;
+	data_size = struct_size(req, cfge.ge, num_gates);
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Fill up NTMP request data buffer */
+	ntmp_fill_crd_eid(&req->rbe, cbdrs->tbl.sgclt_ver, 0, 0,
+			  entry->entry_id);
+	cfge_len = struct_size_t(struct sgclt_cfge_data, ge, num_gates);
+	memcpy(&req->cfge, &entry->cfge, cfge_len);
+
+	/* Request header */
+	len = NTMP_LEN(data_size, 0);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_SGCLT_ID,
+				NTMP_CMD_ADD, NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Add SGCLT entry failed (%d)!", err);
+
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+int ntmp_sgclt_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			   struct ntmp_sgclt_entry *entry, u32 cfge_size)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct sgclt_resp_query *resp;
+	struct ntmp_req_by_eid *req;
+	u32 req_len = sizeof(*req);
+	u32 num_gates, cfge_len;
+	u32 resp_len, dma_len;
+	void *tmp = NULL;
+	dma_addr_t dma;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return -EINVAL;
+
+	resp_len = struct_size(resp, cfge.ge, SGCLT_MAX_GE_NUM);
+	dma_len = max_t(u32, req_len, resp_len);
+	tmp = ntmp_alloc_data_mem(dev, dma_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(req, cbdrs->tbl.sgclt_ver, 0, 0, entry_id);
+	err = ntmp_query_entry_by_id(cbdrs, NTMP_SGCLT_ID,
+				     NTMP_LEN(req_len, resp_len),
+				     req, &dma, true);
+	if (err)
+		goto end;
+
+	resp = (struct sgclt_resp_query *)req;
+	entry->ref_count = resp->ref_count;
+	num_gates = resp->cfge.list_length + 1;
+	cfge_len = struct_size_t(struct sgclt_cfge_data, ge, num_gates);
+	if (cfge_len > cfge_size) {
+		err = -ENOMEM;
+		dev_err(dev, "SGCLT_CFGE buffer size is %u, larger than %u\n",
+			cfge_size, cfge_len);
+
+		goto end;
+	}
+
+	memcpy(&entry->cfge, &resp->cfge, cfge_len);
+
+end:
+	ntmp_free_data_mem(dev, dma_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_sgclt_delete_entry(struct netc_cbdrs *cbdrs, u32 entry_id)
+{
+	return ntmp_delete_entry_by_id(cbdrs, NTMP_SGCLT_ID, cbdrs->tbl.sgclt_ver,
+				       entry_id, 0, 0);
+}
+
+int ntmp_sgit_add_or_update_entry(struct netc_cbdrs *cbdrs,
+				  struct ntmp_sgit_entry *entry)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct sgit_req_ua *req;
+	union netc_cbd cbd;
+	u32 data_size, len;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	data_size = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(&req->rbe, cbdrs->tbl.sgit_ver, 0, SGIT_UA_ACFGEU |
+			  SGIT_UA_CFGEU | SGIT_UA_SGISEU, entry->entry_id);
+	req->acfge = entry->acfge;
+	req->cfge = entry->cfge;
+	req->icfge = entry->icfge;
+
+	/* Request header */
+	len = NTMP_LEN(data_size, 0);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_SGIT_ID,
+				NTMP_CMD_AU, NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Add/Update SGIT entry failed (%d)!", err);
+
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+int ntmp_sgit_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			  struct ntmp_sgit_entry *entry)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct sgit_resp_query *resp;
+	u32 resp_len = sizeof(*resp);
+	struct ntmp_req_by_eid *req;
+	u32 req_len = sizeof(*req);
+	void *tmp = NULL;
+	dma_addr_t dma;
+	u32 dma_len;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return -EINVAL;
+
+	dma_len = max_t(u32, req_len, resp_len);
+	tmp = ntmp_alloc_data_mem(dev, dma_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(req, cbdrs->tbl.sgit_ver, 0, 0, entry_id);
+	err = ntmp_query_entry_by_id(cbdrs, NTMP_SGIT_ID,
+				     NTMP_LEN(req_len, resp_len),
+				     req, &dma, true);
+	if (err)
+		goto end;
+
+	resp = (struct sgit_resp_query *)req;
+	entry->sgise = resp->sgise;
+	entry->cfge = resp->cfge;
+	entry->icfge = resp->icfge;
+	entry->acfge = resp->acfge;
+
+end:
+	ntmp_free_data_mem(dev, dma_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_sgit_delete_entry(struct netc_cbdrs *cbdrs, u32 entry_id)
+{
+	return ntmp_delete_entry_by_id(cbdrs, NTMP_SGIT_ID, cbdrs->tbl.sgit_ver,
+				       entry_id, 0, 0);
+}
+
+int ntmp_isct_operate_entry(struct netc_cbdrs *cbdrs, u32 entry_id, int cmd,
+			    struct isct_stse_data *stse)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct isct_resp_query *resp;
+	struct ntmp_req_by_eid *req;
+	union netc_cbd cbd;
+	u32 data_size, len;
+	dma_addr_t dma;
+	bool query;
+	u16 ua = 0;
+	void *tmp;
+	int err;
+
+	/* Check the command. */
+	switch (cmd) {
+	case NTMP_CMD_QUERY:
+	case NTMP_CMD_QD:
+	case NTMP_CMD_QU:
+		if (!stse)
+			return -EINVAL;
+		fallthrough;
+	case NTMP_CMD_DELETE:
+	case NTMP_CMD_UPDATE:
+	case NTMP_CMD_ADD:
+	break;
+	default:
+		return -EINVAL;
+	}
+
+	query = !!(cmd & NTMP_CMD_QUERY);
+	data_size = query ? sizeof(*resp) : sizeof(*req);
+
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	if (cmd & NTMP_CMD_UPDATE)
+		ua = NTMP_GEN_UA_CFGEU;
+
+	ntmp_fill_crd_eid(req, cbdrs->tbl.isct_ver, 0, ua, entry_id);
+	/* Request header */
+	len = NTMP_LEN(sizeof(*req), query ? sizeof(*resp) : 0);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_ISCT_ID,
+				cmd, NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err) {
+		dev_err(dev, "Operate SGIT entry (%d) failed (%d)!",
+			cmd, err);
+		goto end;
+	}
+
+	if (query) {
+		resp = (struct isct_resp_query *)req;
+		if (unlikely(le32_to_cpu(resp->entry_id) != entry_id)) {
+			dev_err(dev, "ISCT Query EID:0x%0x, Response EID:0x%x\n",
+				entry_id, le32_to_cpu(resp->entry_id));
+			err = -EIO;
+			goto end;
+		}
+
+		*stse = resp->stse;
+	}
+
+end:
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+int ntmp_ipft_add_entry(struct netc_cbdrs *cbdrs, u32 *entry_id,
+			struct ntmp_ipft_entry *entry)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct ipft_resp_query *resp;
+	struct ipft_req_add *req;
+	union netc_cbd cbd;
+	u32 data_size, len;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	data_size = sizeof(*resp);
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Fill up NTMP request data buffer */
+	ntmp_fill_crd(&req->crd, cbdrs->tbl.ipft_ver, NTMP_QA_ENTRY_ID,
+		      NTMP_GEN_UA_CFGEU | NTMP_GEN_UA_STSEU);
+	req->keye = entry->keye;
+	req->cfge = entry->cfge;
+
+	/* Request header */
+	len = NTMP_LEN(sizeof(*req), data_size);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_IPFT_ID,
+				NTMP_CMD_AQ, NTMP_AM_TERNARY_KEY);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err) {
+		dev_err(dev, "Add IPFT entry failed (%d)!", err);
+		goto end;
+	}
+
+	resp = (struct ipft_resp_query *)req;
+	if (entry_id)
+		*entry_id = le32_to_cpu(resp->entry_id);
+
+end:
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+int ntmp_ipft_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			  bool update, struct ntmp_ipft_entry *entry)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct ipft_resp_query *resp;
+	u32 resp_len = sizeof(*resp);
+	struct ipft_req_qd *req;
+	u32 req_len, dma_len;
+	void *tmp = NULL;
+	dma_addr_t dma;
+	u16 ua = 0;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return -EINVAL;
+
+	req_len = sizeof(*req);
+	/* CFGE_DATA is present when performing an update command,
+	 * but we don't need to set this filed because only STSEU
+	 * is updated here.
+	 */
+	if (update)
+		req_len += sizeof(struct ipft_cfge_data);
+
+	dma_len = max_t(u32, req_len, resp_len);
+	tmp = ntmp_alloc_data_mem(dev, dma_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	if (update)
+		ua = NTMP_GEN_UA_STSEU;
+
+	ntmp_fill_crd_eid(&req->rbe, cbdrs->tbl.ipft_ver, 0, ua, entry_id);
+	err = ntmp_query_entry_by_id(cbdrs, NTMP_IPFT_ID,
+				     NTMP_LEN(req_len, resp_len),
+				     (struct ntmp_req_by_eid *)req, &dma, false);
+
+	resp = (struct ipft_resp_query *)req;
+	if (unlikely(le32_to_cpu(resp->entry_id) != entry_id)) {
+		dev_err(dev, "IPFT Query EID:0x%0x, Response EID:0x%x\n",
+			entry_id, le32_to_cpu(resp->entry_id));
+		err = -EIO;
+		goto end;
+	}
+
+	entry->keye = resp->keye;
+	entry->match_count = resp->match_count;
+	entry->cfge = resp->cfge;
+
+end:
+	ntmp_free_data_mem(dev, dma_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_ipft_delete_entry(struct netc_cbdrs *cbdrs, u32 entry_id)
+{
+	u32 resp_len = sizeof(struct common_resp_nq);
+	u32 req_len = sizeof(struct ipft_req_qd);
+
+	return ntmp_delete_entry_by_id(cbdrs, NTMP_IPFT_ID, cbdrs->tbl.ipft_ver,
+				       entry_id, req_len, resp_len);
+}
+
+/**
+ * ntmp_fdbt_update_activity_element - update the aging time of all the dynamic
+ * entries in the FDB table.
+ * @cbdrs: target netc_cbdrs struct
+ *
+ * A single activity update management could be used to process all the dynamic
+ * entries in the FDB table. When hardware process an activity updata management
+ * command for an entry in the FDB table and the entry does not have its activity
+ * flag set, the activity counter is incremented, If, however, the activity flag
+ * is set, then both the activity flag and activity counter are reset.
+ * Software can issue the activity update management commands at predefined times
+ * and the value of the activity counter can then be used to estimate the period
+ * of how long an FDB entry has been inactive.
+ *
+ * Returns 0 on success or < 0 on error
+ */
+int ntmp_fdbt_update_activity_element(struct netc_cbdrs *cbdrs)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct fdbt_req_ua *req;
+	union netc_cbd cbd;
+	u32 len, data_size;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	data_size = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Request data */
+	ntmp_fill_crd(&req->crd, cbdrs->tbl.fdbt_ver, 0, FDBT_UA_ACTEU);
+	req->ak.search.resume_eid = cpu_to_le32(NTMP_NULL_ENTRY_ID);
+
+	/* Request header */
+	len = NTMP_LEN(data_size, sizeof(struct common_resp_nq));
+	/* For activity update, the access method must be search */
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_FDBT_ID,
+				NTMP_CMD_UPDATE, NTMP_AM_SEARCH);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "FDB table activity update command failed (%d)\n", err);
+
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+/**
+ * ntmp_fdbt_delete_aging_entries - delete all the matched dynamic entries
+ * in the FDB table
+ * @cbdrs: target netc_cbdrs struct
+ * @act_cnt: the target value of the activity counter
+ *
+ * The matching rule is that the activity flag is not set and the activity
+ * counter is greater than or equal to act_cnt
+ *
+ * Returns 0 on success or < 0 on error
+ */
+int ntmp_fdbt_delete_aging_entries(struct netc_cbdrs *cbdrs, u8 act_cnt)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct fdbt_req_qd *req;
+	u32 cfg = FDBT_DYNAMIC;
+	u32 len, data_size;
+	union netc_cbd cbd;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	data_size = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	if (act_cnt > FDBT_MAX_ACT_CNT)
+		act_cnt = FDBT_MAX_ACT_CNT;
+
+	/* Request data */
+	ntmp_fill_crd(&req->crd, cbdrs->tbl.fdbt_ver, 0, 0);
+	req->ak.search.resume_eid = cpu_to_le32(NTMP_NULL_ENTRY_ID);
+	req->ak.search.cfge.cfg = cpu_to_le32(cfg);
+	req->ak.search.acte.act = act_cnt & FDBT_ACT_CNT;
+	/* Entry match with ACTE_DATA[ACT_FLAG] AND match >= ACTE_DATA[ACT_CNT] */
+	req->ak.search.acte_mc = FDBT_ACTE_MC;
+	req->ak.search.cfge_mc = FDBT_CFGE_MC_DYNAMIC;
+
+	/* Request header */
+	len = NTMP_LEN(data_size, sizeof(struct common_resp_nq));
+	/* For activity update, the access method must be search */
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_FDBT_ID,
+				NTMP_CMD_DELETE, NTMP_AM_SEARCH);
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Delete FDB table aging entries failed (%d)\n", err);
+
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+/**
+ * ntmp_fdbt_add_entry - add an entry into the FDB table
+ * @cbdrs: target netc_cbdrs struct
+ * @entry_id: retruned value, the ID of the FDB entry
+ * @keye: key element data
+ * @cfge: configuration element data
+ *
+ * Returns two values: entry_id and error code (0 on success or < 0 on error)
+ */
+int ntmp_fdbt_add_entry(struct netc_cbdrs *cbdrs, u32 *entry_id,
+			struct fdbt_keye_data *keye,
+			struct fdbt_cfge_data *cfge)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct fdbt_resp_query *resp;
+	struct fdbt_req_ua *req;
+	union netc_cbd cbd;
+	u32 len, req_len;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	req_len = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, req_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Request data */
+	ntmp_fill_crd(&req->crd, cbdrs->tbl.fdbt_ver, NTMP_QA_ENTRY_ID,
+		      NTMP_GEN_UA_CFGEU);
+	req->ak.exact.keye = *keye;
+	req->cfge = *cfge;
+
+	/* Request header */
+	len = NTMP_LEN(req_len, sizeof(*resp));
+	/* The entry id is allotted by hardware, so we need to a query
+	 * action after the add action to get the entry id from hardware.
+	 */
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_FDBT_ID,
+				NTMP_CMD_AQ, NTMP_AM_EXACT_KEY);
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err) {
+		dev_err(dev, "Add FDB table entry failed (%d)\n", err);
+		goto end;
+	}
+
+	if (entry_id) {
+		resp = (struct fdbt_resp_query *)req;
+		*entry_id = le32_to_cpu(resp->entry_id);
+	}
+
+end:
+	ntmp_free_data_mem(dev, req_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_fdbt_update_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			   struct fdbt_cfge_data *cfge)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct fdbt_req_ua *req;
+	union netc_cbd cbd;
+	u32 len, req_len;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	req_len = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, req_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Request data */
+	ntmp_fill_crd(&req->crd, cbdrs->tbl.fdbt_ver, 0, NTMP_GEN_UA_CFGEU);
+	req->ak.eid.entry_id = cpu_to_le32(entry_id);
+	req->cfge = *cfge;
+
+	/* Request header */
+	len = NTMP_LEN(req_len, sizeof(struct common_resp_nq));
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_FDBT_ID,
+				NTMP_CMD_UPDATE, NTMP_AM_ENTRY_ID);
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Update FDB table entry failed (%d)\n", err);
+
+	ntmp_free_data_mem(dev, req_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_fdbt_delete_entry(struct netc_cbdrs *cbdrs, u32 entry_id)
+{
+	u32 resp_len = sizeof(struct common_resp_nq);
+	u32 req_len = sizeof(struct fdbt_req_qd);
+
+	return ntmp_delete_entry_by_id(cbdrs, NTMP_FDBT_ID, cbdrs->tbl.fdbt_ver,
+				       entry_id, req_len, resp_len);
+}
+
+int ntmp_fdbt_delete_port_dynamic_entries(struct netc_cbdrs *cbdrs, int port)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct fdbt_req_qd *req;
+	u32 cfg = FDBT_DYNAMIC;
+	union netc_cbd cbd;
+	u32 len, data_size;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	data_size = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Request data */
+	ntmp_fill_crd(&req->crd, cbdrs->tbl.fdbt_ver, 0, 0);
+	req->ak.search.resume_eid = cpu_to_le32(NTMP_NULL_ENTRY_ID);
+	req->ak.search.cfge.port_bitmap = cpu_to_le32(BIT(port));
+	req->ak.search.cfge.cfg = cpu_to_le32(cfg);
+	/* Match CFGE_DATA[DYNAMIC & PORT_BITMAP] field */
+	req->ak.search.cfge_mc = FDBT_CFGE_MC_DYNAMIC_AND_PORT_BITMAP;
+
+	/* Request header */
+	len = NTMP_LEN(data_size, sizeof(struct common_resp_nq));
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_FDBT_ID,
+				NTMP_CMD_DELETE, NTMP_AM_SEARCH);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Delete Port:%d FDB table dynamic entries failed (%d)\n",
+			port, err);
+
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+int ntmp_fdbt_search_port_entry(struct netc_cbdrs *cbdrs, int port,
+				u32 *resume_entry_id, u32 *entry_id,
+				struct fdbt_query_data *data)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct fdbt_resp_query *resp;
+	struct fdbt_req_qd *req;
+	union netc_cbd cbd;
+	u32 len, req_len;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	req_len = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, req_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Request data */
+	ntmp_fill_crd(&req->crd, cbdrs->tbl.fdbt_ver, 0, 0);
+	req->ak.search.resume_eid = cpu_to_le32(*resume_entry_id);
+	req->ak.search.cfge.port_bitmap = cpu_to_le32(BIT(port));
+	/* Match CFGE_DATA[PORT_BITMAP] field */
+	req->ak.search.cfge_mc = FDBT_CFGE_MC_PORT_BITMAP;
+
+	/* Request header */
+	len = NTMP_LEN(req_len, sizeof(*resp));
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_FDBT_ID,
+				NTMP_CMD_QUERY, NTMP_AM_SEARCH);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err) {
+		dev_err(dev, "Search port:%d FDB table entry failed (%d)\n",
+			port, err);
+		goto end;
+	}
+
+	if (!cbd.resp_hdr.num_matched) {
+		*entry_id = NTMP_NULL_ENTRY_ID;
+		*resume_entry_id = NTMP_NULL_ENTRY_ID;
+		goto end;
+	}
+
+	resp = (struct fdbt_resp_query *)req;
+	*entry_id = le32_to_cpu(resp->entry_id);
+	*resume_entry_id = le32_to_cpu(resp->status);
+	data->keye = resp->keye;
+	data->cfge = resp->cfge;
+	data->acte = resp->acte;
+
+end:
+	ntmp_free_data_mem(dev, req_len, tmp, dma);
+
+	return err;
+}
+
+/**
+ * ntmp_vft_add_entry - add an entry into the VLAN filter table
+ * @cbdrs: target netc_cbdrs struct
+ * @entry_id: retruned value, the ID of the Vlan filter entry
+ * @vid: VLAN ID
+ * @cfge: configuration elemenet data
+ *
+ * Returns two values: entry_id and error code (0 on success or < 0 on error)
+ */
+int ntmp_vft_add_entry(struct netc_cbdrs *cbdrs, u32 *entry_id,
+		       u16 vid, struct vft_cfge_data *cfge)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct vft_resp_query *resp;
+	struct vft_req_ua *req;
+	union netc_cbd cbd;
+	u32 len, data_size;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	data_size = sizeof(*resp);
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Request data */
+	ntmp_fill_crd(&req->crd, cbdrs->tbl.vft_ver, NTMP_QA_ENTRY_ID,
+		      NTMP_GEN_UA_CFGEU);
+	req->ak.exact.vid = cpu_to_le16(vid);
+	req->cfge = *cfge;
+
+	/* Request header */
+	len = NTMP_LEN(sizeof(*req), data_size);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_VFT_ID,
+				NTMP_CMD_AQ, NTMP_AM_EXACT_KEY);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err) {
+		dev_err(dev, "Add VLAN filter table entry failed (%d)\n", err);
+		goto end;
+	}
+
+	if (entry_id) {
+		resp = (struct vft_resp_query *)req;
+		*entry_id = le32_to_cpu(resp->entry_id);
+	}
+
+end:
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+int ntmp_vft_update_entry(struct netc_cbdrs *cbdrs, u16 vid,
+			  struct vft_cfge_data *cfge)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct vft_req_ua *req;
+	union netc_cbd cbd;
+	u32 len, data_size;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	data_size = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Request data */
+	ntmp_fill_crd(&req->crd, cbdrs->tbl.vft_ver, 0, NTMP_GEN_UA_CFGEU);
+	req->ak.exact.vid = cpu_to_le16(vid);
+	req->cfge = *cfge;
+
+	/* Request header */
+	len = NTMP_LEN(data_size, sizeof(struct common_resp_nq));
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_VFT_ID,
+				NTMP_CMD_UPDATE, NTMP_AM_EXACT_KEY);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Update VLAN filter table entry failed (%d)\n", err);
+
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+int ntmp_vft_delete_entry(struct netc_cbdrs *cbdrs, u16 vid)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct vft_req_qd *req;
+	union netc_cbd cbd;
+	u32 len, data_size;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	data_size = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Request data */
+	ntmp_fill_crd(&req->crd, cbdrs->tbl.vft_ver, 0, 0);
+	req->ak.exact.vid = cpu_to_le16(vid);
+
+	/* Request header */
+	len = NTMP_LEN(data_size, sizeof(struct common_resp_nq));
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_VFT_ID,
+				NTMP_CMD_DELETE, NTMP_AM_EXACT_KEY);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Delete VLAN filter table entry failed (%d)\n", err);
+
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+int ntmp_vft_search_entry(struct netc_cbdrs *cbdrs, u32 *resume_eid, u32 *entry_id,
+			  u16 *vid, struct vft_cfge_data *cfge)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct vft_resp_query *resp;
+	struct vft_req_qd *req;
+	union netc_cbd cbd;
+	u32 len, data_size;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	data_size = sizeof(*resp);
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Request data */
+	ntmp_fill_crd(&req->crd, cbdrs->tbl.vft_ver, 0, 0);
+	req->ak.resume_entry_id = cpu_to_le32(*resume_eid);
+
+	/* Request header */
+	len = NTMP_LEN(sizeof(*req), data_size);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_VFT_ID,
+				NTMP_CMD_QUERY, NTMP_AM_SEARCH);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err) {
+		dev_err(dev, "Search VLAN filter table entry failed (%d)\n", err);
+		goto end;
+	}
+
+	if (!cbd.resp_hdr.num_matched) {
+		*entry_id = NTMP_NULL_ENTRY_ID;
+		*resume_eid = NTMP_NULL_ENTRY_ID;
+		goto end;
+	}
+
+	resp = (struct vft_resp_query *)req;
+	/* Get the response resume_entry_id to continue search */
+	*resume_eid = le32_to_cpu(resp->status);
+	*entry_id = le32_to_cpu(resp->entry_id);
+	*cfge = resp->cfge;
+	*vid = le16_to_cpu(resp->vid);
+
+end:
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+int ntmp_vft_query_entry_by_vid(struct netc_cbdrs *cbdrs, u16 vid, u32 *entry_id,
+				struct vft_cfge_data *cfge)
+{
+	struct device *dev = cbdrs->dma_dev;
+	u32 req_len, resp_len, dma_len, len;
+	struct vft_resp_query *resp;
+	struct vft_req_qd *req;
+	union netc_cbd cbd;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	req_len = sizeof(*req);
+	resp_len = sizeof(*resp);
+	dma_len = max_t(u32, req_len, resp_len);
+	tmp = ntmp_alloc_data_mem(dev, dma_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Request data */
+	ntmp_fill_crd(&req->crd, cbdrs->tbl.vft_ver, 0, 0);
+	req->ak.exact.vid = cpu_to_le16(vid);
+
+	/* Request header */
+	len = NTMP_LEN(req_len, resp_len);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_VFT_ID,
+				NTMP_CMD_QUERY, NTMP_AM_EXACT_KEY);
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err) {
+		dev_err(dev, "Search VLAN filter table entry failed (%d)\n", err);
+		goto end;
+	}
+
+	if (!cbd.resp_hdr.num_matched) {
+		*entry_id = NTMP_NULL_ENTRY_ID;
+		goto end;
+	}
+
+	resp = (struct vft_resp_query *)req;
+	if (vid != le16_to_cpu(resp->vid)) {
+		dev_err(dev, "Response VID (%u) doesn't match query VID (%u)\n",
+			le16_to_cpu(resp->vid), vid);
+		err = -EINVAL;
+		goto end;
+	}
+
+	*entry_id = le32_to_cpu(resp->entry_id);
+	*cfge = resp->cfge;
+
+end:
+	ntmp_free_data_mem(dev, dma_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_ett_add_or_update_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+				 bool add, struct ett_cfge_data *cfge)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct ett_req_ua *req;
+	union netc_cbd cbd;
+	u32 len, req_len;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	req_len = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, req_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Request data */
+	ntmp_fill_crd_eid(&req->rbe, cbdrs->tbl.ett_ver, 0,
+			  NTMP_GEN_UA_CFGEU, entry_id);
+	req->cfge = *cfge;
+
+	/* Request header */
+	len = NTMP_LEN(req_len, 0);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_ETT_ID,
+				add ? NTMP_CMD_ADD : NTMP_CMD_UPDATE,
+				NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "%s Egress treatment table entry failed (%d)\n",
+			add ? "Add" : "Update", err);
+
+	ntmp_free_data_mem(dev, req_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_ett_delete_entry(struct netc_cbdrs *cbdrs, u32 entry_id)
+{
+	return ntmp_delete_entry_by_id(cbdrs, NTMP_ETT_ID, cbdrs->tbl.ett_ver,
+				       entry_id, 0, 0);
+}
+
+int ntmp_ett_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			 struct ett_cfge_data *cfge)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct ntmp_req_by_eid *req;
+	struct ett_resp_query *resp;
+	u32 req_len = sizeof(*req);
+	u32 resp_len, dma_len;
+	void *tmp = NULL;
+	dma_addr_t dma;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return -EINVAL;
+
+	resp_len = sizeof(*resp);
+	dma_len = max_t(u32, req_len, resp_len);
+	tmp = ntmp_alloc_data_mem(dev, dma_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(req, cbdrs->tbl.ett_ver, 0, 0, entry_id);
+	err = ntmp_query_entry_by_id(cbdrs, NTMP_ETT_ID,
+				     NTMP_LEN(req_len, resp_len),
+				     req, &dma, true);
+	if (err)
+		goto end;
+
+	resp = (struct ett_resp_query *)req;
+	*cfge = resp->cfge;
+
+end:
+	ntmp_free_data_mem(dev, dma_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_esrt_update_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			   struct esrt_cfge_data *cfge)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct esrt_req_update *req;
+	union netc_cbd cbd;
+	u32 len, req_len;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	req_len = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, req_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Request data */
+	ntmp_fill_crd_eid(&req->rbe, cbdrs->tbl.esrt_ver, 0, NTMP_GEN_UA_CFGEU |
+			  NTMP_GEN_UA_STSEU | ESRT_UA_SRSEU, entry_id);
+	req->cfge = *cfge;
+
+	/* Request header */
+	len = NTMP_LEN(req_len, 0);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_ESRT_ID,
+				NTMP_CMD_UPDATE, NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Update ESRT entry failed (%d)\n",
+			err);
+
+	ntmp_free_data_mem(dev, req_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_esrt_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			  struct esrt_query_data *data)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct esrt_resp_query *resp;
+	u32 resp_len = sizeof(*resp);
+	struct ntmp_req_by_eid *req;
+	u32 req_len = sizeof(*req);
+	void *tmp = NULL;
+	dma_addr_t dma;
+	u32 dma_len;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return -EINVAL;
+
+	dma_len = max_t(u32, req_len, resp_len);
+	tmp = ntmp_alloc_data_mem(dev, dma_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(req, cbdrs->tbl.esrt_ver, 0, 0, entry_id);
+	err = ntmp_query_entry_by_id(cbdrs, NTMP_ESRT_ID,
+				     NTMP_LEN(req_len, resp_len),
+				     req, &dma, true);
+	if (err)
+		goto end;
+
+	resp = (struct esrt_resp_query *)req;
+	data->stse = resp->stse;
+	data->cfge = resp->cfge;
+	data->srse = resp->srse;
+
+end:
+	ntmp_free_data_mem(dev, dma_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_ect_update_entry(struct netc_cbdrs *cbdrs, u32 entry_id)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct ntmp_req_by_eid *req;
+	union netc_cbd cbd;
+	u32 len, data_size;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	data_size = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Request data */
+	ntmp_fill_crd_eid(req, cbdrs->tbl.ect_ver, 0, ECT_UA_STSEU, entry_id);
+
+	/* Request header */
+	len = NTMP_LEN(data_size, 0);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_ECT_ID,
+				NTMP_CMD_UPDATE, NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Update ECT entry failed (%d)\n", err);
+
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+int ntmp_ect_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			 struct ect_stse_data *stse, bool update)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct ect_resp_query *resp;
+	struct ntmp_req_by_eid *req;
+	union netc_cbd cbd;
+	u32 len, data_size;
+	dma_addr_t dma;
+	u16 ua = 0;
+	void *tmp;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return -EINVAL;
+
+	data_size = sizeof(*resp);
+	tmp = ntmp_alloc_data_mem(dev, data_size, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Request data */
+	if (update)
+		/* Query, followed by Update. */
+		ua = ECT_UA_STSEU;
+
+	ntmp_fill_crd_eid(req, cbdrs->tbl.ect_ver, 0, ua, entry_id);
+
+	/* Request header */
+	len = NTMP_LEN(sizeof(*req), data_size);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_ECT_ID,
+				update ? NTMP_CMD_QU : NTMP_CMD_QUERY,
+				NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err) {
+		dev_err(dev, "Query ECT entry failed (%d)\n", err);
+		goto end;
+	}
+
+	resp = (struct ect_resp_query *)req;
+	if (unlikely(entry_id != le32_to_cpu(resp->entry_id))) {
+		dev_err(dev, "ECT wuery EID:0x%0x, Response EID:0x%x\n",
+			entry_id, le32_to_cpu(resp->entry_id));
+		err = -EIO;
+		goto end;
+	}
+
+	*stse = resp->stse;
+
+end:
+	ntmp_free_data_mem(dev, data_size, tmp, dma);
+
+	return err;
+}
+
+int ntmp_fmt_add_or_update_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+				 bool add, struct fmt_cfge_data *cfge)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct fmt_req_ua *req;
+	union netc_cbd cbd;
+	u32 len, req_len;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	req_len = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, req_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	/* Request data */
+	ntmp_fill_crd_eid(&req->rbe, cbdrs->tbl.fmt_ver, 0,
+			  NTMP_GEN_UA_CFGEU, entry_id);
+	req->cfge = *cfge;
+
+	/* Request header */
+	len = NTMP_LEN(req_len, 0);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_FMT_ID,
+				add ? NTMP_CMD_ADD : NTMP_CMD_UPDATE,
+				NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "%s Frame Modification table entry failed (%d)\n",
+			add ? "Add" : "Update", err);
+
+	ntmp_free_data_mem(dev, req_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_fmt_delete_entry(struct netc_cbdrs *cbdrs, u32 entry_id)
+{
+	return ntmp_delete_entry_by_id(cbdrs, NTMP_FMT_ID, cbdrs->tbl.fmt_ver,
+				       entry_id, 0, 0);
+}
+
+int ntmp_fmt_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			 struct fmt_cfge_data *cfge)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct ntmp_req_by_eid *req;
+	struct fmt_resp_query *resp;
+	u32 req_len = sizeof(*req);
+	u32 resp_len, dma_len;
+	void *tmp = NULL;
+	dma_addr_t dma;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return -EINVAL;
+
+	resp_len = sizeof(*resp);
+	dma_len = max_t(u32, req_len, resp_len);
+	tmp = ntmp_alloc_data_mem(dev, dma_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(req, cbdrs->tbl.fmt_ver, 0, 0, entry_id);
+	err = ntmp_query_entry_by_id(cbdrs, NTMP_FMT_ID,
+				     NTMP_LEN(req_len, resp_len),
+				     req, &dma, true);
+	if (err)
+		goto end;
+
+	resp = (struct fmt_resp_query *)req;
+	*cfge = resp->cfge;
+
+end:
+	ntmp_free_data_mem(dev, dma_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_bpt_update_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			  struct bpt_cfge_data *cfge)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct bpt_req_update *req;
+	union netc_cbd cbd;
+	u32 len, req_len;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	req_len = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, req_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(&req->rbe, cbdrs->tbl.bpt_ver, 0,
+			  NTMP_GEN_UA_CFGEU | BPT_UA_BPSEU, entry_id);
+	req->cfge = *cfge;
+
+	len = NTMP_LEN(req_len, 0);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_BPT_ID,
+				NTMP_CMD_UPDATE, NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Update Buffer Pool table entry failed (%d)\n", err);
+
+	ntmp_free_data_mem(dev, req_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_bpt_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			 struct bpt_query_data *data)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct ntmp_req_by_eid *req;
+	struct bpt_resp_query *resp;
+	u32 req_len = sizeof(*req);
+	u32 resp_len, dma_len;
+	void *tmp = NULL;
+	dma_addr_t dma;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return -EINVAL;
+
+	resp_len = sizeof(*resp);
+	dma_len = max_t(u32, req_len, resp_len);
+	tmp = ntmp_alloc_data_mem(dev, dma_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(req, cbdrs->tbl.bpt_ver, 0, 0, entry_id);
+	err = ntmp_query_entry_by_id(cbdrs, NTMP_BPT_ID,
+				     NTMP_LEN(req_len, resp_len),
+				     req, &dma, true);
+	if (err)
+		goto end;
+
+	resp = (struct bpt_resp_query *)req;
+	data->bpse = resp->bpse;
+	data->cfge = resp->cfge;
+
+end:
+	ntmp_free_data_mem(dev, dma_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_sbpt_update_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			   struct sbpt_cfge_data *cfge)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct sbpt_req_update *req;
+	union netc_cbd cbd;
+	u32 len, req_len;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	req_len = sizeof(*req);
+	tmp = ntmp_alloc_data_mem(dev, req_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(&req->rbe, cbdrs->tbl.sbpt_ver, 0,
+			  NTMP_GEN_UA_CFGEU | SBPT_UA_BPSEU, entry_id);
+	req->cfge = *cfge;
+
+	len = NTMP_LEN(req_len, 0);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_SBPT_ID,
+				NTMP_CMD_UPDATE, NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Update Shared Buffer Pool table entry failed (%d)\n",
+			err);
+
+	ntmp_free_data_mem(dev, req_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_sbpt_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			  struct sbpt_query_data *data)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct sbpt_resp_query *resp;
+	u32 resp_len = sizeof(*resp);
+	struct ntmp_req_by_eid *req;
+	u32 req_len = sizeof(*req);
+	void *tmp = NULL;
+	dma_addr_t dma;
+	u32 dma_len;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return -EINVAL;
+
+	dma_len = max_t(u32, req_len, resp_len);
+	tmp = ntmp_alloc_data_mem(dev, dma_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(req, cbdrs->tbl.sbpt_ver, 0, 0, entry_id);
+	err = ntmp_query_entry_by_id(cbdrs, NTMP_SBPT_ID,
+				     NTMP_LEN(req_len, resp_len),
+				     req, &dma, true);
+	if (err)
+		goto end;
+
+	resp = (struct sbpt_resp_query *)req;
+	data->sbpse = resp->sbpse;
+	data->cfge = resp->cfge;
+
+end:
+	ntmp_free_data_mem(dev, dma_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_fmdt_update_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			   u8 *data_buff, u32 data_len)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct fmdt_req_update *req;
+	u32 align = data_len;
+	union netc_cbd cbd;
+	u32 len, req_len;
+	dma_addr_t dma;
+	void *tmp;
+	int err;
+
+	if (align % FMDT_DATA_LEN_ALIGN) {
+		align = DIV_ROUND_UP(align, FMDT_DATA_LEN_ALIGN);
+		align *= FMDT_DATA_LEN_ALIGN;
+	}
+
+	req_len = struct_size(req, data, align);
+	tmp = ntmp_alloc_data_mem(dev, req_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(&req->rbe, cbdrs->tbl.fmdt_ver, 0,
+			  NTMP_GEN_UA_CFGEU, entry_id);
+
+	/* Fill configuration element data */
+	memcpy(req->data, data_buff, data_len);
+
+	len = NTMP_LEN(req_len, 0);
+	ntmp_fill_request_headr(&cbd, dma, len, NTMP_FMDT_ID,
+				NTMP_CMD_UPDATE, NTMP_AM_ENTRY_ID);
+
+	err = netc_xmit_ntmp_cmd(cbdrs, &cbd);
+	if (err)
+		dev_err(dev, "Update Frame Modification Data table entry failed (%d)\n",
+			err);
+
+	ntmp_free_data_mem(dev, req_len, tmp, dma);
+
+	return err;
+}
+
+int ntmp_fmdt_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			  u8 *data_buff, u32 data_len)
+{
+	struct device *dev = cbdrs->dma_dev;
+	struct fmdt_resp_query *resp;
+	struct ntmp_req_by_eid *req;
+	u32 req_len = sizeof(*req);
+	u32 resp_len, dma_len;
+	void *tmp = NULL;
+	dma_addr_t dma;
+	int err;
+
+	if (entry_id == NTMP_NULL_ENTRY_ID)
+		return -EINVAL;
+
+	resp_len = struct_size(resp, data, data_len);
+	dma_len = max_t(u32, req_len, resp_len);
+	tmp = ntmp_alloc_data_mem(dev, dma_len, &dma, (void **)&req);
+	if (!tmp)
+		return -ENOMEM;
+
+	ntmp_fill_crd_eid(req, cbdrs->tbl.fmdt_ver, 0, 0, entry_id);
+	err = ntmp_query_entry_by_id(cbdrs, NTMP_FMDT_ID,
+				     NTMP_LEN(req_len, resp_len),
+				     req, &dma, true);
+	if (err)
+		goto end;
+
+	resp = (struct fmdt_resp_query *)req;
+	memcpy(data_buff, resp->data, data_len);
+
+end:
+	ntmp_free_data_mem(dev, dma_len, tmp, dma);
+
+	return err;
+}
+
+MODULE_DESCRIPTION("NXP NETC Library");
+MODULE_LICENSE("Dual BSD/GPL");
diff --git a/devices/enetc/ntmp_private.h b/devices/enetc/ntmp_private.h
new file mode 100755
index 0000000..a0cae04
--- /dev/null
+++ b/devices/enetc/ntmp_private.h
@@ -0,0 +1,503 @@
+/* SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause) */
+/*
+ * NTMP table request and response data buffer formats
+ * and some private macros and functions
+ *
+ * Copyright 2025 NXP
+ *
+ */
+#ifndef __NTMP_PRIAVTE_H
+#define __NTMP_PRIAVTE_H
+#include <linux/fsl/ntmp.h>
+
+#define TGST_MAX_ENTRY_NUM		64
+#define SGCLT_MAX_GE_NUM		256
+#define SGIT_MAX_CT_PLUS_CT_EXT		0x3fffffffU
+
+#pragma pack(1)
+struct common_req_data {
+	__le16 update_act;
+	u8 dbg_opt;
+	u8 tblv_qact;
+#define NTMP_QUERY_ACT		GENMASK(3, 0)
+#define NTMP_TBL_VER		GENMASK(7, 0)
+#define NTMP_TBLV_QACT(v, a)	(FIELD_PREP(NTMP_TBL_VER, (v)) | \
+				 ((a) & NTMP_QUERY_ACT))
+};
+
+struct common_resp_query {
+	__le32 entry_id;
+};
+
+struct common_resp_nq {
+	__le32 status;
+};
+
+/* Generic structure for request data by entry ID  */
+struct ntmp_req_by_eid {
+	struct common_req_data crd;
+	__le32 entry_id;
+};
+
+/* MAC Address Filter Table Request and Response Data Buffer Format */
+struct maft_req_add {
+	struct ntmp_req_by_eid rbe;
+	struct maft_keye_data keye;
+	struct maft_cfge_data cfge;
+};
+
+/*struct for response data buffer */
+struct maft_resp_query {
+	__le32 entry_id;
+	struct maft_keye_data keye;
+	struct maft_cfge_data cfge;
+};
+
+/* VLAM Address Filter table Request and Response Data Buffer Format */
+struct vaft_req_add {
+	struct ntmp_req_by_eid rbe;
+	struct vaft_keye_data keye;
+	struct vaft_cfge_data cfge;
+};
+
+/* VLAN Address Filter Table Response to Query action */
+struct vaft_resp_query {
+	__le32 entry_id;
+	struct vaft_keye_data keye;
+	struct vaft_cfge_data cfge;
+};
+
+/* RSS Table Request and Response Data Buffer Format */
+struct rsst_req_update {
+	struct ntmp_req_by_eid rbe;
+	u8 groups[];
+};
+
+/* RFS Table Request and Response Data Buffer Format */
+struct rfst_req_add {
+	struct ntmp_req_by_eid rbe;
+	struct rfst_keye_data keye;
+	struct rfst_cfge_data cfge;
+};
+
+struct rfst_resp_query {
+	__le32 entry_id;
+	struct rfst_keye_data keye;
+	__le64 matched_frames; /* STSE_DATA */
+	struct rfst_cfge_data cfge;
+};
+
+/* Time Gate Scheduling Table Resquet and Response Data Buffer Format */
+struct tgst_ge {
+	__le32 interval;
+	u8 tc_state;
+	u8 resv0;
+	u8 hr_cb;
+#define TGST_HR_CB		GENMASK(3, 0)
+	u8 resv1;
+};
+
+struct tgst_cfge_data {
+	__le64 admin_bt;
+	__le32 admin_ct;
+	__le32 admin_ct_ext;
+	__le16 admin_cl_len;
+	__le16 resv;
+	struct tgst_ge ge[];
+};
+
+struct tgst_olse_data {
+	__le64 oper_cfg_ct;
+	__le64 oper_cfg_ce;
+	__le64 oper_bt;
+	__le32 oper_ct;
+	__le32 oper_ct_ext;
+	__le16 oper_cl_len;
+	__le16 resv;
+	struct tgst_ge ge[];
+};
+
+struct tgst_req_update {
+	struct ntmp_req_by_eid rbe;
+	struct tgst_cfge_data cfge;
+};
+
+struct tgst_resp_status {
+	__le64 cfg_ct;
+	__le32 status_resv;
+};
+
+struct tgst_resp_query {
+	struct tgst_resp_status status;
+	__le32 entry_id;
+	u8 data[];
+};
+
+/* Rate Policer Table Request and Response Data Buffer Format */
+struct rpt_req_ua {
+	struct ntmp_req_by_eid rbe;
+	struct rpt_cfge_data cfge;
+	struct rpt_fee_data fee;
+};
+
+struct rpt_resp_query {
+	__le32 entry_id;
+	struct rpt_stse_data stse;
+	struct rpt_cfge_data cfge;
+	struct rpt_fee_data fee;
+	struct rpt_pse_data pse;
+};
+
+/* Ingress Stream Identification Table Resquet and Response Data Buffer Format */
+struct isit_ak_eid {
+	__le32 entry_id;
+	__le32 resv[4];
+};
+
+struct isit_ak_search {
+	__le32 resume_eid;
+	__le32 resv[4];
+};
+
+union isit_access_key {
+	struct isit_ak_eid eid;
+	struct isit_keye_data keye;
+	struct isit_ak_search search;
+};
+
+/* struct for update or add operation*/
+struct isit_req_ua {
+	struct common_req_data crd;
+	union isit_access_key ak;
+	__le32 is_eid;
+};
+
+/* struct for not update or add operation, such as delete, query */
+struct isit_req_qd {
+	struct common_req_data crd;
+	union isit_access_key ak;
+};
+
+struct isit_resp_query {
+	__le32 status;
+	__le32 entry_id;
+	struct isit_keye_data keye;
+	__le32 is_eid;
+};
+
+/* Ingress Stream Table version 0 Resquet and Response Data Buffer Format */
+struct ist_req_ua {
+	struct ntmp_req_by_eid rbe;
+	struct ist_cfge_data cfge;
+};
+
+struct ist_resp_query {
+	__le32 entry_id;
+	struct ist_cfge_data cfge;
+};
+
+/* Ingress Stream filter Table Resquet and Response Data Buffer Format */
+struct isft_ak_eid {
+	__le32 entry_id;
+	__le32 resv;
+};
+
+struct isft_ak_search {
+	__le32 resume_eid;
+	__le32 resv;
+};
+
+union isft_access_key {
+	struct isft_ak_eid eid;
+	struct isft_keye_data keye;
+	struct isft_ak_search search;
+};
+
+struct isft_req_ua {
+	struct common_req_data crd;
+	union isft_access_key ak;
+	struct isft_cfge_data cfge;
+};
+
+struct isft_req_qd {
+	struct common_req_data crd;
+	union isft_access_key ak;
+};
+
+struct isft_resp_query {
+	__le32 status;
+	__le32 entry_id;
+	struct isft_keye_data keye;
+	struct isft_cfge_data cfge;
+};
+
+/* Stream Gate Instance Table Resquet and Response Data Buffer Format */
+struct sgit_req_ua {
+	struct ntmp_req_by_eid rbe;
+	struct sgit_acfge_data acfge;
+	struct sgit_cfge_data cfge;
+	struct sgit_icfge_data icfge;
+};
+
+struct sgit_resp_query {
+	__le32 entry_id;
+	struct sgit_sgise_data sgise;
+	struct sgit_cfge_data cfge;
+	struct sgit_icfge_data icfge;
+	u8 resv;
+	struct sgit_acfge_data acfge;
+};
+
+/* Stream Gate Control List Table Request and Response Data Buffer Format */
+struct sgclt_req_add {
+	struct ntmp_req_by_eid rbe;
+	struct sgclt_cfge_data cfge;
+};
+
+struct sgclt_resp_query {
+	__le32 entry_id;
+	u8 ref_count;
+	u8 resv[3];
+	struct sgclt_cfge_data cfge;
+};
+
+/* Ingress Stream Count Table Request and Response Data Buffer Format */
+struct isct_resp_query {
+	__le32 entry_id;
+	struct isct_stse_data stse;
+};
+
+/* Ingress Port Filter Table Request and Response Data Buffer Format */
+struct ipft_req_add {
+	struct common_req_data crd;
+	struct ipft_keye_data keye;
+	struct ipft_cfge_data cfge;
+};
+
+/* request data format of query or delete action */
+struct ipft_req_qd {
+	struct ntmp_req_by_eid rbe;
+	__le32 resv[52];
+};
+
+struct ipft_resp_query {
+	__le32 status;
+	__le32 entry_id;
+	struct ipft_keye_data keye;
+	__le64 match_count; /* STSE_DATA */
+	struct ipft_cfge_data cfge;
+};
+
+/* Access Key: Entry ID Match */
+struct fdbt_ak_eid {
+	__le32 entry_id;
+	__le32 resv[7];
+};
+
+/* Access Key: Key Element Match */
+struct fdbt_ak_exact {
+	struct fdbt_keye_data keye;
+	__le32 resv[5];
+};
+
+/* Access Key: Search */
+struct fdbt_ak_search {
+	__le32 resume_eid;
+	struct fdbt_keye_data keye;
+	struct fdbt_cfge_data cfge;
+	struct fdbt_acte_data acte;
+	u8 keye_mc;
+#define FDBT_KEYE_MAC		GENMASK(1, 0)
+	u8 cfge_mc;
+#define FDBT_CFGE_MC		GENMASK(2, 0)
+#define FDBT_CFGE_MC_ANY		0
+#define FDBT_CFGE_MC_DYNAMIC		1
+#define FDBT_CFGE_MC_PORT_BITMAP	2
+#define FDBT_CFGE_MC_DYNAMIC_AND_PORT_BITMAP	3
+	u8 acte_mc;
+#define FDBT_ACTE_MC		BIT(0)
+};
+
+union fdbt_access_key {
+	struct fdbt_ak_eid eid;
+	struct fdbt_ak_exact exact;
+	struct fdbt_ak_search search;
+};
+
+struct fdbt_req_ua {
+	struct common_req_data crd;
+	union fdbt_access_key ak;
+	struct fdbt_cfge_data cfge;
+};
+
+struct fdbt_req_qd {
+	struct common_req_data crd;
+	union fdbt_access_key ak;
+};
+
+struct fdbt_resp_query {
+	__le32 status;
+	__le32 entry_id;
+	struct fdbt_keye_data keye;
+	struct fdbt_cfge_data cfge;
+	struct fdbt_acte_data acte;
+	u8 resv[3];
+};
+
+struct vft_ak_exact {
+	__le16 vid; /* bit0~11: VLAN ID, other bits are reserved */
+	__le16 resv;
+};
+
+union vft_access_key {
+	__le32 entry_id; /* entry_id match */
+	struct vft_ak_exact exact;
+	__le32 resume_entry_id; /* search */
+};
+
+struct vft_req_ua {
+	struct common_req_data crd;
+	union vft_access_key ak;
+	struct vft_cfge_data cfge;
+};
+
+struct vft_req_qd {
+	struct common_req_data crd;
+	union vft_access_key ak;
+};
+
+struct vft_resp_query {
+	__le32 status;
+	__le32 entry_id;
+	__le16 vid; /* KEYE_DATA */
+	__le16 resv;
+	struct vft_cfge_data cfge;
+};
+
+struct ett_req_ua {
+	struct ntmp_req_by_eid rbe;
+	struct ett_cfge_data cfge;
+};
+
+struct ett_resp_query {
+	__le32 entry_id;
+	struct ett_cfge_data cfge;
+};
+
+struct esrt_req_update {
+	struct ntmp_req_by_eid rbe;
+	struct esrt_cfge_data cfge;
+};
+
+struct esrt_resp_query {
+	__le32 entry_id;
+	struct esrt_stse_data stse;
+	struct esrt_cfge_data cfge;
+	struct esrt_srse_data srse;
+};
+
+struct ect_resp_query {
+	__le32 entry_id;
+	struct ect_stse_data stse;
+};
+
+struct fmt_req_ua {
+	struct ntmp_req_by_eid rbe;
+	struct fmt_cfge_data cfge;
+};
+
+struct fmt_resp_query {
+	__le32 entry_id;
+	struct fmt_cfge_data cfge;
+};
+
+struct bpt_req_update {
+	struct ntmp_req_by_eid rbe;
+	struct bpt_cfge_data cfge;
+};
+
+/* Buffer Pool Table Response Data Buffer Format of Query action*/
+struct bpt_resp_query {
+	__le32 entry_id;
+	struct bpt_bpse_data bpse;
+	u8 resv[3];
+	struct bpt_cfge_data cfge;
+};
+
+/* Shared Buffer Pool Table Request Data Buffer Format of Update action */
+struct sbpt_req_update {
+	struct ntmp_req_by_eid rbe;
+	struct sbpt_cfge_data cfge;
+};
+
+/* Shared Buffer Pool Table Response Data Buffer Format of Query action */
+struct sbpt_resp_query {
+	__le32 entry_id;
+	struct sbpt_sbpse_data sbpse;
+	u8 resv[3];
+	struct sbpt_cfge_data cfge;
+};
+
+struct fmdt_req_update {
+	struct ntmp_req_by_eid rbe;
+	u8 data[]; /* big-endian, must be aligned to 4 bytes */
+};
+
+struct fmdt_resp_query {
+	__le32 entry_id;
+	u8 data[];
+};
+
+#pragma pack()
+
+struct tgst_query_data {
+	__le64 config_change_time;
+	__le64 admin_bt;
+	__le32 admin_ct;
+	__le32 admin_ct_ext;
+	__le16 admin_cl_len;
+	__le64 oper_cfg_ct;
+	__le64 oper_cfg_ce;
+	__le64 oper_bt;
+	__le32 oper_ct;
+	__le32 oper_ct_ext;
+	__le16 oper_cl_len;
+	struct tgst_ge olse_ge[TGST_MAX_ENTRY_NUM];
+	struct tgst_ge cfge_ge[TGST_MAX_ENTRY_NUM];
+};
+
+u32 ntmp_lookup_free_words(unsigned long *bitmap, u32 bitmap_size,
+			   u32 num_words);
+void ntmp_clear_words_bitmap(unsigned long *bitmap, u32 entry_id,
+			     u32 num_words);
+int ntmp_tgst_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			  struct tgst_query_data *data);
+int ntmp_tgst_update_admin_gate_list(struct netc_cbdrs *cbdrs, u32 entry_id,
+				     struct tgst_cfge_data *cfge);
+int ntmp_tgst_delete_admin_gate_list(struct netc_cbdrs *cbdrs, u32 entry_id);
+int ntmp_rpt_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			 struct ntmp_rpt_entry *entry);
+int ntmp_isit_add_or_update_entry(struct netc_cbdrs *cbdrs, bool add,
+				  struct ntmp_isit_entry *entry);
+int ntmp_isit_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			  struct ntmp_isit_entry *entry);
+int ntmp_isit_delete_entry(struct netc_cbdrs *cbdrs, u32 entry_id);
+int ntmp_ist_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			 struct ist_cfge_data *cfge);
+int ntmp_isft_add_or_update_entry(struct netc_cbdrs *cbdrs, bool add,
+				  struct ntmp_isft_entry *entry);
+int ntmp_isft_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			  struct ntmp_isft_entry *entry);
+int ntmp_isft_delete_entry(struct netc_cbdrs *cbdrs, u32 entry_id);
+int ntmp_sgit_add_or_update_entry(struct netc_cbdrs *cbdrs,
+				  struct ntmp_sgit_entry *entry);
+int ntmp_sgit_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			  struct ntmp_sgit_entry *entry);
+int ntmp_sgit_delete_entry(struct netc_cbdrs *cbdrs, u32 entry_id);
+int ntmp_sgclt_add_entry(struct netc_cbdrs *cbdrs,
+			 struct ntmp_sgclt_entry *entry);
+int ntmp_sgclt_delete_entry(struct netc_cbdrs *cbdrs, u32 entry_id);
+int ntmp_sgclt_query_entry(struct netc_cbdrs *cbdrs, u32 entry_id,
+			   struct ntmp_sgclt_entry *entry, u32 cfge_size);
+
+#endif
diff --git a/devices/generic.c b/devices/generic.c
index 4be3f44..170bb10 100644
--- a/devices/generic.c
+++ b/devices/generic.c
@@ -407,14 +407,12 @@ int __init ec_gen_init_module(void)
     INIT_LIST_HEAD(&generic_devices);
     INIT_LIST_HEAD(&descs);
 
-    read_lock(&dev_base_lock);
     for_each_netdev(&init_net, netdev) {
         if (netdev->type != ARPHRD_ETHER)
             continue;
         desc = kmalloc(sizeof(ec_gen_interface_desc_t), GFP_ATOMIC);
         if (!desc) {
             ret = -ENOMEM;
-            read_unlock(&dev_base_lock);
             goto out_err;
         }
         strncpy(desc->name, netdev->name, IFNAMSIZ);
@@ -423,7 +421,6 @@ int __init ec_gen_init_module(void)
         memcpy(desc->dev_addr, netdev->dev_addr, ETH_ALEN);
         list_add_tail(&desc->list, &descs);
     }
-    read_unlock(&dev_base_lock);
 
     list_for_each_entry_safe(desc, next, &descs, list) {
         ret = offer_device(desc);
-- 
2.43.0

