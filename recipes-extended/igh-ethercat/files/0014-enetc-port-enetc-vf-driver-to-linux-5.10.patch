From f1672ca3ed1de6c0736bf46ccc1b83c1b9749247 Mon Sep 17 00:00:00 2001
From: Wenbin Song <wenbin.song@nxp.com>
Date: Mon, 28 Jun 2021 19:18:55 +0800
Subject: [PATCH 14/18] enetc: port enetc vf driver to linux 5.10

Signed-off-by: Wenbin Song <wenbin.song@nxp.com>
---
 devices/enetc/Kbuild.in       |    1 +
 devices/enetc/Makefile.am     |    1 +
 devices/enetc/enetc.c         | 1001 ++++++---------------------------
 devices/enetc/enetc.h         |  222 +++++++-
 devices/enetc/enetc_cbdr.c    |    5 +-
 devices/enetc/enetc_ethtool.c |  314 ++++++++++-
 devices/enetc/enetc_hw.h      |  517 ++++++++++-------
 devices/enetc/enetc_ptp.c     |   68 ++-
 devices/enetc/enetc_tsn.c     |  535 +++++++++++-------
 devices/enetc/enetc_vf.c      |  146 +----
 10 files changed, 1387 insertions(+), 1423 deletions(-)

diff --git a/devices/enetc/Kbuild.in b/devices/enetc/Kbuild.in
index f089179..e938df2 100644
--- a/devices/enetc/Kbuild.in
+++ b/devices/enetc/Kbuild.in
@@ -43,6 +43,7 @@ ifeq (@ENABLE_ENETC@,1)
 		enetc.o \
 		enetc_vf.o \
 		enetc_cbdr.o \
+		enetc_qos.o \
 		enetc_ethtool.o
 	obj-m += ec_enetc.o
 	ec_enetc-objs := $(EC_ENETC_OBJ)
diff --git a/devices/enetc/Makefile.am b/devices/enetc/Makefile.am
index 2049a7f..b8fec1c 100644
--- a/devices/enetc/Makefile.am
+++ b/devices/enetc/Makefile.am
@@ -32,6 +32,7 @@ EXTRA_DIST = \
 	enetc_ethtool.c \
 	enetc_tsn.c \
 	enetc_cbdr.c \
+	enetc_qos.c \
 	enetc_ptp.c \
 	enetc_vf.c 
 
diff --git a/devices/enetc/enetc.c b/devices/enetc/enetc.c
index e87fdab..7839b06 100644
--- a/devices/enetc/enetc.c
+++ b/devices/enetc/enetc.c
@@ -4,7 +4,8 @@
 #include "enetc.h"
 #include <linux/tcp.h>
 #include <linux/udp.h>
-#include <linux/of_mdio.h>
+#include <linux/vmalloc.h>
+#include <linux/ptp_clock_kernel.h>
 #include "../ecdev.h"
 
 /* ENETC overhead: optional extension BD + 1 BD gap */
@@ -14,7 +15,7 @@
 #define ENETC_TXBDS_MAX_NEEDED	ENETC_TXBDS_NEEDED(ENETC_MAX_SKB_FRAGS + 1)
 
 static int enetc_map_tx_buffs(struct enetc_bdr *tx_ring, struct sk_buff *skb,
-			      enum enetc_hw_features hw_features);
+			      struct enetc_ndev_priv *priv);
 
 netdev_tx_t enetc_xmit(struct sk_buff *skb, struct net_device *ndev)
 {
@@ -22,34 +23,11 @@ netdev_tx_t enetc_xmit(struct sk_buff *skb, struct net_device *ndev)
 	struct enetc_bdr *tx_ring;
 	int count;
 
-	tx_ring = priv->tx_ring[skb->queue_mapping];
+	tx_ring = priv->tx_ring[0];
 
-	if (unlikely(skb_shinfo(skb)->nr_frags > ENETC_MAX_SKB_FRAGS))
-		if (unlikely(skb_linearize(skb)))
-			goto drop_packet_err;
-
-	count = skb_shinfo(skb)->nr_frags + 1; /* fragments + head */
-	if (enetc_bd_unused(tx_ring) < ENETC_TXBDS_NEEDED(count)) {
-		if (!priv->ecdev) {
-			netif_stop_subqueue(ndev, tx_ring->index);
-		}
-		return NETDEV_TX_BUSY;
-	}
-
-	count = enetc_map_tx_buffs(tx_ring, skb, priv->hw_features);
-	if (unlikely(!count))
-		goto drop_packet_err;
-
-	if (enetc_bd_unused(tx_ring) < ENETC_TXBDS_MAX_NEEDED)
-		if (!priv->ecdev) {
-			netif_stop_subqueue(ndev, tx_ring->index);
-		}
-	return NETDEV_TX_OK;
-
-drop_packet_err:
-	if (!priv->ecdev) {
-		dev_kfree_skb_any(skb);
-	}
+	enetc_lock_mdio();
+	count = enetc_map_tx_buffs(tx_ring, skb, priv);
+	enetc_unlock_mdio();
 	return NETDEV_TX_OK;
 }
 
@@ -102,29 +80,20 @@ static void enetc_unmap_tx_buff(struct enetc_bdr *tx_ring,
 static void enetc_free_tx_skb(struct enetc_bdr *tx_ring,
 			      struct enetc_tx_swbd *tx_swbd)
 {
-	struct enetc_ndev_priv *priv = netdev_priv(tx_ring->ndev);
 	if (tx_swbd->dma)
 		enetc_unmap_tx_buff(tx_ring, tx_swbd);
 
-	if (tx_swbd->skb) {
-		if (!priv->ecdev) {
-			dev_kfree_skb_any(tx_swbd->skb);
-		}
-		tx_swbd->skb = NULL;
-	}
+	tx_swbd->skb = NULL;
 }
 
 static int enetc_map_tx_buffs(struct enetc_bdr *tx_ring, struct sk_buff *skb,
-			      enum enetc_hw_features hw_features)
+			      struct enetc_ndev_priv *priv)
 {
 	struct enetc_tx_swbd *tx_swbd;
-	struct skb_frag_struct *frag;
 	int len = skb_headlen(skb);
 	union enetc_tx_bd temp_bd;
 	union enetc_tx_bd *txbd;
-	bool do_vlan, do_tstamp;
 	int i, count = 0;
-	unsigned int f;
 	dma_addr_t dma;
 	u8 flags = 0;
 
@@ -146,84 +115,22 @@ static int enetc_map_tx_buffs(struct enetc_bdr *tx_ring, struct sk_buff *skb,
 	tx_swbd->is_dma_page = 0;
 	count++;
 
-	do_vlan = skb_vlan_tag_present(skb);
-	do_tstamp = (hw_features & ENETC_F_TX_TSTAMP) &&
-		    (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP);
-	tx_swbd->do_tstamp = do_tstamp;
-	tx_swbd->qbv_en = hw_features & ENETC_F_QBV;
-	tx_swbd->check_wb = tx_swbd->do_tstamp || tx_swbd->qbv_en;
-
-	if (do_vlan || do_tstamp)
-		flags |= ENETC_TXBD_FLAGS_EX;
 
 	if (enetc_tx_csum(skb, &temp_bd))
 		flags |= ENETC_TXBD_FLAGS_CSUM | ENETC_TXBD_FLAGS_L4CS;
+	else if (tx_ring->tsd_enable)
+		flags |= ENETC_TXBD_FLAGS_TSE | ENETC_TXBD_FLAGS_TXSTART;
 
 	/* first BD needs frm_len and offload flags set */
 	temp_bd.frm_len = cpu_to_le16(skb->len);
 	temp_bd.flags = flags;
 
-	if (flags & ENETC_TXBD_FLAGS_EX) {
-		u8 e_flags = 0;
-		*txbd = temp_bd;
-		enetc_clear_tx_bd(&temp_bd);
-
-		/* add extension BD for VLAN and/or timestamping */
-		flags = 0;
-		tx_swbd++;
-		txbd++;
-		i++;
-		if (unlikely(i == tx_ring->bd_count)) {
-			i = 0;
-			tx_swbd = tx_ring->tx_swbd;
-			txbd = ENETC_TXBD(*tx_ring, 0);
-		}
-		prefetchw(txbd);
-
-		if (do_vlan) {
-			temp_bd.ext.vid = cpu_to_le16(skb_vlan_tag_get(skb));
-			temp_bd.ext.tpid = 0; /* < C-TAG */
-			e_flags |= ENETC_TXBD_E_FLAGS_VLAN_INS;
-		}
-
-		if (do_tstamp) {
-			skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
-			e_flags |= ENETC_TXBD_E_FLAGS_TWO_STEP_PTP;
-		}
+	if (flags & ENETC_TXBD_FLAGS_TSE) {
+		u32 temp;
 
-		temp_bd.ext.e_flags = e_flags;
-		count++;
-	}
-
-	frag = &skb_shinfo(skb)->frags[0];
-	for (f = 0; f < skb_shinfo(skb)->nr_frags; f++, frag++) {
-		len = skb_frag_size(frag);
-		dma = skb_frag_dma_map(tx_ring->dev, frag, 0, len,
-				       DMA_TO_DEVICE);
-		if (dma_mapping_error(tx_ring->dev, dma))
-			goto dma_err;
-
-		*txbd = temp_bd;
-		enetc_clear_tx_bd(&temp_bd);
-
-		flags = 0;
-		tx_swbd++;
-		txbd++;
-		i++;
-		if (unlikely(i == tx_ring->bd_count)) {
-			i = 0;
-			tx_swbd = tx_ring->tx_swbd;
-			txbd = ENETC_TXBD(*tx_ring, 0);
-		}
-		prefetchw(txbd);
-
-		temp_bd.addr = cpu_to_le64(dma);
-		temp_bd.buf_len = cpu_to_le16(len);
-
-		tx_swbd->dma = dma;
-		tx_swbd->len = len;
-		tx_swbd->is_dma_page = 1;
-		count++;
+		temp = (skb->skb_mstamp_ns >> 5 & ENETC_TXBD_TXSTART_MASK)
+			| (flags << ENETC_TXBD_FLAGS_OFFSET);
+		temp_bd.txstart = cpu_to_le32(temp);
 	}
 
 	/* last BD needs 'F' bit set */
@@ -237,13 +144,12 @@ static int enetc_map_tx_buffs(struct enetc_bdr *tx_ring, struct sk_buff *skb,
 	tx_ring->next_to_use = i;
 
 	/* let H/W know BD ring has been updated */
-	enetc_wr_reg(tx_ring->tpir, i); /* includes wmb() */
+	enetc_wr_reg_hot(tx_ring->tpir, i); /* includes wmb() */
 
 	return count;
 
 dma_err:
 	dev_err(tx_ring->dev, "DMA map error");
-
 	do {
 		tx_swbd = &tx_ring->tx_swbd[i];
 		enetc_free_tx_skb(tx_ring, tx_swbd);
@@ -251,155 +157,51 @@ dma_err:
 			i = tx_ring->bd_count;
 		i--;
 	} while (count--);
-
 	return 0;
 }
 
-static irqreturn_t enetc_msix(int irq, void *data)
-{
-	struct enetc_int_vector	*v = data;
-	int i;
-
-	/* disable interrupts */
-	enetc_wr_reg(v->rbier, 0);
-
-	for_each_set_bit(i, &v->tx_rings_map, v->count_tx_rings)
-		enetc_wr_reg(v->tbier_base + ENETC_BDR_OFF(i), 0);
-
-	napi_schedule_irqoff(&v->napi);
-
-	return IRQ_HANDLED;
-}
-
-static bool enetc_clean_tx_ring(struct enetc_bdr *tx_ring, int napi_budget);
-static int enetc_clean_rx_ring(struct enetc_bdr *rx_ring,
-			       struct napi_struct *napi, int work_limit);
+static bool enetc_clean_tx_ring(struct enetc_bdr *tx_ring);
+static int enetc_clean_rx_ring(struct enetc_bdr *rx_ring);
 
 void ec_poll(struct net_device *ndev)
 {
 
 	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-	int i, j;
-	struct enetc_int_vector *v;
-	for (i = 0; i < priv->bdr_int_num; i++){
-		v = priv->int_vector[i];
-		for (j = 0; j < v->count_tx_rings; j++){
-			enetc_clean_tx_ring(&v->tx_ring[j], 16);
-		}
-		enetc_clean_rx_ring(&v->rx_ring, NULL, 16);
-	}
-
-}
-
-static int enetc_poll(struct napi_struct *napi, int budget)
-{
-	struct enetc_int_vector
-		*v = container_of(napi, struct enetc_int_vector, napi);
-	bool complete = true;
-	int work_done;
 	int i;
-
-	for (i = 0; i < v->count_tx_rings; i++)
-		if (!enetc_clean_tx_ring(&v->tx_ring[i], budget))
-			complete = false;
-
-	work_done = enetc_clean_rx_ring(&v->rx_ring, napi, budget);
-	if (work_done == budget)
-		complete = false;
-
-	if (!complete)
-		return budget;
-
-	napi_complete_done(napi, work_done);
-
-	/* enable interrupts */
-	enetc_wr_reg(v->rbier, ENETC_RBIER_RXTIE);
-
-	for_each_set_bit(i, &v->tx_rings_map, v->count_tx_rings)
-		enetc_wr_reg(v->tbier_base + ENETC_BDR_OFF(i),
-			     ENETC_TBIER_TXTIE);
-
-	return work_done;
+	for (i = 0; i < priv->num_tx_rings; i++)
+		enetc_clean_tx_ring(priv->tx_ring[i]);
+	for (i = 0; i < priv->num_rx_rings; i++)
+		enetc_clean_rx_ring(priv->rx_ring[i]);
 }
 
 static int enetc_bd_ready_count(struct enetc_bdr *tx_ring, int ci)
 {
-	int pi = enetc_rd_reg(tx_ring->tcir) & ENETC_TBCIR_IDX_MASK;
+	int pi = enetc_rd_reg_hot(tx_ring->tcir) & ENETC_TBCIR_IDX_MASK;
 
 	return pi >= ci ? pi - ci : tx_ring->bd_count - ci + pi;
 }
 
-static void enetc_get_tx_tstamp(struct enetc_hw *hw, union enetc_tx_bd *txbd,
-				u64 *tstamp)
-{
-	u32 lo, hi;
-
-	lo = enetc_rd(hw, ENETC_SICTR0);
-	hi = enetc_rd(hw, ENETC_SICTR1);
-	if (lo <= txbd->wb.tstamp)
-		hi -= 1;
-	*tstamp = (u64)hi << 32 | txbd->wb.tstamp;
-}
-
-static void enetc_tstamp_tx(struct sk_buff *skb, u64 tstamp)
-{
-	struct skb_shared_hwtstamps shhwtstamps;
-
-	if (skb_shinfo(skb)->tx_flags & SKBTX_IN_PROGRESS) {
-		memset(&shhwtstamps, 0, sizeof(shhwtstamps));
-		shhwtstamps.hwtstamp = ns_to_ktime(tstamp);
-		skb_tstamp_tx(skb, &shhwtstamps);
-	}
-}
-
-static bool enetc_clean_tx_ring(struct enetc_bdr *tx_ring, int napi_budget)
+static bool enetc_clean_tx_ring(struct enetc_bdr *tx_ring)
 {
-	int tx_frm_cnt = 0, tx_byte_cnt = 0, tx_win_drop = 0;
-	struct net_device *ndev = tx_ring->ndev;
-	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	int tx_frm_cnt = 0, tx_byte_cnt = 0;
 	struct enetc_tx_swbd *tx_swbd;
 	int i, bds_to_clean;
-	bool do_tstamp;
-	u64 tstamp = 0;
 
 	i = tx_ring->next_to_clean;
 	tx_swbd = &tx_ring->tx_swbd[i];
-	do_tstamp = false;
+
+	enetc_lock_mdio();
 	bds_to_clean = enetc_bd_ready_count(tx_ring, i);
+	enetc_unlock_mdio();
+
 
 	while (bds_to_clean && tx_frm_cnt < ENETC_DEFAULT_TX_WORK) {
 		bool is_eof = !!tx_swbd->skb;
 
-		if (unlikely(tx_swbd->check_wb)) {
-			struct enetc_ndev_priv *priv = netdev_priv(ndev);
-			union enetc_tx_bd *txbd;
-
-			txbd = ENETC_TXBD(*tx_ring, i);
-			if (!(txbd->flags & ENETC_TXBD_FLAGS_W))
-				goto no_wb;
-
-			if (tx_swbd->do_tstamp) {
-				enetc_get_tx_tstamp(&priv->si->hw, txbd,
-						    &tstamp);
-				do_tstamp = true;
-			}
-
-			if (tx_swbd->qbv_en &&
-			    txbd->wb.status & ENETC_TXBD_STATS_WIN)
-				tx_win_drop++;
-		}
-no_wb:
 		if (likely(tx_swbd->dma))
 			enetc_unmap_tx_buff(tx_ring, tx_swbd);
 
 		if (is_eof) {
-			if (unlikely(do_tstamp)) {
-				enetc_tstamp_tx(tx_swbd->skb, tstamp);
-				do_tstamp = false;
-			}
-			if (!priv->ecdev) {
-				napi_consume_skb(tx_swbd->skb, napi_budget);
-			}
 			tx_swbd->skb = NULL;
 		}
 
@@ -413,30 +215,16 @@ no_wb:
 			tx_swbd = tx_ring->tx_swbd;
 		}
 
-		/* BD iteration loop end */
-		if (is_eof) {
-			tx_frm_cnt++;
-			/* re-arm interrupt source */
-			enetc_wr_reg(tx_ring->idr, BIT(tx_ring->index) |
-				     BIT(16 + tx_ring->index));
-		}
+		enetc_lock_mdio();
 
 		if (unlikely(!bds_to_clean))
 			bds_to_clean = enetc_bd_ready_count(tx_ring, i);
+
+		enetc_unlock_mdio();
 	}
 
 	tx_ring->next_to_clean = i;
 	tx_ring->stats.packets += tx_frm_cnt;
-	tx_ring->stats.bytes += tx_byte_cnt;
-	tx_ring->stats.win_drop += tx_win_drop;
-
-	if (!priv->ecdev) {
-		if (unlikely(tx_frm_cnt && netif_carrier_ok(ndev) &&
-			     __netif_subqueue_stopped(ndev, tx_ring->index) &&
-				(enetc_bd_unused(tx_ring) >= ENETC_TXBDS_MAX_NEEDED))) {
-			netif_wake_subqueue(ndev, tx_ring->index);
-		}
-	}
 
 	return tx_frm_cnt != ENETC_DEFAULT_TX_WORK;
 }
@@ -473,7 +261,7 @@ static int enetc_refill_rx_ring(struct enetc_bdr *rx_ring, const int buff_cnt)
 
 	i = rx_ring->next_to_use;
 	rx_swbd = &rx_ring->rx_swbd[i];
-	rxbd = ENETC_RXBD(*rx_ring, i);
+	rxbd = enetc_rxbd(rx_ring, i);
 
 	for (j = 0; j < buff_cnt; j++) {
 		/* try reuse page */
@@ -490,79 +278,23 @@ static int enetc_refill_rx_ring(struct enetc_bdr *rx_ring, const int buff_cnt)
 		/* clear 'R" as well */
 		rxbd->r.lstatus = 0;
 
+		rxbd = enetc_rxbd_next(rx_ring, rxbd, i);
 		rx_swbd++;
-		rxbd++;
 		i++;
 		if (unlikely(i == rx_ring->bd_count)) {
 			i = 0;
 			rx_swbd = rx_ring->rx_swbd;
-			rxbd = ENETC_RXBD(*rx_ring, 0);
 		}
 	}
 
 	if (likely(j)) {
 		rx_ring->next_to_alloc = i; /* keep track from page reuse */
 		rx_ring->next_to_use = i;
-		/* update ENETC's consumer index */
-		enetc_wr_reg(rx_ring->rcir, i);
 	}
 
 	return j;
 }
 
-#ifdef CONFIG_FSL_ENETC_HW_TIMESTAMPING
-static void enetc_get_rx_tstamp(struct net_device *ndev,
-				union enetc_rx_bd *rxbd,
-				struct sk_buff *skb)
-{
-	struct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);
-	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-	struct enetc_hw *hw = &priv->si->hw;
-	u32 lo, hi;
-	u64 tstamp;
-
-	if (rxbd->r.flags & ENETC_RXBD_FLAG_TSTMP) {
-		lo = enetc_rd(hw, ENETC_SICTR0);
-		hi = enetc_rd(hw, ENETC_SICTR1);
-		if (lo <= rxbd->r.tstamp)
-			hi -= 1;
-
-		tstamp = (u64)hi << 32 | rxbd->r.tstamp;
-		memset(shhwtstamps, 0, sizeof(*shhwtstamps));
-		shhwtstamps->hwtstamp = ns_to_ktime(tstamp);
-	}
-}
-#endif
-
-static void enetc_get_offloads(struct enetc_bdr *rx_ring,
-			       union enetc_rx_bd *rxbd, struct sk_buff *skb)
-{
-	/* TODO: add tstamp, hashing */
-	if (rx_ring->ndev->features & NETIF_F_RXCSUM) {
-		u16 inet_csum = le16_to_cpu(rxbd->r.inet_csum);
-
-		skb->csum = csum_unfold((__force __sum16)~htons(inet_csum));
-		skb->ip_summed = CHECKSUM_COMPLETE;
-	}
-
-	/* copy VLAN to skb, if one is extracted, for now we assume it's a
-	 * standard TPID, but HW also supports custom values
-	 */
-	if (le16_to_cpu(rxbd->r.flags) & ENETC_RXBD_FLAG_VLAN)
-		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),
-				       le16_to_cpu(rxbd->r.vlan_opt));
-#ifdef CONFIG_FSL_ENETC_HW_TIMESTAMPING
-	enetc_get_rx_tstamp(rx_ring->ndev, rxbd, skb);
-#endif
-}
-
-static void enetc_process_skb(struct enetc_bdr *rx_ring,
-			      struct sk_buff *skb)
-{
-	skb_record_rx_queue(skb, rx_ring->index);
-	skb->protocol = eth_type_trans(skb, rx_ring->ndev);
-}
-
 static bool enetc_page_reusable(struct page *page)
 {
 	return (!page_is_pfmemalloc(page) && page_ref_count(page) == 1);
@@ -615,17 +347,6 @@ static void enetc_put_rx_buff(struct enetc_bdr *rx_ring,
 	rx_swbd->page = NULL;
 }
 
-static void *enetc_map_rx_buff_to_buf(struct enetc_bdr *rx_ring,
-						int i, u16 size)
-{
-	struct enetc_rx_swbd *rx_swbd = enetc_get_rx_buff(rx_ring, i, size);
-	void *ba;
-
-	ba = page_address(rx_swbd->page) + rx_swbd->page_offset - ENETC_RXB_PAD;
-	enetc_put_rx_buff(rx_ring, rx_swbd);
-
-	return ba;
-}
 static struct sk_buff *enetc_map_rx_buff_to_skb(struct enetc_bdr *rx_ring,
 						int i, u16 size)
 {
@@ -649,26 +370,19 @@ static struct sk_buff *enetc_map_rx_buff_to_skb(struct enetc_bdr *rx_ring,
 }
 
 static void enetc_add_rx_buff_to_skb(struct enetc_bdr *rx_ring, int i,
-				     u16 size, struct sk_buff *skb)
+                                     u16 size, struct sk_buff *skb)
 {
-	struct enetc_rx_swbd *rx_swbd = enetc_get_rx_buff(rx_ring, i, size);
+        struct enetc_rx_swbd *rx_swbd = enetc_get_rx_buff(rx_ring, i, size);
 
-	skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, rx_swbd->page,
-			rx_swbd->page_offset, size, ENETC_RXB_TRUESIZE);
+        skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, rx_swbd->page,
+                        rx_swbd->page_offset, size, ENETC_RXB_TRUESIZE);
 
-	enetc_put_rx_buff(rx_ring, rx_swbd);
+        enetc_put_rx_buff(rx_ring, rx_swbd);
 }
 
 #define ENETC_RXBD_BUNDLE 16 /* # of BDs to update at once */
 
-struct ec_data
-{
-	void *ptr;
-	u16 size;
-};
-
-static int enetc_clean_rx_ring(struct enetc_bdr *rx_ring,
-			       struct napi_struct *napi, int work_limit)
+static int enetc_clean_rx_ring(struct enetc_bdr *rx_ring)
 {
 	int rx_frm_cnt = 0, rx_byte_cnt = 0;
 	int cleaned_cnt, i;
@@ -677,51 +391,55 @@ static int enetc_clean_rx_ring(struct enetc_bdr *rx_ring,
 	cleaned_cnt = enetc_bd_unused(rx_ring);
 	/* next descriptor to process */
 	i = rx_ring->next_to_clean;
-	while (likely(rx_frm_cnt < work_limit)) {
+
+	while (likely(rx_frm_cnt < rx_ring->bd_count)) {
 		union enetc_rx_bd *rxbd;
 		struct sk_buff *skb;
 		u32 bd_status;
 		u16 size;
 
-		if (cleaned_cnt >= ENETC_RXBD_BUNDLE) {
-			int count = enetc_refill_rx_ring(rx_ring, cleaned_cnt);
+		enetc_lock_mdio();
+                if (cleaned_cnt >= ENETC_RXBD_BUNDLE) {
+                        int count = enetc_refill_rx_ring(rx_ring, cleaned_cnt);
 
-			cleaned_cnt -= count;
-		}
+                        /* update ENETC's consumer index */
+                        enetc_wr_reg_hot(rx_ring->rcir, rx_ring->next_to_use);
+                        cleaned_cnt -= count;
+                }
 
-		rxbd = ENETC_RXBD(*rx_ring, i);
+		rxbd = enetc_rxbd(rx_ring, i);
 		bd_status = le32_to_cpu(rxbd->r.lstatus);
-		if (!bd_status)
+		if (!bd_status) {
+			enetc_unlock_mdio();
 			break;
+		}
 
-		enetc_wr_reg(rx_ring->idr, BIT(rx_ring->index));
+		enetc_wr_reg_hot(rx_ring->idr, BIT(rx_ring->index));
 		dma_rmb(); /* for reading other rxbd fields */
 		size = le16_to_cpu(rxbd->r.buf_len);
 		skb = enetc_map_rx_buff_to_skb(rx_ring, i, size);
-		if (!skb)
+		if (!skb) {
+			enetc_unlock_mdio();
 			break;
-		enetc_get_offloads(rx_ring, rxbd, skb);
+		}
 
 		cleaned_cnt++;
-		rxbd++;
-		i++;
-		if (unlikely(i == rx_ring->bd_count)) {
+
+		rxbd = enetc_rxbd_next(rx_ring, rxbd, i);
+		if (unlikely(++i == rx_ring->bd_count))
 			i = 0;
-			rxbd = ENETC_RXBD(*rx_ring, 0);
-		}
 
 		if (unlikely(bd_status &
 			     ENETC_RXBD_LSTATUS(ENETC_RXBD_ERR_MASK))) {
+			enetc_unlock_mdio();
 			dev_kfree_skb(skb);
 			while (!(bd_status & ENETC_RXBD_LSTATUS_F)) {
 				dma_rmb();
 				bd_status = le32_to_cpu(rxbd->r.lstatus);
-				rxbd++;
-				i++;
-				if (unlikely(i == rx_ring->bd_count)) {
+
+				rxbd = enetc_rxbd_next(rx_ring, rxbd, i);
+				if (unlikely(++i == rx_ring->bd_count))
 					i = 0;
-					rxbd = ENETC_RXBD(*rx_ring, 0);
-				}
 			}
 
 			rx_ring->ndev->stats.rx_dropped++;
@@ -739,26 +457,20 @@ static int enetc_clean_rx_ring(struct enetc_bdr *rx_ring,
 				dma_rmb();
 				size = le16_to_cpu(rxbd->r.buf_len);
 			}
+
 			enetc_add_rx_buff_to_skb(rx_ring, i, size, skb);
+
 			cleaned_cnt++;
-			rxbd++;
-			i++;
-			if (unlikely(i == rx_ring->bd_count)) {
+
+			rxbd = enetc_rxbd_next(rx_ring, rxbd, i);
+			if (unlikely(++i == rx_ring->bd_count))
 				i = 0;
-				rxbd = ENETC_RXBD(*rx_ring, 0);
-			}
 		}
 
-		
 		rx_byte_cnt += skb->len;
-		if (priv->ecdev) {
-			ecdev_receive(priv->ecdev, skb->data, size);
-			dev_kfree_skb(skb);
-		}
-		else {
-			enetc_process_skb(rx_ring, skb);
-			napi_gro_receive(napi, skb);
-		}
+		ecdev_receive(priv->ecdev, skb->data, size);
+		dev_kfree_skb(skb);
+		enetc_unlock_mdio();
 		rx_frm_cnt++;
 	}
 
@@ -789,18 +501,26 @@ void enetc_get_si_caps(struct enetc_si *si)
 	si->num_rss = 0;
 	val = enetc_rd(hw, ENETC_SIPCAPR0);
 	if (val & ENETC_SIPCAPR0_RSS) {
-		u32 rsscap = enetc_rd(hw, ENETC_SIRSSCAPR);
+		u32 rss;
 
-		si->num_rss = ENETC_SIRSSCAPR_GET_NUM_RSS(rsscap);
+		rss = enetc_rd(hw, ENETC_SIRSSCAPR);
+		si->num_rss = ENETC_SIRSSCAPR_GET_NUM_RSS(rss);
 	}
+
 	if (val & ENETC_SIPCAPR0_QBV)
 		si->hw_features |= ENETC_SI_F_QBV;
+
+	if (val & ENETC_SIPCAPR0_QBU)
+		si->hw_features |= ENETC_SI_F_QBU;
+
+	if (val & ENETC_SIPCAPR0_PSFP)
+		si->hw_features |= ENETC_SI_F_PSFP;
 }
 
 static int enetc_dma_alloc_bdr(struct enetc_bdr *r, size_t bd_size)
 {
-	r->bd_base = dma_zalloc_coherent(r->dev, r->bd_count * bd_size,
-					 &r->bd_dma_base, GFP_KERNEL);
+	r->bd_base = dma_alloc_coherent(r->dev, r->bd_count * bd_size,
+					&r->bd_dma_base, GFP_KERNEL);
 	if (!r->bd_base)
 		return -ENOMEM;
 
@@ -814,6 +534,36 @@ static int enetc_dma_alloc_bdr(struct enetc_bdr *r, size_t bd_size)
 	return 0;
 }
 
+int enetc_alloc_rings(struct enetc_ndev_priv *priv)
+{
+	struct enetc_bdr *bdrs;
+	int i, j;
+	bdrs = kmalloc_array(sizeof(struct enetc_bdr), (priv->num_tx_rings
+				+priv->num_rx_rings), GFP_KERNEL);
+	if (!bdrs) {
+		return -ENOMEM;
+	}
+	for (i = 0; i < priv->num_tx_rings; i++) {
+		bdrs[i].ndev = priv->ndev;
+		bdrs[i].dev = priv->dev;
+		bdrs[i].bd_count = priv->tx_bd_count;
+		priv->tx_ring[i] = &bdrs[i];
+	}
+
+	for (j = 0; j < priv->num_rx_rings; i++, j++) {
+		bdrs[i].ndev = priv->ndev;
+		bdrs[i].dev = priv->dev;
+		bdrs[i].bd_count = priv->tx_bd_count;
+		priv->rx_ring[j] = &bdrs[i];
+	}
+	return 0;
+}
+
+void enetc_free_rings(struct enetc_ndev_priv *priv)
+{
+	kfree(priv->tx_ring[0]);
+}
+
 static int enetc_alloc_txbdr(struct enetc_bdr *txr)
 {
 	int err;
@@ -878,15 +628,19 @@ static void enetc_free_tx_resources(struct enetc_ndev_priv *priv)
 		enetc_free_txbdr(priv->tx_ring[i]);
 }
 
-static int enetc_alloc_rxbdr(struct enetc_bdr *rxr)
+static int enetc_alloc_rxbdr(struct enetc_bdr *rxr, bool extended)
 {
+	size_t size = sizeof(union enetc_rx_bd);
 	int err;
 
 	rxr->rx_swbd = vzalloc(rxr->bd_count * sizeof(struct enetc_rx_swbd));
 	if (!rxr->rx_swbd)
 		return -ENOMEM;
 
-	err = enetc_dma_alloc_bdr(rxr, sizeof(union enetc_rx_bd));
+	if (extended)
+		size *= 2;
+
+	err = enetc_dma_alloc_bdr(rxr, size);
 	if (err) {
 		vfree(rxr->rx_swbd);
 		return err;
@@ -895,6 +649,7 @@ static int enetc_alloc_rxbdr(struct enetc_bdr *rxr)
 	rxr->next_to_clean = 0;
 	rxr->next_to_use = 0;
 	rxr->next_to_alloc = 0;
+	rxr->ext_en = extended;
 
 	return 0;
 }
@@ -914,10 +669,11 @@ static void enetc_free_rxbdr(struct enetc_bdr *rxr)
 
 static int enetc_alloc_rx_resources(struct enetc_ndev_priv *priv)
 {
+	bool extended = !!(priv->active_offloads & ENETC_F_RX_TSTAMP);
 	int i, err;
 
 	for (i = 0; i < priv->num_rx_rings; i++) {
-		err = enetc_alloc_rxbdr(priv->rx_ring[i]);
+		err = enetc_alloc_rxbdr(priv->rx_ring[i], extended);
 
 		if (err)
 			goto fail;
@@ -996,8 +752,8 @@ static int enetc_alloc_cbdr(struct device *dev, struct enetc_cbdr *cbdr)
 {
 	int size = cbdr->bd_count * sizeof(struct enetc_cbd);
 
-	cbdr->bd_base = dma_zalloc_coherent(dev, size, &cbdr->bd_dma_base,
-					    GFP_KERNEL);
+	cbdr->bd_base = dma_alloc_coherent(dev, size, &cbdr->bd_dma_base,
+					   GFP_KERNEL);
 	if (!cbdr->bd_base)
 		return -ENOMEM;
 
@@ -1092,18 +848,17 @@ static int enetc_configure_si(struct enetc_ndev_priv *priv)
 void enetc_init_si_rings_params(struct enetc_ndev_priv *priv)
 {
 	struct enetc_si *si = priv->si;
-	int cpus = num_online_cpus();
 
-	priv->tx_bd_count = ENETC_BDR_DEFAULT_SIZE;
-	priv->rx_bd_count = ENETC_BDR_DEFAULT_SIZE;
+	priv->tx_bd_count = ENETC_TX_RING_DEFAULT_SIZE;
+	priv->rx_bd_count = ENETC_RX_RING_DEFAULT_SIZE;
 
 	/* Enable all available TX rings in order to configure as many
 	 * priorities as possible, when needed.
 	 * TODO: Make # of TX rings run-time configurable
 	 */
-	priv->num_rx_rings = min_t(int, cpus, si->num_rx_rings);
-	priv->num_tx_rings = si->num_tx_rings;
-	priv->bdr_int_num = cpus;
+	priv->num_rx_rings = 1;
+	priv->num_tx_rings = 1;
+	priv->bdr_int_num = 1;
 
 	/* SI specific */
 	si->cbd_ring.bd_count = ENETC_CBDR_DEFAULT_SIZE;
@@ -1170,7 +925,7 @@ static void enetc_setup_txbdr(struct enetc_hw *hw, struct enetc_bdr *tx_ring)
 	tx_ring->next_to_clean = enetc_txbdr_rd(hw, idx, ENETC_TBCIR);
 
 	/* enable Tx ints by setting pkt thr to 1 */
-	enetc_txbdr_wr(hw, idx, ENETC_TBICIR0, ENETC_TBICIR0_ICEN | 0x1);
+	enetc_txbdr_wr(hw, idx, ENETC_TBICR0, ENETC_TBICR0_ICEN | 0x1);
 
 	tbmr = ENETC_TBMR_EN;
 	if (tx_ring->ndev->features & NETIF_F_HW_VLAN_CTAG_TX)
@@ -1204,18 +959,21 @@ static void enetc_setup_rxbdr(struct enetc_hw *hw, struct enetc_bdr *rx_ring)
 	enetc_rxbdr_wr(hw, idx, ENETC_RBPIR, 0);
 
 	/* enable Rx ints by setting pkt thr to 1 */
-	enetc_rxbdr_wr(hw, idx, ENETC_RBICIR0, ENETC_RBICIR0_ICEN | 0x1);
+	enetc_rxbdr_wr(hw, idx, ENETC_RBICR0, ENETC_RBICR0_ICEN | 0x1);
 
 	rbmr = ENETC_RBMR_EN;
+
+	if (rx_ring->ext_en)
+		rbmr |= ENETC_RBMR_BDS;
+
 	if (rx_ring->ndev->features & NETIF_F_HW_VLAN_CTAG_RX)
 		rbmr |= ENETC_RBMR_VTE;
-	if (enetc_has_extended_rxbds())
-		rbmr |= ENETC_RBMR_BDS;
 
 	rx_ring->rcir = hw->reg + ENETC_BDR(RX, idx, ENETC_RBCIR);
 	rx_ring->idr = hw->reg + ENETC_SIRXIDR;
 
 	enetc_refill_rx_ring(rx_ring, enetc_bd_unused(rx_ring));
+	enetc_wr(hw, ENETC_SIRXIDR, rx_ring->next_to_use);
 
 	/* enable ring */
 	enetc_rxbdr_wr(hw, idx, ENETC_RBMR, rbmr);
@@ -1224,7 +982,6 @@ static void enetc_setup_rxbdr(struct enetc_hw *hw, struct enetc_bdr *rx_ring)
 static void enetc_setup_bdrs(struct enetc_ndev_priv *priv)
 {
 	int i;
-
 	for (i = 0; i < priv->num_tx_rings; i++)
 		enetc_setup_txbdr(&priv->si->hw, priv->tx_ring[i]);
 
@@ -1273,88 +1030,7 @@ static void enetc_clear_bdrs(struct enetc_ndev_priv *priv)
 	udelay(1);
 }
 
-static int enetc_setup_irqs(struct enetc_ndev_priv *priv)
-{
-	struct pci_dev *pdev = priv->si->pdev;
-	cpumask_t cpu_mask;
-	int i, j, err;
-
-	for (i = 0; i < priv->bdr_int_num; i++) {
-		int irq = pci_irq_vector(pdev, ENETC_BDR_INT_BASE_IDX + i);
-		struct enetc_int_vector *v = priv->int_vector[i];
-		int entry = ENETC_BDR_INT_BASE_IDX + i;
-		struct enetc_hw *hw = &priv->si->hw;
-
-		snprintf(v->name, sizeof(v->name), "%s-rxtx%d",
-			 priv->ndev->name, i);
-
-		if (!priv->ecdev) {
-			err = request_irq(irq, enetc_msix, 0, v->name, v);
-			if (err) {
-				dev_err(priv->dev, "request_irq() failed!\n");
-				goto irq_err;
-			}
-		}
-		v->tbier_base = hw->reg + ENETC_BDR(TX, 0, ENETC_TBIER);
-		v->rbier = hw->reg + ENETC_BDR(RX, i, ENETC_RBIER);
-
-		enetc_wr(hw, ENETC_SIMSIRRV(i), entry);
-
-		for (j = 0; j < v->count_tx_rings; j++) {
-			int idx = v->tx_ring[j].index;
-
-			enetc_wr(hw, ENETC_SIMSITRV(idx), entry);
-		}
-		cpumask_clear(&cpu_mask);
-		cpumask_set_cpu(i % num_online_cpus(), &cpu_mask);
-		irq_set_affinity_hint(irq, &cpu_mask);
-	}
-
-	return 0;
-
-irq_err:
-	while (i--) {
-		int irq = pci_irq_vector(pdev, ENETC_BDR_INT_BASE_IDX + i);
-
-		irq_set_affinity_hint(irq, NULL);
-		free_irq(irq, priv->int_vector[i]);
-	}
-
-	return err;
-}
-
-static void enetc_free_irqs(struct enetc_ndev_priv *priv)
-{
-	struct pci_dev *pdev = priv->si->pdev;
-	int i;
-
-	for (i = 0; i < priv->bdr_int_num; i++) {
-		int irq = pci_irq_vector(pdev, ENETC_BDR_INT_BASE_IDX + i);
-
-		irq_set_affinity_hint(irq, NULL);
-		if (!priv->ecdev) {
-			free_irq(irq, priv->int_vector[i]);
-		}
-	}
-}
-
-static void enetc_enable_interrupts(struct enetc_ndev_priv *priv)
-{
-	int i;
-
-	/* enable Tx & Rx event indication */
-	for (i = 0; i < priv->num_rx_rings; i++) {
-		enetc_rxbdr_wr(&priv->si->hw, i,
-			       ENETC_RBIER, ENETC_RBIER_RXTIE);
-	}
-
-	for (i = 0; i < priv->num_tx_rings; i++) {
-		enetc_txbdr_wr(&priv->si->hw, i,
-			       ENETC_TBIER, ENETC_TBIER_TXTIE);
-	}
-}
-
-static void enetc_disable_interrupts(struct enetc_ndev_priv *priv)
+static void enetc_clear_interrupts(struct enetc_ndev_priv *priv)
 {
 	int i;
 
@@ -1365,46 +1041,46 @@ static void enetc_disable_interrupts(struct enetc_ndev_priv *priv)
 		enetc_rxbdr_wr(&priv->si->hw, i, ENETC_RBIER, 0);
 }
 
-static void adjust_link(struct net_device *ndev)
-{
-	struct phy_device *phydev = ndev->phydev;
-
-	phy_print_status(phydev);
-}
-
-static int enetc_phy_connect(struct net_device *ndev)
+static int enetc_phylink_connect(struct net_device *ndev)
 {
 	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-	struct phy_device *phydev;
+	struct ethtool_eee edata;
+	int err;
 
-	if (!priv->phy_node)
+	if (!priv->phylink)
 		return 0; /* phy-less mode */
 
-	phydev = of_phy_connect(ndev, priv->phy_node, &adjust_link,
-				0, priv->if_mode);
-	if (!phydev) {
+	err = phylink_of_phy_connect(priv->phylink, priv->dev->of_node, 0);
+	if (err) {
 		dev_err(&ndev->dev, "could not attach to PHY\n");
-		return -ENODEV;
+		return err;
 	}
 
-	phy_attached_info(phydev);
+	/* disable EEE autoneg, until ENETC driver supports it */
+	memset(&edata, 0, sizeof(struct ethtool_eee));
+	phylink_ethtool_set_eee(priv->phylink, &edata);
 
 	return 0;
 }
 
+void enetc_start(struct net_device *ndev)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+
+	if (priv->phylink)
+		phylink_start(priv->phylink);
+}
+
 int enetc_open(struct net_device *ndev)
 {
 	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-	int i, err;
+	int err;
 
-	err = enetc_setup_irqs(priv);
-	if (err)
-		return err;
+	enetc_clear_interrupts(priv);
 
-	err = enetc_phy_connect(ndev);
+	err = enetc_phylink_connect(ndev);
 	if (err)
 		goto err_phy_connect;
-
 	err = enetc_alloc_tx_resources(priv);
 	if (err)
 		goto err_alloc_tx;
@@ -1414,368 +1090,43 @@ int enetc_open(struct net_device *ndev)
 		goto err_alloc_rx;
 
 	enetc_setup_bdrs(priv);
-
-	err = netif_set_real_num_tx_queues(ndev, priv->num_tx_rings);
-	if (err)
-		goto err_set_queues;
-
-	err = netif_set_real_num_rx_queues(ndev, priv->num_rx_rings);
-	if (err)
-		goto err_set_queues;
-
-	if (!priv->ecdev) {
-		for (i = 0; i < priv->bdr_int_num; i++)
-			napi_enable(&priv->int_vector[i]->napi);
-		enetc_enable_interrupts(priv);
-	}
-
-	if (ndev->phydev)
-		phy_start(ndev->phydev);
-	else
-		if (!priv->ecdev) {
-			netif_carrier_off(ndev);
-		}
-
-	if (!priv->ecdev) {
-		netif_tx_start_all_queues(ndev);
-	}
-
+	enetc_start(ndev);
+	ecdev_set_link(priv->ecdev, 1);
 	return 0;
 
-err_set_queues:
-	enetc_free_rx_resources(priv);
 err_alloc_rx:
-	enetc_free_tx_resources(priv);
+	enetc_free_rx_resources(priv);
 err_alloc_tx:
-	if (ndev->phydev)
-		phy_disconnect(ndev->phydev);
+	enetc_free_tx_resources(priv);
+	if (priv->phylink)
+		phylink_disconnect_phy(priv->phylink);
 err_phy_connect:
-	enetc_free_irqs(priv);
 
 	return err;
 }
 
-int enetc_close(struct net_device *ndev)
+void enetc_stop(struct net_device *ndev)
 {
 	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-	int i;
+	if (priv->phylink)
+		phylink_stop(priv->phylink);
 
-	if (!priv->ecdev) {
-		netif_tx_stop_all_queues(ndev);
-	}
-
-	if (ndev->phydev) {
-		phy_stop(ndev->phydev);
-		phy_disconnect(ndev->phydev);
-	} else {
-		if (!priv->ecdev) {
-			netif_carrier_off(ndev);
-		}
-	}
+}
 
-	if (!priv->ecdev) {
-		for (i = 0; i < priv->bdr_int_num; i++) {
-			napi_synchronize(&priv->int_vector[i]->napi);
-			napi_disable(&priv->int_vector[i]->napi);
-		}
+int enetc_close(struct net_device *ndev)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
 
-		enetc_disable_interrupts(priv);
-	}
+	enetc_stop(ndev);
 	enetc_clear_bdrs(priv);
 
+	if (priv->phylink)
+		phylink_disconnect_phy(priv->phylink);
 	enetc_free_rxtx_rings(priv);
 	enetc_free_rx_resources(priv);
 	enetc_free_tx_resources(priv);
-	enetc_free_irqs(priv);
-
-	return 0;
-}
-
-int enetc_setup_tc(struct net_device *ndev, enum tc_setup_type type,
-		   void *type_data)
-{
-	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-	struct tc_mqprio_qopt *mqprio = type_data;
-	struct enetc_bdr *tx_ring;
-	int i;
-	u8 num_tc;
-	u32 val;
-
-	if (type != TC_SETUP_MQPRIO)
-		return -EOPNOTSUPP;
-
-	mqprio->hw = TC_MQPRIO_HW_OFFLOAD_TCS;
-	num_tc = mqprio->num_tc;
-
-	if (!num_tc) {
-		netdev_reset_tc(ndev);
-		netif_set_real_num_tx_queues(ndev, ENETC_MAX_NUM_TXQS);
-
-		/* Reset all ring priorities to 0 */
-		for (i = 0; i < priv->num_tx_rings; i++) {
-			tx_ring = priv->tx_ring[i];
-			val = enetc_txbdr_rd(&priv->si->hw, tx_ring->index,
-					     ENETC_TBMR);
-			val &= ~ENETC_TBMR_PRIO_MASK;
-			enetc_txbdr_wr(&priv->si->hw, tx_ring->index,
-				       ENETC_TBMR, val);
-		}
-
-		return 0;
-	}
-
-	/* Check if we have enough BD rings available to accommodate all TCs */
-	if (num_tc > priv->num_tx_rings) {
-		netdev_err(ndev, "Max %d traffic classes supported\n",
-			   priv->num_tx_rings);
-		return -EINVAL;
-	}
-
-	/* For the moment, we use only one BD ring per TC.
-	 *
-	 * Configure num_tc BD rings with increasing priorities.
-	 */
-	for (i = 0; i < num_tc; i++) {
-		tx_ring = priv->tx_ring[i];
-		val = enetc_txbdr_rd(&priv->si->hw, tx_ring->index, ENETC_TBMR);
-		/* Clear the old priority and set the new one */
-		val &= ~ENETC_TBMR_PRIO_MASK;
-		val |= ENETC_TBMR_PRIO_SET(i);
-		enetc_txbdr_wr(&priv->si->hw, tx_ring->index, ENETC_TBMR, val);
-	}
-
-	/* Reset the number of netdev queues based on the TC count */
-	netif_set_real_num_tx_queues(ndev, num_tc);
-
-	netdev_set_num_tc(ndev, num_tc);
-
-	/* Each TC is associated with ENETC_TXQ_PER_TC netdev queues */
-	for (i = 0; i < num_tc; i++)
-		netdev_set_tc_queue(ndev, i, 1, i);
-
-	return 0;
-}
-
-struct net_device_stats *enetc_get_stats(struct net_device *ndev)
-{
-	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-	struct net_device_stats *stats = &ndev->stats;
-	unsigned long packets = 0, bytes = 0;
-	int i;
-
-	for (i = 0; i < priv->num_rx_rings; i++) {
-		packets += priv->rx_ring[i]->stats.packets;
-		bytes	+= priv->rx_ring[i]->stats.bytes;
-	}
-
-	stats->rx_packets = packets;
-	stats->rx_bytes = bytes;
-	bytes = 0;
-	packets = 0;
-
-	for (i = 0; i < priv->num_tx_rings; i++) {
-		packets += priv->tx_ring[i]->stats.packets;
-		bytes	+= priv->tx_ring[i]->stats.bytes;
-	}
-
-	stats->tx_packets = packets;
-	stats->tx_bytes = bytes;
-
-	return stats;
-}
-
-static int enetc_set_rss(struct net_device *ndev, int en)
-{
-	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-	struct enetc_hw *hw = &priv->si->hw;
-	u32 reg;
-
-	enetc_wr(hw, ENETC_SIRBGCR, priv->num_rx_rings);
-
-	reg = enetc_rd(hw, ENETC_SIMR);
-	reg &= ~ENETC_SIMR_RSSE;
-	reg |= (en) ? ENETC_SIMR_RSSE : 0;
-	enetc_wr(hw, ENETC_SIMR, reg);
-
-	return 0;
-}
-
-int enetc_set_features(struct net_device *ndev,
-		       netdev_features_t features)
-{
-	netdev_features_t changed = ndev->features ^ features;
-
-	if (changed & NETIF_F_RXHASH)
-		enetc_set_rss(ndev, !!(features & NETIF_F_RXHASH));
-
-	return 0;
-}
-
-#ifdef CONFIG_FSL_ENETC_HW_TIMESTAMPING
-static int enetc_hwtstamp_set(struct net_device *ndev, struct ifreq *ifr)
-{
-	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-	struct hwtstamp_config config;
-
-	if (copy_from_user(&config, ifr->ifr_data, sizeof(config)))
-		return -EFAULT;
-
-	switch (config.tx_type) {
-	case HWTSTAMP_TX_OFF:
-		priv->hw_features &= ~ENETC_F_TX_TSTAMP;
-		break;
-	case HWTSTAMP_TX_ON:
-		priv->hw_features |= ENETC_F_TX_TSTAMP;
-		break;
-	default:
-		return -ERANGE;
-	}
-
-	switch (config.rx_filter) {
-	case HWTSTAMP_FILTER_NONE:
-		priv->hw_features &= ~ENETC_F_RX_TSTAMP;
-		break;
-	default:
-		priv->hw_features |= ENETC_F_RX_TSTAMP;
-		config.rx_filter = HWTSTAMP_FILTER_ALL;
-	}
-
-	return copy_to_user(ifr->ifr_data, &config, sizeof(config)) ?
-	       -EFAULT : 0;
-}
-
-static int enetc_hwtstamp_get(struct net_device *ndev, struct ifreq *ifr)
-{
-	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-	struct hwtstamp_config config;
-
-	config.flags = 0;
-
-	if (priv->hw_features & ENETC_F_TX_TSTAMP)
-		config.tx_type = HWTSTAMP_TX_ON;
-	else
-		config.tx_type = HWTSTAMP_TX_OFF;
-
-	config.rx_filter = (priv->hw_features & ENETC_F_RX_TSTAMP) ?
-			    HWTSTAMP_FILTER_ALL : HWTSTAMP_FILTER_NONE;
-
-	return copy_to_user(ifr->ifr_data, &config, sizeof(config)) ?
-	       -EFAULT : 0;
-}
-#endif
-
-int enetc_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
-{
-#ifdef CONFIG_FSL_ENETC_HW_TIMESTAMPING
-	if (cmd == SIOCSHWTSTAMP)
-		return enetc_hwtstamp_set(dev, rq);
-	if (cmd == SIOCGHWTSTAMP)
-		return enetc_hwtstamp_get(dev, rq);
-#endif
-
-	return -EINVAL;
-}
-
-int enetc_alloc_msix(struct enetc_ndev_priv *priv)
-{
-	struct pci_dev *pdev = priv->si->pdev;
-	int size, v_tx_rings;
-	int i, n, err, nvec;
-
-	nvec = ENETC_BDR_INT_BASE_IDX + priv->bdr_int_num;
-	/* allocate MSIX for both messaging and Rx/Tx interrupts */
-	n = pci_alloc_irq_vectors(pdev, nvec, nvec, PCI_IRQ_MSIX);
-
-	if (n < 0)
-		return n;
-
-	if (n != nvec)
-		return -EPERM;
-
-	/* # of tx rings per int vector */
-	v_tx_rings = priv->num_tx_rings / priv->bdr_int_num;
-	size = sizeof(struct enetc_int_vector) +
-	       sizeof(struct enetc_bdr) * v_tx_rings;
-
-	for (i = 0; i < priv->bdr_int_num; i++) {
-		struct enetc_int_vector *v;
-		struct enetc_bdr *bdr;
-		int j;
-
-		v = kzalloc(size, GFP_KERNEL);
-		if (!v) {
-			err = -ENOMEM;
-			goto fail;
-		}
-
-		priv->int_vector[i] = v;
-
-		netif_napi_add(priv->ndev, &v->napi, enetc_poll,
-			       NAPI_POLL_WEIGHT);
-		v->count_tx_rings = v_tx_rings;
-
-		for (j = 0; j < v_tx_rings; j++) {
-			int idx;
-
-			/* default tx ring mapping policy */
-			if (priv->bdr_int_num == ENETC_MAX_BDR_INT)
-				idx = 2 * j + i; /* 2 CPUs */
-			else
-				idx = j + i * v_tx_rings; /* default */
-
-			__set_bit(idx, &v->tx_rings_map);
-			bdr = &v->tx_ring[j];
-			bdr->index = idx;
-			bdr->ndev = priv->ndev;
-			bdr->dev = priv->dev;
-			bdr->bd_count = priv->tx_bd_count;
-			priv->tx_ring[idx] = bdr;
-		}
-
-		bdr = &v->rx_ring;
-		bdr->index = i;
-		bdr->ndev = priv->ndev;
-		bdr->dev = priv->dev;
-		bdr->bd_count = priv->rx_bd_count;
-		priv->rx_ring[i] = bdr;
-	}
 
 	return 0;
-
-fail:
-	while (i--) {
-		netif_napi_del(&priv->int_vector[i]->napi);
-		kfree(priv->int_vector[i]);
-	}
-
-	pci_free_irq_vectors(pdev);
-
-	return err;
-}
-
-void enetc_free_msix(struct enetc_ndev_priv *priv)
-{
-	int i;
-
-	for (i = 0; i < priv->bdr_int_num; i++) {
-		struct enetc_int_vector *v = priv->int_vector[i];
-
-		netif_napi_del(&v->napi);
-	}
-
-	for (i = 0; i < priv->num_rx_rings; i++)
-		priv->rx_ring[i] = NULL;
-
-	for (i = 0; i < priv->num_tx_rings; i++)
-		priv->tx_ring[i] = NULL;
-
-	for (i = 0; i < priv->bdr_int_num; i++) {
-		kfree(priv->int_vector[i]);
-		priv->int_vector[i] = NULL;
-	}
-
-	/* disable all MSIX for this device */
-	pci_free_irq_vectors(priv->si->pdev);
 }
 
 static void enetc_kfree_si(struct enetc_si *si)
diff --git a/devices/enetc/enetc.h b/devices/enetc/enetc.h
index 1ade9e7..6d48d3f 100644
--- a/devices/enetc/enetc.h
+++ b/devices/enetc/enetc.h
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause) */
-/* Copyright 2017-2019 NXP */
+/* Copyright 2017-2021 NXP */
 
 #include <linux/timer.h>
 #include <linux/pci.h>
@@ -9,15 +9,17 @@
 #include <linux/skbuff.h>
 #include <linux/ethtool.h>
 #include <linux/if_vlan.h>
-#include <linux/phy.h>
+#include <linux/phylink.h>
+#include <linux/dim.h>
+#ifdef CONFIG_ENETC_TSN
 #include <net/tsn.h>
+#endif
 
 #include "enetc_hw.h"
 #include "../ecdev.h"
 #define ENETC_MAC_MAXFRM_SIZE	9600
 #define ENETC_MAX_MTU		(ENETC_MAC_MAXFRM_SIZE - \
 				(ETH_FCS_LEN + ETH_HLEN + VLAN_HLEN))
-#define ENETC_CLK  400000000
 
 struct enetc_tx_swbd {
 	struct sk_buff *skb;
@@ -48,8 +50,9 @@ struct enetc_ring_stats {
 	unsigned int win_drop;
 };
 
-#define ENETC_BDR_DEFAULT_SIZE	1024
-#define ENETC_DEFAULT_TX_WORK	256
+#define ENETC_RX_RING_DEFAULT_SIZE	512
+#define ENETC_TX_RING_DEFAULT_SIZE	256
+#define ENETC_DEFAULT_TX_WORK		(ENETC_TX_RING_DEFAULT_SIZE / 2)
 
 struct enetc_bdr {
 	struct device *dev; /* for DMA mapping */
@@ -76,6 +79,8 @@ struct enetc_bdr {
 	struct enetc_ring_stats stats;
 
 	dma_addr_t bd_dma_base;
+	u8 tsd_enable; /* Time specific departure */
+	bool ext_en; /* enable h/w descriptor extensions */
 } ____cacheline_aligned_in_smp;
 
 static inline void enetc_bdr_idx_inc(struct enetc_bdr *bdr, int *i)
@@ -107,7 +112,37 @@ struct enetc_cbdr {
 };
 
 #define ENETC_TXBD(BDR, i) (&(((union enetc_tx_bd *)((BDR).bd_base))[i]))
-#define ENETC_RXBD(BDR, i) (&(((union enetc_rx_bd *)((BDR).bd_base))[i]))
+
+static inline union enetc_rx_bd *enetc_rxbd(struct enetc_bdr *rx_ring, int i)
+{
+	int hw_idx = i;
+
+#ifdef CONFIG_FSL_ENETC_PTP_CLOCK
+	if (rx_ring->ext_en)
+		hw_idx = 2 * i;
+#endif
+	return &(((union enetc_rx_bd *)rx_ring->bd_base)[hw_idx]);
+}
+
+static inline union enetc_rx_bd *enetc_rxbd_next(struct enetc_bdr *rx_ring,
+						 union enetc_rx_bd *rxbd,
+						 int i)
+{
+	rxbd++;
+#ifdef CONFIG_FSL_ENETC_PTP_CLOCK
+	if (rx_ring->ext_en)
+		rxbd++;
+#endif
+	if (unlikely(++i == rx_ring->bd_count))
+		rxbd = rx_ring->bd_base;
+
+	return rxbd;
+}
+
+static inline union enetc_rx_bd *enetc_rxbd_ext(union enetc_rx_bd *rxbd)
+{
+	return ++rxbd;
+}
 
 struct enetc_msg_swbd {
 	void *vaddr;
@@ -144,6 +179,10 @@ enum enetc_errata {
 	ENETC_ERR_UCMCSWP	= BIT(2),
 };
 
+#define ENETC_SI_F_QBV BIT(0)
+#define ENETC_SI_F_QBU BIT(1)
+#define ENETC_SI_F_PSFP BIT(1)
+
 /* PCI IEP device data */
 struct enetc_si {
 	struct pci_dev *pdev;
@@ -159,11 +198,11 @@ struct enetc_si {
 	int num_fs_entries;
 	int num_rss; /* number of RSS buckets */
 	unsigned short pad;
-#define ENETC_SI_F_QBV	BIT(0)
 	int hw_features;
 #ifdef CONFIG_ENETC_TSN
-	 struct enetc_cbs *ecbs;
+	struct enetc_cbs *ecbs;
 #endif
+
 };
 
 #define ENETC_SI_ALIGN	32
@@ -184,28 +223,58 @@ static inline bool enetc_si_is_pf(struct enetc_si *si)
 struct enetc_int_vector {
 	void __iomem *rbier;
 	void __iomem *tbier_base;
+	void __iomem *ricr1;
 	unsigned long tx_rings_map;
 	int count_tx_rings;
-	struct napi_struct napi;
+	u32 rx_ictt;
+	u16 comp_cnt;
+	bool rx_dim_en, rx_napi_work;
+	struct napi_struct napi ____cacheline_aligned_in_smp;
+	struct dim rx_dim ____cacheline_aligned_in_smp;
 	char name[ENETC_INT_NAME_MAX];
 
-	struct enetc_bdr rx_ring ____cacheline_aligned_in_smp;
-	struct enetc_bdr tx_ring[0];
-};
+	struct enetc_bdr rx_ring;
+	struct enetc_bdr tx_ring[];
+} ____cacheline_aligned_in_smp;
 
 struct enetc_cls_rule {
 	struct ethtool_rx_flow_spec fs;
-	bool used;
+	int used;
 };
 
 #define ENETC_MAX_BDR_INT	2 /* fixed to max # of available cpus */
+struct psfp_cap {
+	u32 max_streamid;
+	u32 max_psfp_filter;
+	u32 max_psfp_gate;
+	u32 max_psfp_gatelist;
+	u32 max_psfp_meter;
+};
 
-enum enetc_hw_features {
+/* TODO: more hardware offloads */
+enum enetc_active_offloads {
 	ENETC_F_RX_TSTAMP	= BIT(0),
 	ENETC_F_TX_TSTAMP	= BIT(1),
-	ENETC_F_QBV		= BIT(2),
+	ENETC_F_QBV             = BIT(2),
+	ENETC_F_QCI		= BIT(3),
+	ENETC_F_QBU             = BIT(4),
 };
 
+/* interrupt coalescing modes */
+enum enetc_ic_mode {
+	/* one interrupt per frame */
+	ENETC_IC_NONE = 0,
+	/* activated when int coalescing time is set to a non-0 value */
+	ENETC_IC_RX_MANUAL = BIT(0),
+	ENETC_IC_TX_MANUAL = BIT(1),
+	/* use dynamic interrupt moderation */
+	ENETC_IC_RX_ADAPTIVE = BIT(2),
+};
+
+#define ENETC_RXIC_PKTTHR	min_t(u32, 256, ENETC_RX_RING_DEFAULT_SIZE / 2)
+#define ENETC_TXIC_PKTTHR	min_t(u32, 128, ENETC_TX_RING_DEFAULT_SIZE / 2)
+#define ENETC_TXIC_TIMETHR	enetc_usecs_to_cycles(600)
+
 struct enetc_ndev_priv {
 	struct net_device *ndev;
 	struct device *dev; /* dma-mapping device */
@@ -217,7 +286,9 @@ struct enetc_ndev_priv {
 	u16 rx_bd_count, tx_bd_count;
 
 	u16 msg_enable;
-	enum enetc_hw_features hw_features;
+	int active_offloads;
+
+	u32 speed; /* store speed for compare update pspeed */
 
 	struct enetc_bdr *tx_ring[16];
 	struct enetc_bdr *rx_ring[16];
@@ -225,8 +296,12 @@ struct enetc_ndev_priv {
 	struct enetc_cls_rule *cls_rules;
 	ec_device_t *ecdev;
 
-	struct device_node *phy_node;
-	phy_interface_t if_mode;
+	struct psfp_cap psfp_cap;
+
+	struct phylink *phylink;
+	int ic_mode;
+	u32 tx_ictt;
+	struct device *ptp_dev;
 };
 
 /* Messaging */
@@ -241,6 +316,9 @@ struct enetc_msg_cmd_set_primary_mac {
 
 #define ENETC_CBDR_TIMEOUT	1000 /* usecs */
 
+/* PTP driver exports */
+extern int enetc_phc_index;
+
 /* SI common */
 int enetc_pci_probe(struct pci_dev *pdev, const char *name, int sizeof_priv);
 void enetc_pci_remove(struct pci_dev *pdev);
@@ -253,12 +331,19 @@ void enetc_free_si_resources(struct enetc_ndev_priv *priv);
 
 int enetc_open(struct net_device *ndev);
 int enetc_close(struct net_device *ndev);
+void enetc_start(struct net_device *ndev);
+void enetc_stop(struct net_device *ndev);
 netdev_tx_t enetc_xmit(struct sk_buff *skb, struct net_device *ndev);
 struct net_device_stats *enetc_get_stats(struct net_device *ndev);
-int enetc_ioctl(struct net_device *dev, struct ifreq *rq, int cmd);
 int enetc_set_features(struct net_device *ndev,
 		       netdev_features_t features);
+int enetc_ioctl(struct net_device *ndev, struct ifreq *rq, int cmd);
+int enetc_setup_tc(struct net_device *ndev, enum tc_setup_type type,
+		   void *type_data);
 void ec_poll(struct net_device *ndev);
+int enetc_alloc_rings(struct enetc_ndev_priv *priv);
+void enetc_free_rings(struct enetc_ndev_priv *priv);
+
 /* ethtool */
 void enetc_set_ethtool_ops(struct net_device *ndev);
 
@@ -271,16 +356,103 @@ int enetc_set_fs_entry(struct enetc_si *si, struct enetc_cmd_rfse *rfse,
 void enetc_set_rss_key(struct enetc_hw *hw, const u8 *bytes);
 int enetc_get_rss_table(struct enetc_si *si, u32 *table, int count);
 int enetc_set_rss_table(struct enetc_si *si, const u32 *table, int count);
-int enetc_setup_tc(struct net_device *ndev, enum tc_setup_type type,
-		   void *type_data);
+int enetc_send_cmd(struct enetc_si *si, struct enetc_cbd *cbd);
+
+#ifdef CONFIG_FSL_ENETC_QOS
+int enetc_setup_tc_taprio(struct net_device *ndev, void *type_data);
+void enetc_sched_speed_set(struct enetc_ndev_priv *priv, int speed);
+int enetc_setup_tc_cbs(struct net_device *ndev, void *type_data);
+int enetc_setup_tc_txtime(struct net_device *ndev, void *type_data);
+int enetc_setup_tc_block_cb(enum tc_setup_type type, void *type_data,
+			    void *cb_priv);
+int enetc_setup_tc_psfp(struct net_device *ndev, void *type_data);
+int enetc_psfp_init(struct enetc_ndev_priv *priv);
+int enetc_psfp_clean(struct enetc_ndev_priv *priv);
+
+static inline void enetc_get_max_cap(struct enetc_ndev_priv *priv)
+{
+	u32 reg;
+
+	reg = enetc_port_rd(&priv->si->hw, ENETC_PSIDCAPR);
+	priv->psfp_cap.max_streamid = reg & ENETC_PSIDCAPR_MSK;
+	/* Port stream filter capability */
+	reg = enetc_port_rd(&priv->si->hw, ENETC_PSFCAPR);
+	priv->psfp_cap.max_psfp_filter = reg & ENETC_PSFCAPR_MSK;
+	/* Port stream gate capability */
+	reg = enetc_port_rd(&priv->si->hw, ENETC_PSGCAPR);
+	priv->psfp_cap.max_psfp_gate = (reg & ENETC_PSGCAPR_SGIT_MSK);
+	priv->psfp_cap.max_psfp_gatelist = (reg & ENETC_PSGCAPR_GCL_MSK) >> 16;
+	/* Port flow meter capability */
+	reg = enetc_port_rd(&priv->si->hw, ENETC_PFMCAPR);
+	priv->psfp_cap.max_psfp_meter = reg & ENETC_PFMCAPR_MSK;
+}
+
+static inline int enetc_psfp_enable(struct enetc_ndev_priv *priv)
+{
+	struct enetc_hw *hw = &priv->si->hw;
+	int err;
+
+	enetc_get_max_cap(priv);
+
+	err = enetc_psfp_init(priv);
+	if (err)
+		return err;
+
+	enetc_wr(hw, ENETC_PPSFPMR, enetc_rd(hw, ENETC_PPSFPMR) |
+		 ENETC_PPSFPMR_PSFPEN | ENETC_PPSFPMR_VS |
+		 ENETC_PPSFPMR_PVC | ENETC_PPSFPMR_PVZC);
+
+	return 0;
+}
+
+static inline int enetc_psfp_disable(struct enetc_ndev_priv *priv)
+{
+	struct enetc_hw *hw = &priv->si->hw;
+	int err;
+
+	err = enetc_psfp_clean(priv);
+	if (err)
+		return err;
+
+	enetc_wr(hw, ENETC_PPSFPMR, enetc_rd(hw, ENETC_PPSFPMR) &
+		 ~ENETC_PPSFPMR_PSFPEN & ~ENETC_PPSFPMR_VS &
+		 ~ENETC_PPSFPMR_PVC & ~ENETC_PPSFPMR_PVZC);
+
+	memset(&priv->psfp_cap, 0, sizeof(struct psfp_cap));
+
+	return 0;
+}
+
+#else
+#define enetc_setup_tc_taprio(ndev, type_data) -EOPNOTSUPP
+#define enetc_sched_speed_set(priv, speed) (void)0
+#define enetc_setup_tc_cbs(ndev, type_data) -EOPNOTSUPP
+#define enetc_setup_tc_txtime(ndev, type_data) -EOPNOTSUPP
+#define enetc_setup_tc_psfp(ndev, type_data) -EOPNOTSUPP
+#define enetc_setup_tc_block_cb NULL
+
+#define enetc_get_max_cap(p)		\
+	memset(&((p)->psfp_cap), 0, sizeof(struct psfp_cap))
+
+static inline int enetc_psfp_enable(struct enetc_ndev_priv *priv)
+{
+	return 0;
+}
+
+static inline int enetc_psfp_disable(struct enetc_ndev_priv *priv)
+{
+	return 0;
+}
+#endif
 #ifdef CONFIG_ENETC_TSN
 void enetc_tsn_pf_init(struct net_device *netdev, struct pci_dev *pdev);
 void enetc_tsn_pf_deinit(struct net_device *netdev);
+#ifndef CONFIG_FSL_ENETC_QOS
+void enetc_pspeed_set(struct enetc_ndev_priv *priv, int speed);
+#undef enetc_sched_speed_set
+#define enetc_sched_speed_set(priv, speed) enetc_pspeed_set(priv, speed)
+#endif
 #else
 #define enetc_tsn_pf_init(netdev, pdev) (void)0
 #define enetc_tsn_pf_deinit(netdev) (void)0
 #endif
-
-/* PTP driver exports */
-#define ENETC_PHC_INDEX_DEFAULT	-1
-extern int enetc_phc_index;
diff --git a/devices/enetc/enetc_cbdr.c b/devices/enetc/enetc_cbdr.c
index de466b7..201cbc3 100644
--- a/devices/enetc/enetc_cbdr.c
+++ b/devices/enetc/enetc_cbdr.c
@@ -32,7 +32,7 @@ static int enetc_cbd_unused(struct enetc_cbdr *r)
 		r->bd_count;
 }
 
-static int enetc_send_cmd(struct enetc_si *si, struct enetc_cbd *cbd)
+int enetc_send_cmd(struct enetc_si *si, struct enetc_cbd *cbd)
 {
 	struct enetc_cbdr *ring = &si->cbd_ring;
 	int timeout = ENETC_CBDR_TIMEOUT;
@@ -66,6 +66,9 @@ static int enetc_send_cmd(struct enetc_si *si, struct enetc_cbd *cbd)
 	if (!timeout)
 		return -EBUSY;
 
+	/* CBD may writeback data, feedback up level */
+	*cbd = *dest_cbd;
+
 	enetc_clean_cbdr(si);
 
 	return 0;
diff --git a/devices/enetc/enetc_ethtool.c b/devices/enetc/enetc_ethtool.c
index a339c88..902eeca 100644
--- a/devices/enetc/enetc_ethtool.c
+++ b/devices/enetc/enetc_ethtool.c
@@ -14,12 +14,14 @@ static const u32 enetc_si_regs[] = {
 
 static const u32 enetc_txbdr_regs[] = {
 	ENETC_TBMR, ENETC_TBSR, ENETC_TBBAR0, ENETC_TBBAR1,
-	ENETC_TBPIR, ENETC_TBCIR, ENETC_TBLENR, ENETC_TBIER
+	ENETC_TBPIR, ENETC_TBCIR, ENETC_TBLENR, ENETC_TBIER, ENETC_TBICR0,
+	ENETC_TBICR1
 };
 
 static const u32 enetc_rxbdr_regs[] = {
 	ENETC_RBMR, ENETC_RBSR, ENETC_RBBSR, ENETC_RBCIR, ENETC_RBBAR0,
-	ENETC_RBBAR1, ENETC_RBPIR, ENETC_RBLENR, ENETC_RBICIR0, ENETC_RBIER
+	ENETC_RBBAR1, ENETC_RBPIR, ENETC_RBLENR, ENETC_RBIER, ENETC_RBICR0,
+	ENETC_RBICR1
 };
 
 static const u32 enetc_port_regs[] = {
@@ -141,8 +143,8 @@ static const struct {
 	{ ENETC_PM0_R255,   "MAC rx 128-255 byte packets" },
 	{ ENETC_PM0_R511,   "MAC rx 256-511 byte packets" },
 	{ ENETC_PM0_R1023,  "MAC rx 512-1023 byte packets" },
-	{ ENETC_PM0_R1518,  "MAC rx 1024-1518 byte packets" },
-	{ ENETC_PM0_R1519X, "MAC rx 1519 to max-octet packets" },
+	{ ENETC_PM0_R1522,  "MAC rx 1024-1522 byte packets" },
+	{ ENETC_PM0_R1523X, "MAC rx 1523 to max-octet packets" },
 	{ ENETC_PM0_ROVR,   "MAC rx oversized packets" },
 	{ ENETC_PM0_RJBR,   "MAC rx jabber packets" },
 	{ ENETC_PM0_RFRG,   "MAC rx fragment packets" },
@@ -161,9 +163,13 @@ static const struct {
 	{ ENETC_PM0_TBCA,   "MAC tx broadcast frames" },
 	{ ENETC_PM0_TPKT,   "MAC tx packets" },
 	{ ENETC_PM0_TUND,   "MAC tx undersized packets" },
+	{ ENETC_PM0_T64,    "MAC tx 64 byte packets" },
 	{ ENETC_PM0_T127,   "MAC tx 65-127 byte packets" },
+	{ ENETC_PM0_T255,   "MAC tx 128-255 byte packets" },
+	{ ENETC_PM0_T511,   "MAC tx 256-511 byte packets" },
 	{ ENETC_PM0_T1023,  "MAC tx 512-1023 byte packets" },
-	{ ENETC_PM0_T1518,  "MAC tx 1024-1518 byte packets" },
+	{ ENETC_PM0_T1522,  "MAC tx 1024-1522 byte packets" },
+	{ ENETC_PM0_T1523X, "MAC tx 1523 to max-octet packets" },
 	{ ENETC_PM0_TCNP,   "MAC tx control packets" },
 	{ ENETC_PM0_TDFR,   "MAC tx deferred packets" },
 	{ ENETC_PM0_TMCOL,  "MAC tx multiple collisions" },
@@ -183,6 +189,21 @@ static const struct {
 	{ ENETC_PICDR(3),   "ICM DR3 discarded frames" },
 };
 
+static const struct {
+	int reg;
+	char name[ETH_GSTRING_LEN];
+} enetc_pmac_counters[] = {
+	{ ENETC_PM1_RFRM,   "PMAC rx frames" },
+	{ ENETC_PM1_RPKT,   "PMAC rx packets" },
+	{ ENETC_PM1_RDRP,   "PMAC rx dropped packets" },
+	{ ENETC_PM1_RFRG,   "PMAC rx fragment packets" },
+	{ ENETC_PM1_TFRM,   "PMAC tx frames" },
+	{ ENETC_PM1_TERR,   "PMAC tx error frames" },
+	{ ENETC_PM1_TPKT,   "PMAC tx packets" },
+	{ ENETC_MAC_MERGE_MMFCRXR,   "MAC merge fragment rx counter" },
+	{ ENETC_MAC_MERGE_MMFCTXR,   "MAC merge fragment tx counter"},
+};
+
 static const char rx_ring_stats[][ETH_GSTRING_LEN] = {
 	"Rx ring %2d frames",
 	"Rx ring %2d alloc errors",
@@ -190,21 +211,36 @@ static const char rx_ring_stats[][ETH_GSTRING_LEN] = {
 
 static const char tx_ring_stats[][ETH_GSTRING_LEN] = {
 	"Tx ring %2d frames",
+};
+
+static const char tx_windrop_stats[][ETH_GSTRING_LEN] = {
 	"Tx window drop %2d frames",
 };
 
 static int enetc_get_sset_count(struct net_device *ndev, int sset)
 {
 	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	int len;
 
-	if (sset == ETH_SS_STATS)
-		return ARRAY_SIZE(enetc_si_counters) +
-			ARRAY_SIZE(tx_ring_stats) * priv->num_tx_rings +
-			ARRAY_SIZE(rx_ring_stats) * priv->num_rx_rings +
-			(enetc_si_is_pf(priv->si) ?
-			ARRAY_SIZE(enetc_port_counters) : 0);
+	if (sset != ETH_SS_STATS)
+		return -EOPNOTSUPP;
+
+	len = ARRAY_SIZE(enetc_si_counters) +
+	      ARRAY_SIZE(tx_ring_stats) * priv->num_tx_rings +
+	      ARRAY_SIZE(rx_ring_stats) * priv->num_rx_rings;
+
+	if (!enetc_si_is_pf(priv->si))
+		return len;
 
-	return -EOPNOTSUPP;
+	len += ARRAY_SIZE(enetc_port_counters);
+
+	if (priv->active_offloads & ENETC_F_QBU)
+		len += ARRAY_SIZE(enetc_pmac_counters);
+
+	if (priv->active_offloads & ENETC_F_QBV)
+		len += ARRAY_SIZE(tx_windrop_stats) * priv->num_tx_rings;
+
+	return len;
 }
 
 static void enetc_get_strings(struct net_device *ndev, u32 stringset, u8 *data)
@@ -242,6 +278,28 @@ static void enetc_get_strings(struct net_device *ndev, u32 stringset, u8 *data)
 				ETH_GSTRING_LEN);
 			p += ETH_GSTRING_LEN;
 		}
+
+		if (!(priv->active_offloads & ENETC_F_QBU))
+			break;
+
+		for (i = 0; i < ARRAY_SIZE(enetc_pmac_counters); i++) {
+			strlcpy(p, enetc_pmac_counters[i].name,
+				ETH_GSTRING_LEN);
+			p += ETH_GSTRING_LEN;
+		}
+
+		if (!((priv->active_offloads & ENETC_F_QBV)))
+			break;
+
+		for (i = 0; i < priv->num_tx_rings; i++) {
+			for (j = 0; j < ARRAY_SIZE(tx_windrop_stats); j++) {
+				snprintf(p, ETH_GSTRING_LEN,
+					 tx_windrop_stats[j],
+					 i);
+				p += ETH_GSTRING_LEN;
+			}
+		}
+
 		break;
 	}
 }
@@ -256,10 +314,8 @@ static void enetc_get_ethtool_stats(struct net_device *ndev,
 	for (i = 0; i < ARRAY_SIZE(enetc_si_counters); i++)
 		data[o++] = enetc_rd64(hw, enetc_si_counters[i].reg);
 
-	for (i = 0; i < priv->num_tx_rings; i++) {
+	for (i = 0; i < priv->num_tx_rings; i++)
 		data[o++] = priv->tx_ring[i]->stats.packets;
-		data[o++] = priv->tx_ring[i]->stats.win_drop;
-	}
 
 	for (i = 0; i < priv->num_rx_rings; i++) {
 		data[o++] = priv->rx_ring[i]->stats.packets;
@@ -271,6 +327,18 @@ static void enetc_get_ethtool_stats(struct net_device *ndev,
 
 	for (i = 0; i < ARRAY_SIZE(enetc_port_counters); i++)
 		data[o++] = enetc_port_rd(hw, enetc_port_counters[i].reg);
+
+	if (!(priv->active_offloads & ENETC_F_QBU))
+		return;
+
+	for (i = 0; i < ARRAY_SIZE(enetc_pmac_counters); i++)
+		data[o++] = enetc_port_rd(hw, enetc_pmac_counters[i].reg);
+
+	if (!((priv->active_offloads & ENETC_F_QBV)))
+		return;
+
+	for (i = 0; i < priv->num_tx_rings; i++)
+		data[o++] = priv->tx_ring[i]->stats.win_drop;
 }
 
 #define ENETC_RSSHASH_L3 (RXH_L2DA | RXH_VLAN | RXH_L3_PROTO | RXH_IP_SRC | \
@@ -558,6 +626,74 @@ static void enetc_get_ringparam(struct net_device *ndev,
 	}
 }
 
+static int enetc_get_coalesce(struct net_device *ndev,
+			      struct ethtool_coalesce *ic)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	struct enetc_int_vector *v = priv->int_vector[0];
+
+	ic->tx_coalesce_usecs = enetc_cycles_to_usecs(priv->tx_ictt);
+	ic->rx_coalesce_usecs = enetc_cycles_to_usecs(v->rx_ictt);
+
+	ic->tx_max_coalesced_frames = ENETC_TXIC_PKTTHR;
+	ic->rx_max_coalesced_frames = ENETC_RXIC_PKTTHR;
+
+	ic->use_adaptive_rx_coalesce = priv->ic_mode & ENETC_IC_RX_ADAPTIVE;
+
+	return 0;
+}
+
+static int enetc_set_coalesce(struct net_device *ndev,
+			      struct ethtool_coalesce *ic)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	u32 rx_ictt, tx_ictt;
+	int i, ic_mode;
+	bool changed;
+
+	tx_ictt = enetc_usecs_to_cycles(ic->tx_coalesce_usecs);
+	rx_ictt = enetc_usecs_to_cycles(ic->rx_coalesce_usecs);
+
+	if (ic->rx_max_coalesced_frames != ENETC_RXIC_PKTTHR)
+		return -EOPNOTSUPP;
+
+	if (ic->tx_max_coalesced_frames != ENETC_TXIC_PKTTHR)
+		return -EOPNOTSUPP;
+
+	ic_mode = ENETC_IC_NONE;
+	if (ic->use_adaptive_rx_coalesce) {
+		ic_mode |= ENETC_IC_RX_ADAPTIVE;
+		rx_ictt = 0x1;
+	} else {
+		ic_mode |= rx_ictt ? ENETC_IC_RX_MANUAL : 0;
+	}
+
+	ic_mode |= tx_ictt ? ENETC_IC_TX_MANUAL : 0;
+
+	/* commit the settings */
+	changed = (ic_mode != priv->ic_mode) || (priv->tx_ictt != tx_ictt);
+
+	priv->ic_mode = ic_mode;
+	priv->tx_ictt = tx_ictt;
+
+	for (i = 0; i < priv->bdr_int_num; i++) {
+		struct enetc_int_vector *v = priv->int_vector[i];
+
+		v->rx_ictt = rx_ictt;
+		v->rx_dim_en = !!(ic_mode & ENETC_IC_RX_ADAPTIVE);
+	}
+
+	if (netif_running(ndev) && changed) {
+		/* reconfigure the operation mode of h/w interrupts,
+		 * traffic needs to be paused in the process
+		 */
+		enetc_stop(ndev);
+		enetc_start(ndev);
+	}
+
+	return 0;
+}
+
 static int enetc_get_ts_info(struct net_device *ndev,
 			     struct ethtool_ts_info *info)
 {
@@ -568,10 +704,10 @@ static int enetc_get_ts_info(struct net_device *ndev,
 		info->phc_index = *phc_idx;
 		symbol_put(enetc_phc_index);
 	} else {
-		info->phc_index = ENETC_PHC_INDEX_DEFAULT;
+		info->phc_index = -1;
 	}
 
-#ifdef CONFIG_FSL_ENETC_HW_TIMESTAMPING
+#ifdef CONFIG_FSL_ENETC_PTP_CLOCK
 	info->so_timestamping = SOF_TIMESTAMPING_TX_HARDWARE |
 				SOF_TIMESTAMPING_RX_HARDWARE |
 				SOF_TIMESTAMPING_RAW_HARDWARE;
@@ -582,12 +718,139 @@ static int enetc_get_ts_info(struct net_device *ndev,
 			   (1 << HWTSTAMP_FILTER_ALL);
 #else
 	info->so_timestamping = SOF_TIMESTAMPING_RX_SOFTWARE |
+				SOF_TIMESTAMPING_TX_SOFTWARE |
 				SOF_TIMESTAMPING_SOFTWARE;
 #endif
 	return 0;
 }
 
+static void enetc_get_wol(struct net_device *dev,
+			  struct ethtool_wolinfo *wol)
+{
+	wol->supported = 0;
+	wol->wolopts = 0;
+
+	if (dev->phydev)
+		phy_ethtool_get_wol(dev->phydev, wol);
+}
+
+static int enetc_set_wol(struct net_device *dev,
+			 struct ethtool_wolinfo *wol)
+{
+	int ret;
+
+	if (!dev->phydev)
+		return -EOPNOTSUPP;
+
+	ret = phy_ethtool_set_wol(dev->phydev, wol);
+	if (!ret)
+		device_set_wakeup_enable(&dev->dev, wol->wolopts);
+
+	return ret;
+}
+
+static int enetc_get_link_ksettings(struct net_device *dev,
+				    struct ethtool_link_ksettings *cmd)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(dev);
+
+	if (!priv->phylink)
+		return -EOPNOTSUPP;
+
+	return phylink_ethtool_ksettings_get(priv->phylink, cmd);
+}
+
+static int enetc_set_link_ksettings(struct net_device *dev,
+				    const struct ethtool_link_ksettings *cmd)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(dev);
+
+	if (!priv->phylink)
+		return -EOPNOTSUPP;
+
+	return phylink_ethtool_ksettings_set(priv->phylink, cmd);
+}
+
+static int enetc_set_preempt(struct net_device *ndev,
+			     struct ethtool_fp *pt)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	u32 preempt, temp;
+	int rafs;
+	int i;
+
+	if (!pt)
+		return -EINVAL;
+
+	if (pt->min_frag_size < 60 || pt->min_frag_size > 252)
+		return -EINVAL;
+
+	rafs = DIV_ROUND_UP((pt->min_frag_size + 4), 64) - 1;
+
+	if (!pt->fp_enabled)
+		preempt = 0x0;
+	else
+		preempt = pt->preemptible_queues_mask;
+
+	temp = enetc_rd(&priv->si->hw, ENETC_QBV_PTGCR_OFFSET);
+	if (temp & ENETC_QBV_TGE)
+		enetc_wr(&priv->si->hw, ENETC_QBV_PTGCR_OFFSET,
+			 temp & (~ENETC_QBV_TGPE));
+
+	for (i = 0; i < 8; i++) {
+		/* 1 Enabled. Traffic is transmitted on the preemptive MAC. */
+		temp = enetc_port_rd(&priv->si->hw, ENETC_PTCFPR(i));
+
+		if ((preempt >> i) & 0x1)
+			enetc_port_wr(&priv->si->hw,
+				      ENETC_PTCFPR(i),
+				      temp | ENETC_FPE);
+		else
+			enetc_port_wr(&priv->si->hw,
+				      ENETC_PTCFPR(i),
+				      temp & ~ENETC_FPE);
+	}
+
+	temp = enetc_port_rd(&priv->si->hw, ENETC_MMCSR);
+	temp &= ~ENETC_MMCSR_RAFS_MASK;
+	temp |= ENETC_MMCSR_RAFS(rafs);
+	enetc_port_wr(&priv->si->hw, ENETC_MMCSR, temp);
+
+	return 0;
+}
+
+static int enetc_get_preempt(struct net_device *ndev,
+			     struct ethtool_fp *pt)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	u32 temp;
+	int i;
+
+	if (!pt)
+		return -EINVAL;
+
+	if (enetc_port_rd(&priv->si->hw, ENETC_PFPMR) & ENETC_PFPMR_PMACE)
+		pt->fp_enabled = true;
+	else
+		pt->fp_enabled = false;
+
+	pt->preemptible_queues_mask = 0;
+	for (i = 0; i < 8; i++)
+		if (enetc_port_rd(&priv->si->hw, ENETC_PTCFPR(i)) & 0x80000000)
+			pt->preemptible_queues_mask |= 1 << i;
+
+	pt->fp_supported = !!(priv->si->hw_features & ENETC_SI_F_QBU);
+	pt->supported_queues_mask = 0xff;
+	temp = enetc_port_rd(&priv->si->hw, ENETC_MMCSR);
+	pt->min_frag_size = (ENETC_MMCSR_GET_RAFS(temp) + 1) * 64;
+
+	return 0;
+}
+
 static const struct ethtool_ops enetc_pf_ethtool_ops = {
+	.supported_coalesce_params = ETHTOOL_COALESCE_USECS |
+				     ETHTOOL_COALESCE_MAX_FRAMES |
+				     ETHTOOL_COALESCE_USE_ADAPTIVE_RX,
 	.get_regs_len = enetc_get_reglen,
 	.get_regs = enetc_get_regs,
 	.get_sset_count = enetc_get_sset_count,
@@ -600,12 +863,22 @@ static const struct ethtool_ops enetc_pf_ethtool_ops = {
 	.get_rxfh = enetc_get_rxfh,
 	.set_rxfh = enetc_set_rxfh,
 	.get_ringparam = enetc_get_ringparam,
+	.get_coalesce = enetc_get_coalesce,
+	.set_coalesce = enetc_set_coalesce,
+	.get_link_ksettings = enetc_get_link_ksettings,
+	.set_link_ksettings = enetc_set_link_ksettings,
+	.get_link = ethtool_op_get_link,
 	.get_ts_info = enetc_get_ts_info,
-	.get_link_ksettings = phy_ethtool_get_link_ksettings,
-	.set_link_ksettings = phy_ethtool_set_link_ksettings,
+	.get_wol = enetc_get_wol,
+	.set_wol = enetc_set_wol,
+	.set_preempt = enetc_set_preempt,
+	.get_preempt = enetc_get_preempt,
 };
 
 static const struct ethtool_ops enetc_vf_ethtool_ops = {
+	.supported_coalesce_params = ETHTOOL_COALESCE_USECS |
+				     ETHTOOL_COALESCE_MAX_FRAMES |
+				     ETHTOOL_COALESCE_USE_ADAPTIVE_RX,
 	.get_regs_len = enetc_get_reglen,
 	.get_regs = enetc_get_regs,
 	.get_sset_count = enetc_get_sset_count,
@@ -617,6 +890,9 @@ static const struct ethtool_ops enetc_vf_ethtool_ops = {
 	.get_rxfh = enetc_get_rxfh,
 	.set_rxfh = enetc_set_rxfh,
 	.get_ringparam = enetc_get_ringparam,
+	.get_coalesce = enetc_get_coalesce,
+	.set_coalesce = enetc_set_coalesce,
+	.get_link = ethtool_op_get_link,
 	.get_ts_info = enetc_get_ts_info,
 };
 
diff --git a/devices/enetc/enetc_hw.h b/devices/enetc/enetc_hw.h
index 2c934e9..911b01a 100644
--- a/devices/enetc/enetc_hw.h
+++ b/devices/enetc/enetc_hw.h
@@ -4,8 +4,9 @@
 #include <linux/bitops.h>
 
 /* ENETC device IDs */
-#define ENETC_DEV_ID_PF	0xe100
-#define ENETC_DEV_ID_VF	0xef00
+#define ENETC_DEV_ID_PF		0xe100
+#define ENETC_DEV_ID_VF		0xef00
+#define ENETC_DEV_ID_PTP	0xee02
 
 /* ENETC register block BAR */
 #define ENETC_BAR_REGS	0
@@ -18,6 +19,8 @@
 #define ENETC_SICTR1	0x1c
 #define ENETC_SIPCAPR0	0x20
 #define ENETC_SIPCAPR0_QBV	BIT(4)
+#define ENETC_SIPCAPR0_QBU	BIT(3)
+#define ENETC_SIPCAPR0_PSFP	BIT(9)
 #define ENETC_SIPCAPR0_RSS	BIT(8)
 #define ENETC_SIPCAPR1	0x24
 #define ENETC_SITGTGR	0x30
@@ -119,15 +122,18 @@ enum enetc_bdr_type {TX, RX};
 #define ENETC_RBIER	0xa0
 #define ENETC_RBIER_RXTIE	BIT(0)
 #define ENETC_RBIDR	0xa4
-#define ENETC_RBICIR0	0xa8
-#define ENETC_RBICIR0_ICEN	BIT(31)
+#define ENETC_RBICR0	0xa8
+#define ENETC_RBICR0_ICEN		BIT(31)
+#define ENETC_RBICR0_ICPT_MASK		0x1ff
+#define ENETC_RBICR0_SET_ICPT(n)	((n) & ENETC_RBICR0_ICPT_MASK)
+#define ENETC_RBICR1	0xac
 
 /* TX BDR reg offsets */
 #define ENETC_TBMR	0
 #define ENETC_TBSR_BUSY	BIT(0)
 #define ENETC_TBMR_VIH	BIT(9)
 #define ENETC_TBMR_PRIO_MASK		GENMASK(2, 0)
-#define ENETC_TBMR_PRIO_SET(val)	val
+#define ENETC_TBMR_SET_PRIO(val)	((val) & ENETC_TBMR_PRIO_MASK)
 #define ENETC_TBMR_EN	BIT(31)
 #define ENETC_TBSR	0x4
 #define ENETC_TBBAR0	0x10
@@ -139,8 +145,11 @@ enum enetc_bdr_type {TX, RX};
 #define ENETC_TBIER	0xa0
 #define ENETC_TBIER_TXTIE	BIT(0)
 #define ENETC_TBIDR	0xa4
-#define ENETC_TBICIR0	0xa8
-#define ENETC_TBICIR0_ICEN	BIT(31)
+#define ENETC_TBICR0	0xa8
+#define ENETC_TBICR0_ICEN		BIT(31)
+#define ENETC_TBICR0_ICPT_MASK		0xf
+#define ENETC_TBICR0_SET_ICPT(n) ((ilog2(n) + 1) & ENETC_TBICR0_ICPT_MASK)
+#define ENETC_TBICR1	0xac
 
 #define ENETC_RTBLENR_LEN(n)	((n) & ~0x7)
 
@@ -148,6 +157,11 @@ enum enetc_bdr_type {TX, RX};
 #define ENETC_PORT_BASE		0x10000
 #define ENETC_PMR		0x0000
 #define ENETC_PMR_EN	GENMASK(18, 16)
+#define ENETC_PMR_PSPEED_MASK GENMASK(11, 8)
+#define ENETC_PMR_PSPEED_10M	0
+#define ENETC_PMR_PSPEED_100M	BIT(8)
+#define ENETC_PMR_PSPEED_1000M	BIT(9)
+#define ENETC_PMR_PSPEED_2500M	BIT(10)
 #define ENETC_PSR		0x0004 /* RO */
 #define ENETC_PSIPMR		0x0018
 #define ENETC_PSIPMR_SET_UP(n)	BIT(n) /* n = SI index */
@@ -179,6 +193,8 @@ enum enetc_bdr_type {TX, RX};
 #define ENETC_PSICFGR0_SIVC(bmp)	(((bmp) & 0xff) << 24) /* VLAN_TYPE */
 
 #define ENETC_PTCCBSR0(n)	(0x1110 + (n) * 8) /* n = 0 to 7*/
+#define ENETC_CBSE		BIT(31)
+#define ENETC_CBS_BW_MASK	GENMASK(6, 0)
 #define ENETC_PTCCBSR1(n)	(0x1114 + (n) * 8) /* n = 0 to 7*/
 #define ENETC_RSSHASH_KEY_SIZE	40
 #define ENETC_PRSSK(n)		(0x1410 + (n) * 4) /* n = [0..9] */
@@ -192,6 +208,7 @@ enum enetc_bdr_type {TX, RX};
 #define ENETC_PFPMR		0x1900
 #define ENETC_PFPMR_PMACE	BIT(1)
 #define ENETC_PFPMR_MWLM	BIT(0)
+#define ENETC_EMDIO_BASE	0x1c00
 #define ENETC_PSIUMHFR0(n, err)	(((err) ? 0x1d08 : 0x1d00) + (n) * 0x10)
 #define ENETC_PSIUMHFR1(n)	(0x1d04 + (n) * 0x10)
 #define ENETC_PSIMMHFR0(n, err)	(((err) ? 0x1d00 : 0x1d08) + (n) * 0x10)
@@ -200,6 +217,9 @@ enum enetc_bdr_type {TX, RX};
 #define ENETC_PSIVHFR1(n)	(0x1e04 + (n) * 8) /* n = SI index */
 #define ENETC_MMCSR		0x1f00
 #define ENETC_MMCSR_ME		BIT(16)
+#define ENETC_MMCSR_RAFS_MASK	GENMASK(9, 8)
+#define ENETC_MMCSR_RAFS(x)	(((x) << 8) & ENETC_MMCSR_RAFS_MASK)
+#define ENETC_MMCSR_GET_RAFS(x)	(((x) & ENETC_MMCSR_RAFS_MASK) >> 8)
 #define ENETC_PTCMSDUR(n)	(0x2020 + (n) * 4) /* n = TC index [0..7] */
 
 #define ENETC_PM0_CMD_CFG	0x8008
@@ -214,12 +234,26 @@ enum enetc_bdr_type {TX, RX};
 #define ENETC_PM0_MAXFRM	0x8014
 #define ENETC_SET_TX_MTU(val)	((val) << 16)
 #define ENETC_SET_MAXFRM(val)	((val) & 0xffff)
+#define ENETC_PM0_RX_FIFO	0x801c
+#define ENETC_PM0_RX_FIFO_VAL	1
+
+#define ENETC_PM_IMDIO_BASE	0x8030
+
 #define ENETC_PM0_IF_MODE	0x8300
-#define ENETC_PM1_IF_MODE	0x9300
+#define ENETC_PM1_IF_MODE       0x9300
 #define ENETC_PMO_IFM_RG	BIT(2)
 #define ENETC_PM0_IFM_RLP	(BIT(5) | BIT(11))
 #define ENETC_PM0_IFM_RGAUTO	(BIT(15) | ENETC_PMO_IFM_RG | BIT(1))
 #define ENETC_PM0_IFM_XGMII	BIT(12)
+#define ENETC_PSIDCAPR		0x1b08
+#define ENETC_PSIDCAPR_MSK	GENMASK(15, 0)
+#define ENETC_PSFCAPR		0x1b18
+#define ENETC_PSFCAPR_MSK	GENMASK(15, 0)
+#define ENETC_PSGCAPR		0x1b28
+#define ENETC_PSGCAPR_GCL_MSK	GENMASK(18, 16)
+#define ENETC_PSGCAPR_SGIT_MSK	GENMASK(15, 0)
+#define ENETC_PFMCAPR		0x1b38
+#define ENETC_PFMCAPR_MSK	GENMASK(15, 0)
 
 /* MAC counters */
 #define ENETC_PM0_REOCT		0x8100
@@ -240,8 +274,8 @@ enum enetc_bdr_type {TX, RX};
 #define ENETC_PM0_R255		0x8180
 #define ENETC_PM0_R511		0x8188
 #define ENETC_PM0_R1023		0x8190
-#define ENETC_PM0_R1518		0x8198
-#define ENETC_PM0_R1519X	0x81A0
+#define ENETC_PM0_R1522		0x8198
+#define ENETC_PM0_R1523X	0x81A0
 #define ENETC_PM0_ROVR		0x81A8
 #define ENETC_PM0_RJBR		0x81B0
 #define ENETC_PM0_RFRG		0x81B8
@@ -260,15 +294,28 @@ enum enetc_bdr_type {TX, RX};
 #define ENETC_PM0_TBCA		0x8250
 #define ENETC_PM0_TPKT		0x8260
 #define ENETC_PM0_TUND		0x8268
+#define ENETC_PM0_T64		0x8270
 #define ENETC_PM0_T127		0x8278
+#define ENETC_PM0_T255		0x8280
+#define ENETC_PM0_T511		0x8288
 #define ENETC_PM0_T1023		0x8290
-#define ENETC_PM0_T1518		0x8298
+#define ENETC_PM0_T1522		0x8298
+#define ENETC_PM0_T1523X	0x82A0
 #define ENETC_PM0_TCNP		0x82C0
 #define ENETC_PM0_TDFR		0x82D0
 #define ENETC_PM0_TMCOL		0x82D8
 #define ENETC_PM0_TSCOL		0x82E0
 #define ENETC_PM0_TLCOL		0x82E8
 #define ENETC_PM0_TECOL		0x82F0
+#define ENETC_PM1_RFRM		0x9120
+#define ENETC_PM1_RDRP		0x9158
+#define ENETC_PM1_RPKT		0x9160
+#define ENETC_PM1_RFRG		0x91B8
+#define ENETC_PM1_TFRM		0x9220
+#define ENETC_PM1_TERR		0x9238
+#define ENETC_PM1_TPKT		0x9260
+#define ENETC_MAC_MERGE_MMFCRXR	0x1f14
+#define ENETC_MAC_MERGE_MMFCTXR	0x1f18
 
 /* Port counters */
 #define ENETC_PICDR(n)		(0x0700 + (n) * 8) /* n = [0..3] */
@@ -287,7 +334,6 @@ enum enetc_bdr_type {TX, RX};
 #define ENETC_G_EPFBLPR(n)	(0xd00 + 4 * (n))
 #define ENETC_G_EPFBLPR1_XGMII	0x80000000
 
-
 /* PCI device info */
 struct enetc_hw {
 	/* SI registers, used by all PCI functions */
@@ -298,14 +344,100 @@ struct enetc_hw {
 	void __iomem *global;
 };
 
-/* general register accessors */
-#define enetc_rd_reg(reg)	ioread32((reg))
-#define enetc_wr_reg(reg, val)	iowrite32((val), (reg))
+/* ENETC register accessors */
+
+/* MDIO issue workaround (on LS1028A) -
+ * Due to a hardware issue, an access to MDIO registers
+ * that is concurrent with other ENETC register accesses
+ * may lead to the MDIO access being dropped or corrupted.
+ * To protect the MDIO accesses a readers-writers locking
+ * scheme is used, where the MDIO register accesses are
+ * protected by write locks to insure exclusivity, while
+ * the remaining ENETC registers are accessed under read
+ * locks since they only compete with MDIO accesses.
+ */
+extern rwlock_t enetc_mdio_lock;
+
+/* use this locking primitive only on the fast datapath to
+ * group together multiple non-MDIO register accesses to
+ * minimize the overhead of the lock
+ */
+static inline void enetc_lock_mdio(void)
+{
+	read_lock(&enetc_mdio_lock);
+}
+
+static inline void enetc_unlock_mdio(void)
+{
+	read_unlock(&enetc_mdio_lock);
+}
+
+/* use these accessors only on the fast datapath under
+ * the enetc_lock_mdio() locking primitive to minimize
+ * the overhead of the lock
+ */
+static inline u32 enetc_rd_reg_hot(void __iomem *reg)
+{
+	lockdep_assert_held(&enetc_mdio_lock);
+
+	return ioread32(reg);
+}
+
+static inline void enetc_wr_reg_hot(void __iomem *reg, u32 val)
+{
+	lockdep_assert_held(&enetc_mdio_lock);
+
+	iowrite32(val, reg);
+}
+
+/* internal helpers for the MDIO w/a */
+static inline u32 _enetc_rd_reg_wa(void __iomem *reg)
+{
+	u32 val;
+
+	enetc_lock_mdio();
+	val = ioread32(reg);
+	enetc_unlock_mdio();
+
+	return val;
+}
+
+static inline void _enetc_wr_reg_wa(void __iomem *reg, u32 val)
+{
+	enetc_lock_mdio();
+	iowrite32(val, reg);
+	enetc_unlock_mdio();
+}
+
+static inline u32 _enetc_rd_mdio_reg_wa(void __iomem *reg)
+{
+	unsigned long flags;
+	u32 val;
+
+	write_lock_irqsave(&enetc_mdio_lock, flags);
+	val = ioread32(reg);
+	write_unlock_irqrestore(&enetc_mdio_lock, flags);
+
+	return val;
+}
+
+static inline void _enetc_wr_mdio_reg_wa(void __iomem *reg, u32 val)
+{
+	unsigned long flags;
+
+	write_lock_irqsave(&enetc_mdio_lock, flags);
+	iowrite32(val, reg);
+	write_unlock_irqrestore(&enetc_mdio_lock, flags);
+}
+
 #ifdef ioread64
-#define enetc_rd_reg64(reg)	ioread64((reg))
+static inline u64 _enetc_rd_reg64(void __iomem *reg)
+{
+	return ioread64(reg);
+}
 #else
 /* using this to read out stats on 32b systems */
-static inline u64 enetc_rd_reg64(void __iomem *reg)
+static inline u64 _enetc_rd_reg64(void __iomem *reg)
 {
 	u32 low, high, tmp;
 
@@ -319,12 +451,29 @@ static inline u64 enetc_rd_reg64(void __iomem *reg)
 }
 #endif
 
+static inline u64 _enetc_rd_reg64_wa(void __iomem *reg)
+{
+	u64 val;
+
+	enetc_lock_mdio();
+	val = _enetc_rd_reg64(reg);
+	enetc_unlock_mdio();
+
+	return val;
+}
+
+/* general register accessors */
+#define enetc_rd_reg(reg)		_enetc_rd_reg_wa((reg))
+#define enetc_wr_reg(reg, val)		_enetc_wr_reg_wa((reg), (val))
 #define enetc_rd(hw, off)		enetc_rd_reg((hw)->reg + (off))
 #define enetc_wr(hw, off, val)		enetc_wr_reg((hw)->reg + (off), val)
-#define enetc_rd64(hw, off)		enetc_rd_reg64((hw)->reg + (off))
+#define enetc_rd64(hw, off)		_enetc_rd_reg64_wa((hw)->reg + (off))
 /* port register accessors - PF only */
 #define enetc_port_rd(hw, off)		enetc_rd_reg((hw)->port + (off))
 #define enetc_port_wr(hw, off, val)	enetc_wr_reg((hw)->port + (off), val)
+#define enetc_port_rd_mdio(hw, off)	_enetc_rd_mdio_reg_wa((hw)->port + (off))
+#define enetc_port_wr_mdio(hw, off, val)	_enetc_wr_mdio_reg_wa(\
+							(hw)->port + (off), val)
 /* global register accessors - PF only */
 #define enetc_global_rd(hw, off)	enetc_rd_reg((hw)->global + (off))
 #define enetc_global_wr(hw, off, val)	enetc_wr_reg((hw)->global + (off), val)
@@ -352,6 +501,7 @@ union enetc_tx_bd {
 				u8 l4_csoff;
 				u8 flags;
 			}; /* default layout */
+			__le32 txstart;
 			__le32 lstatus;
 		};
 	};
@@ -372,11 +522,15 @@ union enetc_tx_bd {
 };
 
 #define ENETC_TXBD_FLAGS_L4CS	BIT(0)
+#define ENETC_TXBD_FLAGS_TSE	BIT(1)
 #define ENETC_TXBD_FLAGS_W	BIT(2)
 #define ENETC_TXBD_FLAGS_CSUM	BIT(3)
+#define ENETC_TXBD_FLAGS_TXSTART BIT(4)
 #define ENETC_TXBD_FLAGS_EX	BIT(6)
 #define ENETC_TXBD_FLAGS_F	BIT(7)
 #define ENETC_TXBD_STATS_WIN	BIT(7)
+#define ENETC_TXBD_TXSTART_MASK GENMASK(24, 0)
+#define ENETC_TXBD_FLAGS_OFFSET 24
 
 static inline void enetc_clear_tx_bd(union enetc_tx_bd *txbd)
 {
@@ -404,17 +558,12 @@ static inline __le16 enetc_txbd_l3_csoff(int start, int hdr_sz, u16 l3_flags)
 #define ENETC_TXBD_L4_UDP	BIT(5)
 #define ENETC_TXBD_L4_TCP	BIT(6)
 
-// TODO: Add support for dynamic allocation of BD rings to replace #ifdefs
-#define enetc_has_extended_rxbds() IS_ENABLED(CONFIG_FSL_ENETC_HW_TIMESTAMPING)
 #define enetc_tsn_is_enabled() IS_ENABLED(CONFIG_ENETC_TSN)
 
 union enetc_rx_bd {
 	struct {
 		__le64 addr;
 		u8 reserved[8];
-#ifdef CONFIG_FSL_ENETC_HW_TIMESTAMPING
-		u8 reserved1[16];
-#endif
 	} w;
 	struct {
 		__le16 inet_csum;
@@ -429,11 +578,11 @@ union enetc_rx_bd {
 			};
 			__le32 lstatus;
 		};
-#ifdef CONFIG_FSL_ENETC_HW_TIMESTAMPING
+	} r;
+	struct {
 		__le32 tstamp;
 		u8 reserved[12];
-#endif
-	} r;
+	} ext;
 };
 
 #define ENETC_RXBD_LSTATUS_R	BIT(30)
@@ -447,24 +596,6 @@ union enetc_rx_bd {
 #define EMETC_MAC_ADDR_FILT_RES	3 /* # of reserved entries at the beginning */
 #define ENETC_MAX_NUM_VFS	2
 
-#ifndef CONFIG_ENETC_TSN
-struct enetc_cbd {
-	union {
-		struct {
-			__le32 addr[2];
-			__le32 opt[4];
-		};
-		__le32 data[6];
-	};
-	__le16 index;
-	__le16 length;
-	u8 cmd;
-	u8 cls;
-	u8 _res;
-	u8 status_flags;
-};
-#endif
-
 #define ENETC_CBD_FLAGS_SF	BIT(7) /* short format */
 #define ENETC_CBD_STATUS_MASK	0xf
 
@@ -522,7 +653,7 @@ enum enetc_msg_cmd_type {
 	ENETC_MSG_CMD_MNG_RX_VLAN_FILTER /* manage RX VLAN table */
 };
 
-/* VS-PSI command action types */
+/* VSI-PSI command action types */
 enum enetc_msg_cmd_action_type {
 	ENETC_MSG_CMD_MNG_ADD = 1,
 	ENETC_MSG_CMD_MNG_REMOVE
@@ -536,27 +667,34 @@ struct enetc_msg_cmd_header {
 
 /* Common H/W utility functions */
 
-static inline void enetc_enable_rxvlan(struct enetc_hw *hw, int si_idx,
-				       bool en)
+static inline void enetc_bdr_enable_rxvlan(struct enetc_hw *hw, int idx,
+					   bool en)
 {
-	u32 val = enetc_rxbdr_rd(hw, si_idx, ENETC_RBMR);
+	u32 val = enetc_rxbdr_rd(hw, idx, ENETC_RBMR);
 
 	val = (val & ~ENETC_RBMR_VTE) | (en ? ENETC_RBMR_VTE : 0);
-	enetc_rxbdr_wr(hw, si_idx, ENETC_RBMR, val);
+	enetc_rxbdr_wr(hw, idx, ENETC_RBMR, val);
 }
 
-static inline void enetc_enable_txvlan(struct enetc_hw *hw, int si_idx,
-				       bool en)
+static inline void enetc_bdr_enable_txvlan(struct enetc_hw *hw, int idx,
+					   bool en)
 {
-	u32 val = enetc_txbdr_rd(hw, si_idx, ENETC_TBMR);
+	u32 val = enetc_txbdr_rd(hw, idx, ENETC_TBMR);
 
 	val = (val & ~ENETC_TBMR_VIH) | (en ? ENETC_TBMR_VIH : 0);
-	enetc_txbdr_wr(hw, si_idx, ENETC_TBMR, val);
+	enetc_txbdr_wr(hw, idx, ENETC_TBMR, val);
 }
 
-#ifdef CONFIG_ENETC_TSN
+static inline void enetc_set_bdr_prio(struct enetc_hw *hw, int bdr_idx,
+				      int prio)
+{
+	u32 val = enetc_txbdr_rd(hw, bdr_idx, ENETC_TBMR);
+
+	val &= ~ENETC_TBMR_PRIO_MASK;
+	val |= ENETC_TBMR_SET_PRIO(prio);
+	enetc_txbdr_wr(hw, bdr_idx, ENETC_TBMR, val);
+}
 
-#define ENETC_BG_V108
 enum bdcr_cmd_class {
 	BDCR_CMD_UNSPEC = 0,
 	BDCR_CMD_MAC_FILTER,
@@ -573,6 +711,39 @@ enum bdcr_cmd_class {
 	BDCR_CMD_MAX_LEN = __BDCR_CMD_MAX_LEN - 1,
 };
 
+/* class 5, command 0 */
+struct tgs_gcl_conf {
+	u8	atc;	/* init gate value */
+	u8	res[7];
+	union {
+		struct {
+			u8	res1[4];
+			__le16	acl_len;
+			u8	res2[2];
+		};
+		struct {
+			u32 cctl;
+			u32 ccth;
+		};
+	};
+};
+
+/* gate control list entry */
+struct gce {
+	__le32	period;
+	u8	gate;
+	u8	res[3];
+};
+
+/* tgs_gcl_conf address point to this data space */
+struct tgs_gcl_data {
+	__le32		btl;
+	__le32		bth;
+	__le32		ct;
+	__le32		cte;
+	struct gce	entry[];
+};
+
 /* class 7, command 0, Stream Identity Entry Configuration */
 struct streamid_conf {
 	__le32	stream_handle;	/* init gate value */
@@ -583,6 +754,7 @@ struct streamid_conf {
 		u8	en;
 };
 
+#define ENETC_CDBR_SID_ENABLE	BIT(7)
 #define ENETC_CBDR_SID_VID_MASK 0xfff
 #define ENETC_CBDR_SID_VIDM BIT(12)
 #define ENETC_CBDR_SID_TG_MASK 0xc000
@@ -597,10 +769,6 @@ struct smac_streamid_data {
 	u16	vid_vidm_tg;
 };
 
-/* class 7, command 1, query config , long format */
-/* No need structure define */
-
-#define ENETC_CDBR_SID_ENABLE	BIT(7)
 /*  Stream ID Query Response Data Buffer */
 struct streamid_query_resp {
 	u32	stream_handle;
@@ -626,6 +794,15 @@ struct streamid_stat_query_resp {
 	u64	pspi[32];
 };
 
+/* streamid_conf address point to this data space */
+struct streamid_data {
+	union {
+		u8 dmac[6];
+		u8 smac[6];
+	};
+	u16     vid_vidm_tg;
+};
+
 #define ENETC_CBDR_SFI_PRI_MASK 0x7
 #define ENETC_CBDR_SFI_PRIM		BIT(3)
 #define ENETC_CBDR_SFI_BLOV		BIT(4)
@@ -639,8 +816,7 @@ struct sfi_conf {
 		u8	multi;
 		u8	res[2];
 		u8	sthm;
-	/*
-	 * Max Service Data Unit or Flow Meter Instance Table index.
+	/* Max Service Data Unit or Flow Meter Instance Table index.
 	 * Depending on the value of FLT this represents either Max
 	 * Service Data Unit (max frame size) allowed by the filter
 	 * entry or is an index into the Flow Meter Instance table
@@ -656,35 +832,10 @@ struct sfi_conf {
 		u8	en;
 };
 
-/* class 8, command 1, Stream Filter Instance, write back, short Format */
-struct sfi_query {
-		u32	stream_handle;
-		u8	multi;
-		u8	res[2];
-		u8	sthm;
-		u16	fm_inst_table_index;
-		u16	msdu;
-		u16	sg_inst_table_index;
-		u8	res1[2];
-		u32	input_ports;
-		u8	res2[3];
-		u8	en;
-};
-
-/* class 8, command 2 stream Filter Instance status query short format */
-/* command no need structure define */
-
-/* Stream Filter Instance Query Statistics Response data */
-#if 0
-struct sfi_query_stat_resp {
-	u32	match_filter_countl;
-	u32	match_filter_counth;
-	u32	sdu_filter_drop_countl;
-	u32	sdu_filter_drop_counth;
-	u32	sdu_filter_pass_countl;
-	u32	sdu_filter_pass_counth;
-};
-#endif
+/* class 8, command 2 stream Filter Instance status query short format
+ * command no need structure define
+ * Stream Filter Instance Query Statistics Response data
+ */
 struct sfi_counter_data {
 	u32 matchl;
 	u32 matchh;
@@ -709,8 +860,10 @@ struct sfi_counter_data {
 #define ENETC_CBDR_SGI_ACLLEN_MASK 0x3
 #define ENETC_CBDR_SGI_OCLLEN_MASK 0xc
 #define	ENETC_CBDR_SGI_EN		BIT(7)
-/* class 9, command 0, Stream Gate Instance Table, Short Format */
-/* class 9, command 2, Stream Gate Instance Table entry query write back ,Short Format */
+/* class 9, command 0, Stream Gate Instance Table, Short Format
+ * class 9, command 2, Stream Gate Instance Table entry query write back
+ * Short Format
+ */
 struct sgi_table {
 	u8	res[8];
 	u8	oipv;
@@ -743,22 +896,32 @@ struct sgcl_conf {
 	};
 };
 
+#define ENETC_CBDR_SGL_IOMEN	BIT(0)
+#define ENETC_CBDR_SGL_IPVEN	BIT(3)
+#define ENETC_CBDR_SGL_GTST		BIT(4)
+#define ENETC_CBDR_SGL_IPV_MASK 0xe
+/* Stream Gate Control List Entry */
+struct sgce {
+	u32	interval;
+	u8	msdu[3];
+	u8	multi;
+};
+
 /* stream control list class 9 , cmd 1 data buffer */
 struct sgcl_data {
-	u32	btl;
-	u32 bth;
-	u32	ct;
-	u32	cte;
-	/*struct sgce	*sgcl;*/
+	u32		btl;
+	u32		bth;
+	u32		ct;
+	u32		cte;
+	struct sgce	sgcl[0];
 };
 
-/* class 9, command 2, stream gate instant table enery query, short format */
-/* write back see struct sgi_table. Do not need define.*/
-
-/* class 9, command 3 Stream Gate Control List Query Descriptor - Long Format
+/* class 9, command 2, stream gate instant table enery query, short format
+ * write back see struct sgi_table. Do not need define.
+ * class 9, command 3 Stream Gate Control List Query Descriptor - Long Format
  * ocl_len or acl_len to be 0, oper or admin would not show in the data space
  * true len will be write back in the space.
- * */
+ */
 struct sgcl_query {
 	u8 res[12];
 	u8 oacl_len;
@@ -848,49 +1011,6 @@ struct fmi_query_stat_resp {
 	u32 bef;
 };
 
-/* class 5, command 0 */
-struct tgs_gcl_conf {
-	u8	atc;	/* init gate value */
-	u8	res[7];
-	union {
-		struct {
-			u8	res1[4];
-			__le16 acl_len;
-			u8 res2[2];
-		};
-		struct {
-			u32 cctl;
-			u32 ccth;
-		};
-	};
-};
-
-#define ENETC_CBDR_SGL_IOMEN	BIT(0)
-#define ENETC_CBDR_SGL_IPVEN	BIT(3)
-#define ENETC_CBDR_SGL_GTST		BIT(4)
-#define ENETC_CBDR_SGL_IPV_MASK 0xe
-/* Stream Gate Control List Entry */
-struct sgce {
-	u32	interval;
-	u8	msdu[3];
-	u8	multi;
-};
-
-/* gate control list entry */
-struct gce {
-	u32	period;
-	u8	gate;
-	u8	res[3];
-};
-
-/* tgs_gcl_conf address point to this data space */
-struct tgs_gcl_data {
-	u32	btl;
-	u32 bth;
-	u32 ct;
-	u32 cte;
-};
-
 /* class 5, command 1 */
 struct tgs_gcl_query {
 		u8	res[12];
@@ -926,25 +1046,22 @@ struct tgs_gcl_resp {
 
 struct enetc_cbd {
 	union{
-		struct sfi_conf sfi_conf; /* Just for Stream Filter Instance Entry */
-	/*	struct sfi_query_stat_resp sfi_query_stat_resp;*/
-		struct sgi_table sgi_table; /* Just for Stream Gate Instance table set */
-	/*	struct sgi_query_resp sgi_query_resp;*/
+		struct sfi_conf sfi_conf;
+		struct sgi_table sgi_table;
 		struct sgi_query_stat_resp sgi_query_stat_resp;
 		struct fmi_conf fmi_conf;
-	/*	struct fmi_query_stat_resp fmi_query_stat_resp;*/
 		struct {
 			__le32	addr[2];
 			union {
 				__le32	opt[4];
-				struct tgs_gcl_conf		gcl_conf;
+				struct tgs_gcl_conf	gcl_conf;
 				struct tgs_gcl_query	gcl_query;
-				struct streamid_conf		sid_set;
+				struct streamid_conf	sid_set;
 				struct streamid_stat_query	sid_stat;
-				struct sgcl_conf		sgcl_conf;
+				struct sgcl_conf	sgcl_conf;
 				struct sgcl_query		sgcl_query;
 			};
-		};			/* Long format */
+		};	/* Long format */
 		__le32 data[6];
 	};
 	__le16 index;
@@ -955,61 +1072,76 @@ struct enetc_cbd {
 	u8 status_flags;
 };
 
+#define ENETC_CLK  400000000ULL
+static inline u32 enetc_cycles_to_usecs(u32 cycles)
+{
+	return (u32)div_u64(cycles * 1000000ULL, ENETC_CLK);
+}
+
+static inline u32 enetc_usecs_to_cycles(u32 usecs)
+{
+	return (u32)div_u64(usecs * ENETC_CLK, 1000000ULL);
+}
+
 #define ENETC_PTCFPR(n)		(0x1910 + (n) * 4) /* n = [0 ..7] */
 #define ENETC_FPE		BIT(31)
 
 /* Port capability register 0 */
-#define ENETC_PCAPR0_PSFPM BIT(10)
-#define ENETC_PCAPR0_PSFP BIT(9)
-#define ENETC_PCAPR0_TSN BIT(4)
-#define ENETC_PCAPR0_QBU BIT(3)
+#define ENETC_PCAPR0_PSFPM	BIT(10)
+#define ENETC_PCAPR0_PSFP	BIT(9)
+#define ENETC_PCAPR0_TSN	BIT(4)
+#define ENETC_PCAPR0_QBU	BIT(3)
 
 /* port time gating control register */
-#define QBV_PTGCR_OFFSET 0x11a00
-#define QBV_TGE		0x80000000
-#define QBV_TGPE	BIT(30)
-#define QBV_TGDROP_DISABLE BIT(29)
+#define ENETC_QBV_PTGCR_OFFSET		0x11a00
+#define ENETC_QBV_TGE			BIT(31)
+#define ENETC_QBV_TGPE			BIT(30)
+#define ENETC_QBV_TGDROP_DISABLE	BIT(29)
 
 /* Port time gating capability register */
-#define QBV_PTGCAPR_OFFSET 0x11a08
-#define QBV_MAX_GCL_LEN_MASK	0xffff
+#define ENETC_QBV_PTGCAPR_OFFSET	0x11a08
+#define ENETC_QBV_MAX_GCL_LEN_MASK	GENMASK(15, 0)
+
+/* Port time specific departure */
+#define ENETC_PTCTSDR(n)	(0x1210 + 4 * (n))
+#define ENETC_TSDE		BIT(31)
 
 /* Port time gating tick granularity register */
-#define QBV_PTGTGR_OFFSET 0x11a0c
-#define QBV_TICK_GRAN_MASK 0xffffffff
+#define ENETC_QBV_PTGTGR_OFFSET 0x11a0c
+#define ENETC_QBV_TICK_GRAN_MASK 0xffffffff
 
 /* Port time gating admin gate list status register */
-#define QBV_PTGAGLSR_OFFSET 0x11a10
+#define ENETC_QBV_PTGAGLSR_OFFSET 0x11a10
 
-#define QBV_CFG_PEND_MASK 0x00000002
+#define ENETC_QBV_CFG_PEND_MASK 0x00000002
 
 /* Port time gating admin gate list length register */
-#define QBV_PTGAGLLR_OFFSET 0x11a14
-#define QBV_ADMIN_GATE_LIST_LENGTH_MASK 0xffff
+#define ENETC_QBV_PTGAGLLR_OFFSET 0x11a14
+#define ENETC_QBV_ADMIN_GATE_LIST_LENGTH_MASK 0xffff
 
 /* Port time gating operational gate list status register */
-#define QBV_PTGOGLSR_OFFSET 0x11a18
-#define QBV_HTA_POS_MASK 0xffff0000
+#define ENETC_QBV_PTGOGLSR_OFFSET 0x11a18
+#define ENETC_QBV_HTA_POS_MASK 0xffff0000
 
-#define QBV_CURR_POS_MASK 0x0000ffff
+#define ENETC_QBV_CURR_POS_MASK 0x0000ffff
 
 /* Port time gating operational gate list length register */
-#define QBV_PTGOGLLR_OFFSET 0x11a1c
-#define QBV_OPER_GATE_LIST_LENGTH_MASK 0xffff
+#define ENETC_QBV_PTGOGLLR_OFFSET 0x11a1c
+#define ENETC_QBV_OPER_GATE_LIST_LENGTH_MASK 0xffff
 
 /* Port time gating current time register */
-#define QBV_PTGCTR_OFFSET 0x11a20
-#define QBV_CURR_TIME_MASK 0xffffffffffffffff
+#define ENETC_QBV_PTGCTR_OFFSET 0x11a20
+#define ENETC_QBV_CURR_TIME_MASK 0xffffffffffffffff
 
 /* Port traffic class a time gating control register */
-#define QBV_PTC0TGCR_OFFSET  0x11a40
-#define QBV_PTC1TGCR_OFFSET  0x11a50
-#define QBV_PTC2TGCR_OFFSET  0x11a60
-#define QBV_PTC3TGCR_OFFSET  0x11a70
-#define QBV_PTC4TGCR_OFFSET  0x11a80
-#define QBV_PTC5TGCR_OFFSET  0x11a90
-#define QBV_PTC6TGCR_OFFSET  0x11aa0
-#define QBV_PTC7TGCR_OFFSET  0x11ab0
+#define ENETC_QBV_PTC0TGCR_OFFSET  0x11a40
+#define ENETC_QBV_PTC1TGCR_OFFSET  0x11a50
+#define ENETC_QBV_PTC2TGCR_OFFSET  0x11a60
+#define ENETC_QBV_PTC3TGCR_OFFSET  0x11a70
+#define ENETC_QBV_PTC4TGCR_OFFSET  0x11a80
+#define ENETC_QBV_PTC5TGCR_OFFSET  0x11a90
+#define ENETC_QBV_PTC6TGCR_OFFSET  0x11aa0
+#define ENETC_QBV_PTC7TGCR_OFFSET  0x11ab0
 
 /* Maximum Service Data Unit. */
 #define ENETC_PTC0MSDUR 0x12020
@@ -1021,24 +1153,23 @@ struct enetc_cbd {
 #define ENETC_PTC6MSDUR 0x12038
 #define ENETC_PTC7MSDUR 0x1203c
 
-#define QBV_MAXSDU_MASK 0xffff
+#define ENETC_QBV_MAXSDU_MASK 0xffff
 
 /* Port traffic class a time gating status register */
-#define QBV_PTC0TGSR_OFFSET  0x11a44
-#define QBV_HTA_STATE_MASK  0x10000
-#define QBV_CURR_STATE_MASK 0x1
+#define ENETC_QBV_PTC0TGSR_OFFSET  0x11a44
+#define ENETC_QBV_HTA_STATE_MASK  0x10000
+#define ENETC_QBV_CURR_STATE_MASK 0x1
 
 /* Port traffic class a time gating transmission overrun counter register*/
-#define QBV_PTC0TGTOCR_OFFSET 0x11a48
-#define QBV_TX_OVERRUN_MASK 0xffffffffffffffff
+#define ENETC_QBV_PTC0TGTOCR_OFFSET 0x11a48
+#define ENETC_QBV_TX_OVERRUN_MASK 0xffffffffffffffff
 #define ENETC_TGLSTR 0xa200
 #define ENETC_TGS_MIN_DIS_MASK 0x80000000
 #define ENETC_MIN_LOOKAHEAD_MASK 0xffff
 
+/* PSFP setting */
 #define ENETC_PPSFPMR 0x11b00
 #define ENETC_PPSFPMR_PSFPEN BIT(0)
 #define ENETC_PPSFPMR_VS BIT(1)
 #define ENETC_PPSFPMR_PVC BIT(2)
 #define ENETC_PPSFPMR_PVZC BIT(3)
-
-#endif
diff --git a/devices/enetc/enetc_ptp.c b/devices/enetc/enetc_ptp.c
index 9656cd9..dabcdaf 100644
--- a/devices/enetc/enetc_ptp.c
+++ b/devices/enetc/enetc_ptp.c
@@ -1,22 +1,32 @@
 // SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
-/* Copyright 2018 NXP */
+/* Copyright 2019-2021 NXP */
 
 #include <linux/module.h>
-#include <linux/interrupt.h>
+#include <linux/of.h>
 #include <linux/fsl/ptp_qoriq.h>
 
 #include "enetc.h"
 
-int enetc_phc_index = ENETC_PHC_INDEX_DEFAULT;
+int enetc_phc_index = -1;
 EXPORT_SYMBOL(enetc_phc_index);
 
+static struct ptp_vclock_cc ptp_qoriq_vclock_cc = {
+	.cc.read		= ptp_qoriq_clock_read,
+	.cc.mask		= CYCLECOUNTER_MASK(64),
+	.cc.shift		= 28,
+	.cc.mult		= (1 << 28),
+	.refresh_interval	= (HZ * 60),
+	.mult_factor		= (1 << 6),
+	.div_factor		= 15625,
+};
+
 static struct ptp_clock_info enetc_ptp_caps = {
 	.owner		= THIS_MODULE,
 	.name		= "ENETC PTP clock",
 	.max_adj	= 512000,
-	.n_alarm	= 2,
+	.n_alarm	= 0,
 	.n_ext_ts	= 2,
-	.n_per_out	= 3,
+	.n_per_out	= 0,
 	.n_pins		= 0,
 	.pps		= 1,
 	.adjfine	= ptp_qoriq_adjfine,
@@ -24,16 +34,21 @@ static struct ptp_clock_info enetc_ptp_caps = {
 	.gettime64	= ptp_qoriq_gettime,
 	.settime64	= ptp_qoriq_settime,
 	.enable		= ptp_qoriq_enable,
+	.vclock_cc	= &ptp_qoriq_vclock_cc,
 };
 
 static int enetc_ptp_probe(struct pci_dev *pdev,
 			   const struct pci_device_id *ent)
 {
-	struct device *ptp_dev = &pdev->dev;
-	struct qoriq_ptp *qoriq_ptp;
+	struct ptp_qoriq *ptp_qoriq;
 	void __iomem *base;
 	int err, len, n;
 
+	if (pdev->dev.of_node && !of_device_is_available(pdev->dev.of_node)) {
+		dev_info(&pdev->dev, "device is disabled, skipping\n");
+		return -ENODEV;
+	}
+
 	err = pci_enable_device_mem(pdev);
 	if (err) {
 		dev_err(&pdev->dev, "device enable failed\n");
@@ -59,8 +74,8 @@ static int enetc_ptp_probe(struct pci_dev *pdev,
 
 	pci_set_master(pdev);
 
-	qoriq_ptp = kzalloc(sizeof(*qoriq_ptp), GFP_KERNEL);
-	if (!qoriq_ptp) {
+	ptp_qoriq = kzalloc(sizeof(*ptp_qoriq), GFP_KERNEL);
+	if (!ptp_qoriq) {
 		err = -ENOMEM;
 		goto err_alloc_ptp;
 	}
@@ -78,32 +93,36 @@ static int enetc_ptp_probe(struct pci_dev *pdev,
 	n = pci_alloc_irq_vectors(pdev, 1, 1, PCI_IRQ_MSIX);
 	if (n != 1) {
 		err = -EPERM;
-		goto err_irq;
+		goto err_irq_vectors;
 	}
 
-	qoriq_ptp->irq = pci_irq_vector(pdev, 0);
+	ptp_qoriq->irq = pci_irq_vector(pdev, 0);
 
-	err = request_irq(qoriq_ptp->irq, ptp_qoriq_isr, 0, DRIVER, qoriq_ptp);
+	err = request_irq(ptp_qoriq->irq, ptp_qoriq_isr, 0, DRIVER, ptp_qoriq);
 	if (err) {
 		dev_err(&pdev->dev, "request_irq() failed!\n");
 		goto err_irq;
 	}
 
-	err = qoriq_ptp_init(ptp_dev, qoriq_ptp, base, enetc_ptp_caps);
+	ptp_qoriq->dev = &pdev->dev;
+
+	err = ptp_qoriq_init(ptp_qoriq, base, &enetc_ptp_caps);
 	if (err)
 		goto err_no_clock;
 
-	enetc_phc_index = qoriq_ptp->phc_index;
-	pci_set_drvdata(pdev, qoriq_ptp);
+	enetc_phc_index = ptp_qoriq->phc_index;
+	pci_set_drvdata(pdev, ptp_qoriq);
 
 	return 0;
 
 err_no_clock:
-	free_irq(qoriq_ptp->irq, qoriq_ptp);
+	free_irq(ptp_qoriq->irq, ptp_qoriq);
 err_irq:
+	pci_free_irq_vectors(pdev);
+err_irq_vectors:
 	iounmap(base);
 err_ioremap:
-	kfree(qoriq_ptp);
+	kfree(ptp_qoriq);
 err_alloc_ptp:
 	pci_release_mem_regions(pdev);
 err_pci_mem_reg:
@@ -115,15 +134,12 @@ err_dma:
 
 static void enetc_ptp_remove(struct pci_dev *pdev)
 {
-	struct qoriq_ptp *qoriq_ptp = pci_get_drvdata(pdev);
-	struct qoriq_ptp_registers *regs = &qoriq_ptp->regs;
-
-	qoriq_write(qoriq_ptp, &regs->ctrl_regs->tmr_temask, 0);
-	qoriq_write(qoriq_ptp, &regs->ctrl_regs->tmr_ctrl,   0);
+	struct ptp_qoriq *ptp_qoriq = pci_get_drvdata(pdev);
 
-	ptp_clock_unregister(qoriq_ptp->clock);
-	iounmap(qoriq_ptp->base);
-	kfree(qoriq_ptp);
+	enetc_phc_index = -1;
+	ptp_qoriq_free(ptp_qoriq);
+	pci_free_irq_vectors(pdev);
+	kfree(ptp_qoriq);
 
 	pci_release_mem_regions(pdev);
 	pci_disable_device(pdev);
@@ -144,4 +160,4 @@ static struct pci_driver enetc_ptp_driver = {
 module_pci_driver(enetc_ptp_driver);
 
 MODULE_DESCRIPTION("ENETC PTP clock driver");
-MODULE_LICENSE("GPL");
+MODULE_LICENSE("Dual BSD/GPL");
diff --git a/devices/enetc/enetc_tsn.c b/devices/enetc/enetc_tsn.c
index fbd153e..cea239c 100644
--- a/devices/enetc/enetc_tsn.c
+++ b/devices/enetc/enetc_tsn.c
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
-/* Copyright 2017-2018 NXP */
+/* Copyright 2017-2019 NXP */
 
 #ifdef CONFIG_ENETC_TSN
 #include "enetc.h"
@@ -41,15 +41,13 @@ static int xmit_cbdr(struct enetc_si *si, int i)
 	do {
 		if (enetc_rd_reg(ring->cir) == i)
 			break;
-		udelay(10);
+		usleep_range(10, 20);
 		timeout -= 10;
 	} while (timeout);
 
 	if (!timeout)
 		return -EBUSY;
-#if 0
-	enetc_clean_cbdr(si);
-#endif
+
 	nc = ring->next_to_clean;
 
 	while (enetc_rd_reg(ring->cir) != nc) {
@@ -57,8 +55,6 @@ static int xmit_cbdr(struct enetc_si *si, int i)
 		if (dest_cbd->status_flags & ENETC_CBD_STATUS_MASK)
 			WARN_ON(1);
 
-		/*memset(dest_cbd, 0, sizeof(*dest_cbd));*/
-
 		nc = (nc + 1) % ring->bd_count;
 	}
 
@@ -67,9 +63,17 @@ static int xmit_cbdr(struct enetc_si *si, int i)
 	return 0;
 }
 
+static inline u64 get_current_time(struct enetc_si *si)
+{
+	u64 tmp = 0;
+
+	tmp = (u64)enetc_rd(&si->hw, ENETC_SICTR0);
+	return ((u64)enetc_rd(&si->hw, ENETC_SICTR1) << 32) + tmp;
+}
+
 /* Class 10: Flow Meter Instance Statistics Query Descriptor - Long Format */
 int enetc_qci_fmi_counters_get(struct net_device *ndev, u32 index,
-			struct fmi_query_stat_resp *counters)
+			       struct fmi_query_stat_resp *counters)
 {
 	struct enetc_cbd *cbdr;
 	struct fmi_query_stat_resp *fmi_data;
@@ -87,14 +91,15 @@ int enetc_qci_fmi_counters_get(struct net_device *ndev, u32 index,
 
 	data_size = sizeof(struct fmi_query_stat_resp);
 
-	fmi_data = (struct fmi_query_stat_resp *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
-	if (fmi_data == NULL)
+	fmi_data = kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (!fmi_data)
 		return -ENOMEM;
 
 	dma_size = cpu_to_le16(data_size);
 	cbdr->length = dma_size;
 
-	dma = dma_map_single(&priv->si->pdev->dev, fmi_data, data_size, DMA_FROM_DEVICE);
+	dma = dma_map_single(&priv->si->pdev->dev, fmi_data,
+			     data_size, DMA_FROM_DEVICE);
 	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
 		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
 		kfree(fmi_data);
@@ -114,11 +119,41 @@ int enetc_qci_fmi_counters_get(struct net_device *ndev, u32 index,
 
 u16 enetc_get_max_gcl_len(struct enetc_hw *hw)
 {
-	return (enetc_rd(hw, QBV_PTGCAPR_OFFSET) & QBV_MAX_GCL_LEN_MASK);
+	return (enetc_rd(hw, ENETC_QBV_PTGCAPR_OFFSET)
+		& ENETC_QBV_MAX_GCL_LEN_MASK);
 }
 
-/*
- * CBD Class 5: Time Gated Scheduling Gate Control List configuration
+void enetc_pspeed_set(struct enetc_ndev_priv *priv, int speed)
+{
+	u32 old_speed = priv->speed;
+	u32 pspeed;
+
+	if (speed == old_speed)
+		return;
+
+	switch (speed) {
+	case SPEED_1000:
+		pspeed = ENETC_PMR_PSPEED_1000M;
+		break;
+	case SPEED_2500:
+		pspeed = ENETC_PMR_PSPEED_2500M;
+		break;
+	case SPEED_100:
+		pspeed = ENETC_PMR_PSPEED_100M;
+		break;
+	case SPEED_10:
+	default:
+		pspeed = ENETC_PMR_PSPEED_10M;
+	}
+
+	priv->speed = speed;
+	enetc_port_wr(&priv->si->hw, ENETC_PMR,
+		      (enetc_port_rd(&priv->si->hw, ENETC_PMR)
+		      & (~ENETC_PMR_PSPEED_MASK))
+		      | pspeed);
+}
+
+/* CBD Class 5: Time Gated Scheduling Gate Control List configuration
  * Descriptor - Long Format
  */
 int enetc_qbv_set(struct net_device *ndev, struct tsn_qbv_conf *admin_conf)
@@ -148,21 +183,23 @@ int enetc_qbv_set(struct net_device *ndev, struct tsn_qbv_conf *admin_conf)
 	if (gcl_len > enetc_get_max_gcl_len(&priv->si->hw))
 		return -EINVAL;
 
-	temp = enetc_rd(&priv->si->hw, QBV_PTGCR_OFFSET);
-	if (admin_conf->gate_enabled && !(temp & QBV_TGE)) {
-		enetc_wr(&priv->si->hw, QBV_PTGCR_OFFSET, temp & (~QBV_TGE));
-		udelay(10);
-		enetc_wr(&priv->si->hw, QBV_PTGCR_OFFSET, temp | QBV_TGE);
+	temp = enetc_rd(&priv->si->hw, ENETC_QBV_PTGCR_OFFSET);
+	if (admin_conf->gate_enabled && !(temp & ENETC_QBV_TGE)) {
+		enetc_wr(&priv->si->hw, ENETC_QBV_PTGCR_OFFSET,
+			 temp & (~ENETC_QBV_TGE));
+		usleep_range(10, 20);
+		enetc_wr(&priv->si->hw, ENETC_QBV_PTGCR_OFFSET,
+			 temp | ENETC_QBV_TGE);
 	} else if (!admin_conf->gate_enabled) {
-		enetc_wr(&priv->si->hw, QBV_PTGCR_OFFSET, temp & (~QBV_TGE));
+		enetc_wr(&priv->si->hw, ENETC_QBV_PTGCR_OFFSET,
+			 temp & (~ENETC_QBV_TGE));
 		memcpy(&port->nd.ntdata, admin_conf, sizeof(*admin_conf));
 		call_tsn_notifiers(TSN_QBV_CONFIGCHANGETIME_ARRIVE,
 				   ndev, &port->nd);
 		return 0;
 	}
 
-	/*
-	 * Set the maximum frame size for each traffic class index
+	/* Set the maximum frame size for each traffic class index
 	 * PTCaMSDUR[MAXSDU]. The maximum frame size cannot exceed
 	 * 9,600 bytes (0x2580). Frames that exceed the limit are
 	 * discarded.
@@ -178,31 +215,34 @@ int enetc_qbv_set(struct net_device *ndev, struct tsn_qbv_conf *admin_conf)
 		enetc_wr(&priv->si->hw, ENETC_PTC7MSDUR, admin_conf->maxsdu);
 	}
 
-	/*
-	 * Configure the (administrative) gate control list using the
+	/* Configure the (administrative) gate control list using the
 	 * control BD descriptor.
 	 */
 	curr_cbd = alloc_cbdr(priv->si, &cbdr);
 
 	gcl_config = &cbdr->gcl_conf;
 
-	data_size = sizeof(struct tgs_gcl_data) + gcl_len * sizeof(struct gce);
+	data_size = struct_size(gcl_data, entry, gcl_len);
 
-	gcl_data = (struct tgs_gcl_data *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
-	if (gcl_data == NULL)
+	gcl_data = kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (!gcl_data)
 		return -ENOMEM;
 
-	gce = (struct gce *)(gcl_data + 1);
+	gce = &gcl_data->entry[0];
 
 	gcl_config->atc = admin_basic->gate_states;
 	gcl_config->acl_len = cpu_to_le16(gcl_len);
 
 	if (!admin_basic->base_time) {
-		gcl_data->btl = cpu_to_le32(enetc_rd(&priv->si->hw, ENETC_SICTR0));
-		gcl_data->bth = cpu_to_le32(enetc_rd(&priv->si->hw, ENETC_SICTR1));
+		gcl_data->btl =
+			cpu_to_le32(enetc_rd(&priv->si->hw, ENETC_SICTR0));
+		gcl_data->bth =
+			cpu_to_le32(enetc_rd(&priv->si->hw, ENETC_SICTR1));
 	} else {
-		gcl_data->btl = cpu_to_le32(lower_32_bits(admin_basic->base_time));
-		gcl_data->bth = cpu_to_le32(upper_32_bits(admin_basic->base_time));
+		gcl_data->btl =
+			cpu_to_le32(lower_32_bits(admin_basic->base_time));
+		gcl_data->bth =
+			cpu_to_le32(upper_32_bits(admin_basic->base_time));
 	}
 
 	gcl_data->ct = cpu_to_le32(admin_basic->cycle_time);
@@ -210,16 +250,19 @@ int enetc_qbv_set(struct net_device *ndev, struct tsn_qbv_conf *admin_conf)
 
 	for (i = 0; i < gcl_len; i++) {
 		struct gce *temp_gce = gce + i;
-		struct tsn_qbv_entry *temp_entry = admin_basic->control_list + i;
+		struct tsn_qbv_entry *temp_entry;
+
+		temp_entry = admin_basic->control_list + i;
 
 		temp_gce->gate = temp_entry->gate_state;
 		temp_gce->period = cpu_to_le32(temp_entry->time_interval);
 	}
 
 	cbdr->length = cpu_to_le16(data_size);
-	cbdr->status_flags = 0; /* long format command no ie */
+	cbdr->status_flags = 0;
 
-	dma = dma_map_single(&priv->si->pdev->dev, gcl_data, data_size, DMA_TO_DEVICE);
+	dma = dma_map_single(&priv->si->pdev->dev, gcl_data,
+			     data_size, DMA_TO_DEVICE);
 	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
 		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
 		kfree(gcl_data);
@@ -231,8 +274,7 @@ int enetc_qbv_set(struct net_device *ndev, struct tsn_qbv_conf *admin_conf)
 	cbdr->cmd = 0;
 	cbdr->cls = BDCR_CMD_PORT_GCL;
 
-	/*
-	 * Updated by ENETC on completion of the configuration
+	/* Updated by ENETC on completion of the configuration
 	 * command. A zero value indicates success.
 	 */
 	cbdr->status_flags = 0;
@@ -245,8 +287,6 @@ int enetc_qbv_set(struct net_device *ndev, struct tsn_qbv_conf *admin_conf)
 	port->nd.ntdata.qbv_notify.admin.base_time =
 		le32_to_cpu(gcl_config->cctl) + tempclock;
 
-	 /* config change time could be read in the, but up layer could not get it
-	 */
 	memset(cbdr, 0, sizeof(struct enetc_cbd));
 	dma_unmap_single(&priv->si->pdev->dev, dma, data_size, DMA_TO_DEVICE);
 	kfree(gcl_data);
@@ -257,8 +297,7 @@ int enetc_qbv_set(struct net_device *ndev, struct tsn_qbv_conf *admin_conf)
 	return 0;
 }
 
-/*
- * CBD Class 5: Time Gated Scheduling Gate Control List query
+/* CBD Class 5: Time Gated Scheduling Gate Control List query
  * Descriptor - Long Format
  */
 int enetc_qbv_get(struct net_device *ndev, struct tsn_qbv_conf *admin_conf)
@@ -267,7 +306,6 @@ int enetc_qbv_get(struct net_device *ndev, struct tsn_qbv_conf *admin_conf)
 	struct tgs_gcl_resp *gcl_data;
 	struct tgs_gcl_query *gcl_query;
 	struct gce *gce;
-
 	struct tsn_qbv_basic *admin_basic = &admin_conf->admin;
 	struct enetc_ndev_priv *priv = netdev_priv(ndev);
 	dma_addr_t dma;
@@ -279,7 +317,7 @@ int enetc_qbv_get(struct net_device *ndev, struct tsn_qbv_conf *admin_conf)
 	u64 temp;
 	int i;
 
-	if (enetc_rd(&priv->si->hw, QBV_PTGCR_OFFSET) & QBV_TGE) {
+	if (enetc_rd(&priv->si->hw, ENETC_QBV_PTGCR_OFFSET) & ENETC_QBV_TGE) {
 		admin_conf->gate_enabled = true;
 	} else {
 		admin_conf->gate_enabled = false;
@@ -292,10 +330,11 @@ int enetc_qbv_get(struct net_device *ndev, struct tsn_qbv_conf *admin_conf)
 
 	maxlen = enetc_get_max_gcl_len(&priv->si->hw);
 
-	data_size = sizeof(struct tgs_gcl_resp) + sizeof(struct gce) * 2 * maxlen;
+	data_size = sizeof(struct tgs_gcl_resp)
+			+ sizeof(struct gce) * 2 * maxlen;
 
-	gcl_data = (struct tgs_gcl_resp *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
-	if (gcl_data == NULL)
+	gcl_data = kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (!gcl_data)
 		return -ENOMEM;
 
 	gce = (struct gce *)(gcl_data + 1);
@@ -304,9 +343,10 @@ int enetc_qbv_get(struct net_device *ndev, struct tsn_qbv_conf *admin_conf)
 
 	dma_size = cpu_to_le16(data_size);
 	cbdr->length = dma_size;
-	cbdr->status_flags = 0; /* long format command no ie */
+	cbdr->status_flags = 0;
 
-	dma = dma_map_single(&priv->si->pdev->dev, gcl_data, data_size, DMA_FROM_DEVICE);
+	dma = dma_map_single(&priv->si->pdev->dev, gcl_data,
+			     data_size, DMA_FROM_DEVICE);
 	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
 		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
 		kfree(gcl_data);
@@ -332,9 +372,10 @@ int enetc_qbv_get(struct net_device *ndev, struct tsn_qbv_conf *admin_conf)
 	admin_basic->cycle_time = le32_to_cpu(gcl_data->act);
 	admin_basic->cycle_time_extension = le32_to_cpu(gcl_data->acte);
 
-	admin_basic->control_list =
-		kzalloc(admin_len *	sizeof(*(admin_basic->control_list)), GFP_KERNEL);
-	if (admin_basic->control_list == NULL) {
+	admin_basic->control_list = kcalloc(admin_len,
+					    sizeof(*admin_basic->control_list),
+					    GFP_KERNEL);
+	if (!admin_basic->control_list) {
 		memset(cbdr, 0, sizeof(*cbdr));
 		kfree(gcl_data);
 		return -ENOMEM;
@@ -342,7 +383,9 @@ int enetc_qbv_get(struct net_device *ndev, struct tsn_qbv_conf *admin_conf)
 
 	for (i = 0; i < admin_len; i++) {
 		struct gce *temp_gce = gce + i;
-		struct tsn_qbv_entry *temp_entry = admin_basic->control_list + i;
+		struct tsn_qbv_entry *temp_entry;
+
+		temp_entry = admin_basic->control_list + i;
 
 		temp_entry->gate_state = temp_gce->gate;
 		temp_entry->time_interval = le32_to_cpu(temp_gce->period);
@@ -386,7 +429,7 @@ int enetc_qbv_get_status(struct net_device *ndev,
 	oper_basic = &status->oper;
 	priv = netdev_priv(ndev);
 
-	if (!(enetc_rd(&priv->si->hw, QBV_PTGCR_OFFSET) & QBV_TGE))
+	if (!(enetc_rd(&priv->si->hw, ENETC_QBV_PTGCR_OFFSET) & ENETC_QBV_TGE))
 		return -EINVAL;
 
 	curr_cbd = alloc_cbdr(priv->si, &cbdr);
@@ -395,10 +438,11 @@ int enetc_qbv_get_status(struct net_device *ndev,
 
 	maxlen = enetc_get_max_gcl_len(&priv->si->hw);
 
-	data_size = sizeof(struct tgs_gcl_resp) + sizeof(struct gce) * 2 * maxlen;
+	data_size = sizeof(struct tgs_gcl_resp) +
+			sizeof(struct gce) * 2 * maxlen;
 
-	gcl_data = (struct tgs_gcl_resp *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
-	if (gcl_data == NULL)
+	gcl_data = kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (!gcl_data)
 		return -ENOMEM;
 
 	gce = (struct gce *)(gcl_data + 1);
@@ -410,7 +454,8 @@ int enetc_qbv_get_status(struct net_device *ndev,
 	cbdr->length = dma_size;
 	cbdr->status_flags = 0; /* long format command no ie */
 
-	dma = dma_map_single(&priv->si->pdev->dev, gcl_data, data_size, DMA_FROM_DEVICE);
+	dma = dma_map_single(&priv->si->pdev->dev, gcl_data,
+			     data_size, DMA_FROM_DEVICE);
 	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
 		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
 		kfree(gcl_data);
@@ -428,8 +473,8 @@ int enetc_qbv_get_status(struct net_device *ndev,
 	admin_len = le16_to_cpu(gcl_query->admin_list_len);
 	oper_len = le16_to_cpu(gcl_query->oper_list_len);
 
-	if (enetc_rd(&priv->si->hw, QBV_PTGAGLSR_OFFSET) &
-						QBV_CFG_PEND_MASK) {
+	if (enetc_rd(&priv->si->hw, ENETC_QBV_PTGAGLSR_OFFSET) &
+						ENETC_QBV_CFG_PEND_MASK) {
 		status->config_pending = true;
 		goto exit;
 	}
@@ -449,8 +494,7 @@ int enetc_qbv_get_status(struct net_device *ndev,
 	status->tick_granularity = enetc_rd(&priv->si->hw, ENETC_SITGTGR);
 
 	/* current time */
-	temp = ((u64)enetc_rd(&priv->si->hw, ENETC_SICTR1)) << 32;
-	status->current_time = enetc_rd(&priv->si->hw, ENETC_SICTR0) + temp;
+	status->current_time = get_current_time(priv->si);
 
 	status->supported_list_max = maxlen;
 
@@ -461,10 +505,9 @@ int enetc_qbv_get_status(struct net_device *ndev,
 	status->oper.cycle_time = le32_to_cpu(gcl_data->oct);
 	status->oper.cycle_time_extension = le32_to_cpu(gcl_data->octe);
 
-
 	oper_basic->control_list =
-		kzalloc(oper_len * sizeof(*(oper_basic->control_list)), GFP_KERNEL);
-	if (oper_basic->control_list == NULL) {
+		kcalloc(oper_len, sizeof(*oper_basic->control_list), GFP_KERNEL);
+	if (!oper_basic->control_list) {
 		memset(cbdr, 0, sizeof(*cbdr));
 		kfree(gcl_data);
 		return -ENOMEM;
@@ -473,6 +516,7 @@ int enetc_qbv_get_status(struct net_device *ndev,
 	for (i = 0; i < oper_len; i++) {
 		struct gce *temp_gce = gce + maxlen + i;
 		struct tsn_qbv_entry *temp_entry = oper_basic->control_list + i;
+
 		temp_entry->gate_state = temp_gce->gate;
 		temp_entry->time_interval = le32_to_cpu(temp_gce->period);
 	}
@@ -485,7 +529,7 @@ exit:
 
 /* CBD Class 7: Stream Identity Entry Set Descriptor - Long Format */
 int enetc_cb_streamid_set(struct net_device *ndev, u32 index,
-				bool en, struct tsn_cb_streamid *streamid)
+			  bool en, struct tsn_cb_streamid *streamid)
 {
 	struct enetc_cbd *cbdr;
 	void *si_data;
@@ -510,10 +554,11 @@ int enetc_cb_streamid_set(struct net_device *ndev, u32 index,
 	cbdr->status_flags = 0;
 
 	data_size = sizeof(struct null_streamid_data);
-	si_data = (struct null_streamid_data *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	si_data = kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
 	cbdr->length = cpu_to_le16(data_size);
 
-	dma = dma_map_single(&priv->si->pdev->dev, si_data, data_size, DMA_FROM_DEVICE);
+	dma = dma_map_single(&priv->si->pdev->dev, si_data,
+			     data_size, DMA_FROM_DEVICE);
 	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
 		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
 		kfree(si_data);
@@ -529,8 +574,9 @@ int enetc_cb_streamid_set(struct net_device *ndev, u32 index,
 	si_data1->dmac[3] = 0xFF;
 	si_data1->dmac[4] = 0xFF;
 	si_data1->dmac[5] = 0xFF;
-	si_data1->vid_vidm_tg =	cpu_to_le16(ENETC_CBDR_SID_VID_MASK +
-								((0x3 << 14) | ENETC_CBDR_SID_VIDM));
+	si_data1->vid_vidm_tg =
+		cpu_to_le16(ENETC_CBDR_SID_VID_MASK
+			    + ((0x3 << 14) | ENETC_CBDR_SID_VIDM));
 
 	si_conf = &cbdr->sid_set;
 	/* Only one port supported for one entry, set itself */
@@ -566,21 +612,23 @@ int enetc_cb_streamid_set(struct net_device *ndev, u32 index,
 
 	if (si_conf->id_type == 1) {
 		data_size = sizeof(struct null_streamid_data);
-		si_data = (struct null_streamid_data *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+		si_data = kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
 	} else if (si_conf->id_type == 2) {
 		data_size = sizeof(struct smac_streamid_data);
-		si_data = (struct smac_streamid_data *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
-	} else
+		si_data = kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	} else {
 		return -EINVAL;
+	}
 
-	if (si_data == NULL)
+	if (!si_data)
 		return -ENOMEM;
 
 	dma_size = cpu_to_le16(data_size);
 	cbdr->length = dma_size;
-	cbdr->status_flags = 0; /* long format command no ie */
+	cbdr->status_flags = 0;
 
-	dma = dma_map_single(&priv->si->pdev->dev, si_data, data_size, DMA_FROM_DEVICE);
+	dma = dma_map_single(&priv->si->pdev->dev, si_data,
+			     data_size, DMA_FROM_DEVICE);
 	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
 		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
 		memset(cbdr, 0, sizeof(*cbdr));
@@ -593,7 +641,8 @@ int enetc_cb_streamid_set(struct net_device *ndev, u32 index,
 	/* VIDM default to be 1.
 	 * VID Match. If set (b1) then the VID must match, otherwise
 	 * any VID is considered a match. VIDM setting is only used
-	 * when TG is set to b01. */
+	 * when TG is set to b01.
+	 */
 	if (si_conf->id_type == 1) {
 		si_data1 = (struct null_streamid_data *)si_data;
 		si_data1->dmac[0] = streamid->para.nid.dmac & 0xFF;
@@ -604,7 +653,8 @@ int enetc_cb_streamid_set(struct net_device *ndev, u32 index,
 		si_data1->dmac[5] = (streamid->para.nid.dmac >> 40) & 0xFF;
 		si_data1->vid_vidm_tg =
 		cpu_to_le16((streamid->para.nid.vid & ENETC_CBDR_SID_VID_MASK) +
-			((((u16)(streamid->para.nid.tagged) & 0x3) << 14) | ENETC_CBDR_SID_VIDM));
+			    ((((u16)(streamid->para.nid.tagged) & 0x3) << 14)
+			     | ENETC_CBDR_SID_VIDM));
 	} else if (si_conf->id_type == 2) {
 		si_data2 = (struct smac_streamid_data *)si_data;
 		si_data2->smac[0] = streamid->para.sid.smac & 0xFF;
@@ -615,7 +665,8 @@ int enetc_cb_streamid_set(struct net_device *ndev, u32 index,
 		si_data2->smac[5] = (streamid->para.sid.smac >> 40) & 0xFF;
 		si_data2->vid_vidm_tg =
 		cpu_to_le16((streamid->para.sid.vid & ENETC_CBDR_SID_VID_MASK) +
-			((((u16)(streamid->para.sid.tagged) & 0x3) << 14) | ENETC_CBDR_SID_VIDM));
+			    ((((u16)(streamid->para.sid.tagged) & 0x3) << 14)
+			     | ENETC_CBDR_SID_VIDM));
 	}
 
 	xmit_cbdr(priv->si, curr_cbd);
@@ -651,15 +702,16 @@ int enetc_cb_streamid_get(struct net_device *ndev, u32 index,
 	cbdr->status_flags = 0;
 
 	data_size = sizeof(struct streamid_query_resp);
-	si_data = (struct streamid_query_resp *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
-	if (si_data == NULL)
+	si_data = kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (!si_data)
 		return -ENOMEM;
 
 	dma_size = cpu_to_le16(data_size);
 	cbdr->length = dma_size;
 	cbdr->status_flags = 0; /* long format command no ie */
 
-	dma = dma_map_single(&priv->si->pdev->dev, si_data, data_size, DMA_FROM_DEVICE);
+	dma = dma_map_single(&priv->si->pdev->dev, si_data,
+			     data_size, DMA_FROM_DEVICE);
 	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
 		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
 		kfree(si_data);
@@ -673,25 +725,33 @@ int enetc_cb_streamid_get(struct net_device *ndev, u32 index,
 	streamid->type = si_data->id_type;
 
 	if (streamid->type == 1) {
-		streamid->para.nid.dmac = si_data->mac[0] + ((u64)si_data->mac[1] << 8)
-			+ ((u64)si_data->mac[2] << 16) + ((u64)si_data->mac[3] << 24)
-			+ ((u64)si_data->mac[4] << 32) + ((u64)si_data->mac[5] << 40);
+		streamid->para.nid.dmac = si_data->mac[0]
+			+ ((u64)si_data->mac[1] << 8)
+			+ ((u64)si_data->mac[2] << 16)
+			+ ((u64)si_data->mac[3] << 24)
+			+ ((u64)si_data->mac[4] << 32)
+			+ ((u64)si_data->mac[5] << 40);
 		/* VID Match. If set (b1) then the VID must match, otherwise
 		 * any VID is considered a match.
-		*/
+		 */
 		streamid->para.nid.vid =
-				le16_to_cpu(si_data->vid_vidm_tg & ENETC_CBDR_SID_VID_MASK);
+				le16_to_cpu(si_data->vid_vidm_tg
+					    & ENETC_CBDR_SID_VID_MASK);
 		streamid->para.nid.tagged =
 				le16_to_cpu(si_data->vid_vidm_tg >> 14 & 0x3);
 	} else if (streamid->type == 2) {
-		streamid->para.sid.smac = si_data->mac[0] + ((u64)si_data->mac[1] << 8)
-			+ ((u64)si_data->mac[2] << 16) + ((u64)si_data->mac[3] << 24)
-			+ ((u64)si_data->mac[4] << 32) + ((u64)si_data->mac[5] << 40);
+		streamid->para.sid.smac = si_data->mac[0]
+			+ ((u64)si_data->mac[1] << 8)
+			+ ((u64)si_data->mac[2] << 16)
+			+ ((u64)si_data->mac[3] << 24)
+			+ ((u64)si_data->mac[4] << 32)
+			+ ((u64)si_data->mac[5] << 40);
 		/* VID Match. If set (b1) then the VID must match, otherwise
 		 * any VID is considered a match.
 		 */
 		streamid->para.sid.vid =
-				le16_to_cpu(si_data->vid_vidm_tg & ENETC_CBDR_SID_VID_MASK);
+				le16_to_cpu(si_data->vid_vidm_tg
+					    & ENETC_CBDR_SID_VID_MASK);
 		streamid->para.sid.tagged =
 				le16_to_cpu(si_data->vid_vidm_tg >> 14 & 0x3);
 	}
@@ -716,20 +776,20 @@ int enetc_cb_streamid_counters_get(struct net_device *ndev, u32 index,
 void enetc_qci_enable(struct enetc_hw *hw)
 {
 	enetc_wr(hw, ENETC_PPSFPMR, enetc_rd(hw, ENETC_PPSFPMR)
-					| ENETC_PPSFPMR_PSFPEN | ENETC_PPSFPMR_VS
-					| ENETC_PPSFPMR_PVC | ENETC_PPSFPMR_PVZC);
+		 | ENETC_PPSFPMR_PSFPEN | ENETC_PPSFPMR_VS
+		 | ENETC_PPSFPMR_PVC | ENETC_PPSFPMR_PVZC);
 }
 
 void enetc_qci_disable(struct enetc_hw *hw)
 {
 	enetc_wr(hw, ENETC_PPSFPMR, enetc_rd(hw, ENETC_PPSFPMR)
-					& ~ENETC_PPSFPMR_PSFPEN & ~ENETC_PPSFPMR_VS
-					& ~ENETC_PPSFPMR_PVC & ~ENETC_PPSFPMR_PVZC);
+		 & ~ENETC_PPSFPMR_PSFPEN & ~ENETC_PPSFPMR_VS
+		 & ~ENETC_PPSFPMR_PVC & ~ENETC_PPSFPMR_PVZC);
 }
 
 /* CBD Class 8: Stream Filter Instance Set Descriptor - Short Format */
 int enetc_qci_sfi_set(struct net_device *ndev, u32 index, bool en,
-		struct tsn_qci_psfp_sfi_conf *tsn_qci_sfi)
+		      struct tsn_qci_psfp_sfi_conf *tsn_qci_sfi)
 {
 	struct enetc_cbd *cbdr;
 	struct sfi_conf *sfi_config;
@@ -797,7 +857,7 @@ int enetc_qci_sfi_set(struct net_device *ndev, u32 index, bool en,
 
 /* CBD Class 8: Stream Filter Instance Query Descriptor - Short Format */
 int enetc_qci_sfi_get(struct net_device *ndev, u32 index,
-						struct tsn_qci_psfp_sfi_conf *tsn_qci_sfi)
+		      struct tsn_qci_psfp_sfi_conf *tsn_qci_sfi)
 {
 	struct enetc_cbd *cbdr;
 	struct sfi_conf *sfi_config;
@@ -824,7 +884,8 @@ int enetc_qci_sfi_get(struct net_device *ndev, u32 index,
 		le16_to_cpu(sfi_config->sg_inst_table_index);
 
 	if (sfi_config->multi & 0x8)
-		tsn_qci_sfi->priority_spec = le16_to_cpu(sfi_config->multi & 0x7);
+		tsn_qci_sfi->priority_spec =
+			le16_to_cpu(sfi_config->multi & 0x7);
 	else
 		tsn_qci_sfi->priority_spec = -1;
 
@@ -862,7 +923,7 @@ int enetc_qci_sfi_get(struct net_device *ndev, u32 index,
  * Descriptor - Long Format
  */
 int enetc_qci_sfi_counters_get(struct net_device *ndev, u32 index,
-							struct tsn_qci_psfp_sfi_counters *counters)
+			       struct tsn_qci_psfp_sfi_counters *counters)
 {
 	struct enetc_cbd *cbdr;
 	struct enetc_ndev_priv *priv = netdev_priv(ndev);
@@ -879,11 +940,12 @@ int enetc_qci_sfi_counters_get(struct net_device *ndev, u32 index,
 	cbdr->status_flags = 0;
 
 	data_size = sizeof(struct sfi_counter_data);
-	sfi_counter_data = (struct sfi_counter_data *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
-	if (sfi_counter_data == NULL)
+	sfi_counter_data = kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (!sfi_counter_data)
 		return -ENOMEM;
 
-	dma = dma_map_single(&priv->si->pdev->dev, sfi_counter_data, data_size, DMA_FROM_DEVICE);
+	dma = dma_map_single(&priv->si->pdev->dev, sfi_counter_data,
+			     data_size, DMA_FROM_DEVICE);
 	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
 		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
 		kfree(sfi_counter_data);
@@ -908,26 +970,27 @@ int enetc_qci_sfi_counters_get(struct net_device *ndev, u32 index,
 	counters->passing_sdu_count = counters->matching_frames_count
 				- counters->not_passing_sdu_count;
 
-	counters->not_passing_frames_count = ((u64)le32_to_cpu(sfi_counter_data->stream_gate_droph) << 32)
-				+ le32_to_cpu(sfi_counter_data->stream_gate_dropl);
+	counters->not_passing_frames_count =
+		((u64)le32_to_cpu(sfi_counter_data->stream_gate_droph) << 32)
+		+ le32_to_cpu(sfi_counter_data->stream_gate_dropl);
 
 	counters->passing_frames_count = counters->matching_frames_count
 				- counters->not_passing_sdu_count
 				- counters->not_passing_frames_count;
 
-	counters->red_frames_count = ((u64)le32_to_cpu(sfi_counter_data->flow_meter_droph) << 32)
-				+ le32_to_cpu(sfi_counter_data->flow_meter_dropl);
+	counters->red_frames_count =
+		((u64)le32_to_cpu(sfi_counter_data->flow_meter_droph) << 32)
+		+ le32_to_cpu(sfi_counter_data->flow_meter_dropl);
 
 	memset(cbdr, 0, sizeof(*cbdr));
 	return 0;
 }
 
-/*
- * CBD Class 9: Stream Gate Instance Table Entry Set
+/* CBD Class 9: Stream Gate Instance Table Entry Set
  * Descriptor - Short Format
  */
 int enetc_qci_sgi_set(struct net_device *ndev, u32 index,
-				struct tsn_qci_psfp_sgi_conf *tsn_qci_sgi)
+		      struct tsn_qci_psfp_sgi_conf *tsn_qci_sgi)
 {
 	struct enetc_cbd *cbdr, *cbdr_sgcl;
 	struct sgi_table *sgi_config;
@@ -970,9 +1033,10 @@ int enetc_qci_sgi_set(struct net_device *ndev, u32 index,
 	sgi_config->ocgtst = tsn_qci_sgi->admin.control_list_length ?
 			0x80 : (tsn_qci_sgi->admin.gate_states ? 0x80 : 0x0);
 
-	sgi_config->oipv = tsn_qci_sgi->admin.control_list_length ?
-			0x0 : ((tsn_qci_sgi->admin.init_ipv < 0) ?
-					0x0 : ((tsn_qci_sgi->admin.init_ipv & 0x7) | 0x8));
+	sgi_config->oipv =
+		tsn_qci_sgi->admin.control_list_length ?
+		0x0 : ((tsn_qci_sgi->admin.init_ipv < 0) ?
+		       0x0 : ((tsn_qci_sgi->admin.init_ipv & 0x7) | 0x8));
 
 	sgi_config->en = 0x80;
 
@@ -1003,20 +1067,23 @@ int enetc_qci_sgi_set(struct net_device *ndev, u32 index,
 	/* tsn_qci_sgi->admin.control_list_length is not zero now */
 	if (tsn_qci_sgi->admin.control_list_length > 4)
 		return -EINVAL;
-	else
-		sgcl_config->acl_len = (tsn_qci_sgi->admin.control_list_length - 1) & 0x3;
 
-	data_size = sizeof(struct sgcl_data) +
-		(sgcl_config->acl_len + 1) * sizeof(struct sgce);
+	sgcl_config->acl_len =
+		(tsn_qci_sgi->admin.control_list_length - 1) & 0x3;
 
-	sgcl_data = (struct sgcl_data *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
-	if (sgcl_data == NULL)
+	data_size = struct_size(sgcl_data, sgcl,
+				tsn_qci_sgi->admin.control_list_length);
+
+	sgcl_data = kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (!sgcl_data)
 		return -ENOMEM;
 
 	dma_size = cpu_to_le16(data_size);
 	cbdr_sgcl->length = dma_size;
 
-	dma = dma_map_single(&priv->si->pdev->dev, sgcl_data, data_size, DMA_FROM_DEVICE);
+	dma = dma_map_single(&priv->si->pdev->dev,
+			     sgcl_data, data_size,
+			     DMA_FROM_DEVICE);
 	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
 		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
 		memset(cbdr, 0, sizeof(*cbdr));
@@ -1046,7 +1113,8 @@ int enetc_qci_sgi_set(struct net_device *ndev, u32 index,
 			temp_entry->multi |= 0x10;
 
 		if (temp_sgcl->ipv >= 0)
-			temp_entry->multi |= ((temp_sgcl->ipv & 0x7) << 5) | 0x08;
+			temp_entry->multi |= ((temp_sgcl->ipv & 0x7) << 5)
+						| 0x08;
 
 		if (temp_sgcl->octet_max)
 			temp_entry->multi |= 0x01;
@@ -1058,11 +1126,17 @@ int enetc_qci_sgi_set(struct net_device *ndev, u32 index,
 	}
 
 	if (!tsn_qci_sgi->admin.base_time) {
-		sgcl_data->btl = cpu_to_le32(enetc_rd(&priv->si->hw, ENETC_SICTR0));
-		sgcl_data->bth = cpu_to_le32(enetc_rd(&priv->si->hw, ENETC_SICTR1));
+		sgcl_data->btl =
+			cpu_to_le32(enetc_rd(&priv->si->hw, ENETC_SICTR0));
+		sgcl_data->bth =
+			cpu_to_le32(enetc_rd(&priv->si->hw, ENETC_SICTR1));
 	} else {
-		sgcl_data->bth = cpu_to_le32(upper_32_bits(tsn_qci_sgi->admin.base_time));
-		sgcl_data->btl = cpu_to_le32(lower_32_bits(tsn_qci_sgi->admin.base_time));
+		u32 tempu, templ;
+
+		tempu = upper_32_bits(tsn_qci_sgi->admin.base_time);
+		templ = lower_32_bits(tsn_qci_sgi->admin.base_time);
+		sgcl_data->bth = cpu_to_le32(tempu);
+		sgcl_data->btl = cpu_to_le32(templ);
 	}
 
 	xmit_cbdr(priv->si, curr_cbd);
@@ -1079,7 +1153,7 @@ exit:
  * Descriptor - Short Format
  */
 int enetc_qci_sgi_get(struct net_device *ndev, u32 index,
-				struct tsn_qci_psfp_sgi_conf *tsn_qci_sgi)
+		      struct tsn_qci_psfp_sgi_conf *tsn_qci_sgi)
 {
 	struct enetc_cbd *cbdr, *cbdr_sgcl;
 	struct sgi_table *sgi_config;
@@ -1103,7 +1177,8 @@ int enetc_qci_sgi_get(struct net_device *ndev, u32 index,
 
 	sgi_config = &cbdr->sgi_table;
 
-	tsn_qci_sgi->admin.gate_states = ((sgi_config->ocgtst & 0x80) ? true : false);
+	tsn_qci_sgi->admin.gate_states = (sgi_config->ocgtst & 0x80) ?
+						true : false;
 	if (sgi_config->oipv & 0x08)
 		tsn_qci_sgi->admin.init_ipv = sgi_config->oipv & 0x7;
 	else
@@ -1133,10 +1208,10 @@ int enetc_qci_sgi_get(struct net_device *ndev, u32 index,
 	cbdr_sgcl->cls = BDCR_CMD_STREAM_GCL;
 	cbdr_sgcl->status_flags = 0;
 
-	data_size = sizeof(struct sgcl_query_resp) + 4 * sizeof(struct sgce); /* Max is 4 */
+	data_size = sizeof(struct sgcl_query_resp) + 4 * sizeof(struct sgce);
 
-	sgcl_data = (struct sgcl_query_resp *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
-	if (sgcl_data == NULL)
+	sgcl_data = kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (!sgcl_data)
 		return -ENOMEM;
 
 	dma_size = cpu_to_le16(data_size);
@@ -1147,7 +1222,8 @@ int enetc_qci_sgi_get(struct net_device *ndev, u32 index,
 
 	sgcl_query->oacl_len = 0x10;
 
-	dma = dma_map_single(&priv->si->pdev->dev, sgcl_data, data_size, DMA_FROM_DEVICE);
+	dma = dma_map_single(&priv->si->pdev->dev, sgcl_data,
+			     data_size, DMA_FROM_DEVICE);
 	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
 		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
 		memset(cbdr, 0, sizeof(*cbdr));
@@ -1171,17 +1247,21 @@ int enetc_qci_sgi_get(struct net_device *ndev, u32 index,
 	else
 		tsn_qci_sgi->admin.init_ipv = -1;
 
-	/* admin_len can also get from gcl_data_stat bit 5,6 OR sgi_config->oacl_len */
+	/* admin_len can also get from gcl_data_stat bit 5,6
+	 * OR sgi_config->oacl_len
+	 */
 	admin_len = (sgcl_query->oacl_len & 0x3) + 1;
 	tsn_qci_sgi->admin.control_list_length = admin_len;
 	tsn_qci_sgi->admin.cycle_time = le32_to_cpu(sgcl_data->act);
 	tsn_qci_sgi->admin.cycle_time_extension = le32_to_cpu(sgcl_data->acte);
-	tsn_qci_sgi->admin.base_time = ((u64)(le32_to_cpu(sgcl_data->abth)) << 32)
-						+ le32_to_cpu(sgcl_data->abtl);
-
-	tsn_qci_sgi->admin.gcl =
-		kzalloc(admin_len * sizeof(struct tsn_qci_psfp_gcl), GFP_KERNEL);
-	if (tsn_qci_sgi->admin.gcl == NULL) {
+	tsn_qci_sgi->admin.base_time = ((u64)(le32_to_cpu(sgcl_data->abth))
+					      << 32)
+					+ le32_to_cpu(sgcl_data->abtl);
+
+	tsn_qci_sgi->admin.gcl = kcalloc(admin_len,
+					 sizeof(struct tsn_qci_psfp_gcl),
+					 GFP_KERNEL);
+	if (!tsn_qci_sgi->admin.gcl) {
 		kfree(sgcl_data);
 		return -ENOMEM;
 	}
@@ -1202,8 +1282,8 @@ int enetc_qci_sgi_get(struct net_device *ndev, u32 index,
 
 		if (temp_entry->multi & 0x01)
 			temp_sgcl->octet_max = (temp_entry->msdu[0] & 0xff)
-							| (((u32)temp_entry->msdu[1] << 8) & 0xff00)
-							| (((u32)temp_entry->msdu[1] << 16) & 0xff0000);
+				| (((u32)temp_entry->msdu[1] << 8) & 0xff00)
+				| (((u32)temp_entry->msdu[1] << 16) & 0xff0000);
 		else
 			temp_sgcl->octet_max = 0;
 	}
@@ -1216,10 +1296,11 @@ exit:
 	return 0;
 }
 
-/* CBD Class 9: Stream Gate Instance Table Entry Query Descriptor - Short Format */
-/* CBD Class 9: Stream Gate Control List Query Descriptor - Long Format */
+/* CBD Class 9: Stream Gate Instance Table Entry Query Descriptor
+ * CBD Class 9: Stream Gate Control List Query Descriptor
+ */
 int enetc_qci_sgi_status_get(struct net_device *ndev, u16 index,
-				struct tsn_psfp_sgi_status *status)
+			     struct tsn_psfp_sgi_status *status)
 {
 	struct enetc_cbd *cbdr_sgi, *cbdr_sgcl;
 	struct sgi_table *sgi_config;
@@ -1230,7 +1311,6 @@ int enetc_qci_sgi_status_get(struct net_device *ndev, u16 index,
 	dma_addr_t dma;
 	u16 data_size, dma_size, gcl_data_stat = 0;
 	u8 oper_len = 0;
-	u64 temp;
 	int curr_cbd, i;
 
 	curr_cbd = alloc_cbdr(priv->si, &cbdr_sgi);
@@ -1247,7 +1327,7 @@ int enetc_qci_sgi_status_get(struct net_device *ndev, u16 index,
 
 	status->oper.gate_states = ((sgi_config->ocgtst & 0x80) ? true : false);
 
-	/* Check gate list length is zero? */
+	/* Check gate list length is zero */
 	if (!(sgi_config->oacl_len & 0x30)) {
 		status->oper.control_list_length = 0;
 		goto cmd2quit;
@@ -1262,10 +1342,11 @@ int enetc_qci_sgi_status_get(struct net_device *ndev, u16 index,
 	cbdr_sgcl->cls = BDCR_CMD_STREAM_GCL;
 	cbdr_sgcl->status_flags = 0;
 
-	data_size = sizeof(struct sgcl_query_resp) + 4 * sizeof(struct sgce); /* Max is 4 */
+	/* Max size */
+	data_size = sizeof(struct sgcl_query_resp) + 4 * sizeof(struct sgce);
 
-	sgcl_data = (struct sgcl_query_resp *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
-	if (sgcl_data == NULL)
+	sgcl_data = kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (!sgcl_data)
 		return -ENOMEM;
 
 	dma_size = cpu_to_le16(data_size);
@@ -1276,7 +1357,8 @@ int enetc_qci_sgi_status_get(struct net_device *ndev, u16 index,
 
 	sgcl_query->oacl_len = 0x20;
 
-	dma = dma_map_single(&priv->si->pdev->dev, sgcl_data, data_size, DMA_FROM_DEVICE);
+	dma = dma_map_single(&priv->si->pdev->dev, sgcl_data,
+			     data_size, DMA_FROM_DEVICE);
 	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
 		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
 		memset(cbdr_sgi, 0, sizeof(*cbdr_sgi));
@@ -1291,13 +1373,16 @@ int enetc_qci_sgi_status_get(struct net_device *ndev, u16 index,
 
 	sgce = (struct sgce *)(sgcl_data + 1);
 
-	/* oper_len can also get from gcl_data_stat bit 5,6 OR sgi_config->oacl_len */
+	/* oper_len can also get from gcl_data_stat bit 5,6
+	 * OR sgi_config->oacl_len
+	 */
 	oper_len = ((sgcl_query->oacl_len & 0x0c) >> 2) + 1;
 
 	/* Get Stream Gate Control List */
 	status->oper.cycle_time = le32_to_cpu(sgcl_data->oct);
 	status->oper.cycle_time_extension = le32_to_cpu(sgcl_data->octe);
-	status->oper.base_time = le32_to_cpu(sgcl_data->obtl) + ((u64)le32_to_cpu(sgcl_data->obth) << 32);
+	status->oper.base_time = le32_to_cpu(sgcl_data->obtl)
+				+ ((u64)le32_to_cpu(sgcl_data->obth) << 32);
 	status->oper.control_list_length = oper_len;
 
 	gcl_data_stat = le16_to_cpu(sgcl_data->stat);
@@ -1309,9 +1394,10 @@ int enetc_qci_sgi_status_get(struct net_device *ndev, u16 index,
 	if (gcl_data_stat & 0x800)
 		status->oper.gate_states = true;
 
-	status->oper.gcl =
-		kzalloc(oper_len * sizeof(struct tsn_qci_psfp_gcl), GFP_KERNEL);
-	if (status->oper.gcl == NULL) {
+	status->oper.gcl = kcalloc(oper_len,
+				   sizeof(struct tsn_qci_psfp_gcl),
+				   GFP_KERNEL);
+	if (!status->oper.gcl) {
 		memset(cbdr_sgi, 0, sizeof(*cbdr_sgi));
 		memset(cbdr_sgcl, 0, sizeof(*cbdr_sgcl));
 		kfree(sgcl_data);
@@ -1334,13 +1420,16 @@ int enetc_qci_sgi_status_get(struct net_device *ndev, u16 index,
 
 		if (temp_entry->multi & 0x01)
 			temp_sgcl->octet_max = temp_entry->msdu[0]
-					| ((((u32)temp_entry->msdu[1]) << 8) & 0xff00)
-					| ((((u32)temp_entry->msdu[2]) << 16) & 0xff0000);
+					| ((((u32)temp_entry->msdu[1]) << 8)
+					   & 0xff00)
+					| ((((u32)temp_entry->msdu[2]) << 16)
+					   & 0xff0000);
 		else
 			temp_sgcl->octet_max = 0;
 	}
 
-	status->config_change_time = le32_to_cpu(sgcl_data->cctl) + ((u64)le32_to_cpu(sgcl_data->ccth) << 32);
+	status->config_change_time = le32_to_cpu(sgcl_data->cctl)
+				+ ((u64)le32_to_cpu(sgcl_data->ccth) << 32);
 
 	memset(cbdr_sgcl, 0, sizeof(*cbdr_sgcl));
 	kfree(sgcl_data);
@@ -1350,8 +1439,7 @@ cmd2quit:
 	status->tick_granularity = enetc_rd(&priv->si->hw, ENETC_SITGTGR);
 
 	/* current time */
-	temp = ((u64)enetc_rd(&priv->si->hw, ENETC_SICTR1)) << 32;
-	status->current_time = enetc_rd(&priv->si->hw, ENETC_SICTR0) + temp;
+	status->current_time = get_current_time(priv->si);
 
 	memset(cbdr_sgi, 0, sizeof(*cbdr_sgi));
 
@@ -1360,7 +1448,7 @@ cmd2quit:
 
 /* CBD Class 10: Flow Meter Instance Set Descriptor - Short Format */
 int enetc_qci_fmi_set(struct net_device *ndev, u32 index, bool enable,
-				struct tsn_qci_psfp_fmi *tsn_qci_fmi)
+		      struct tsn_qci_psfp_fmi *tsn_qci_fmi)
 {
 	struct enetc_cbd *cbdr;
 	struct fmi_conf *fmi_config;
@@ -1394,14 +1482,14 @@ int enetc_qci_fmi_set(struct net_device *ndev, u32 index, bool enable,
 	fmi_config = &cbdr->fmi_conf;
 	fmi_config->en = 0x80;
 	if (tsn_qci_fmi->cir) {
-		temp = (u64)1000 * tsn_qci_fmi->cir;
+		temp = (u64)1000000 * tsn_qci_fmi->cir;
 		temp = temp / 3725;
 	}
 	fmi_config->cir = cpu_to_le32((u32)temp);
 	fmi_config->cbs = cpu_to_le32(tsn_qci_fmi->cbs);
 	temp = 0;
 	if (tsn_qci_fmi->eir) {
-		temp = (u64)1000 * tsn_qci_fmi->eir;
+		temp = (u64)1000000 * tsn_qci_fmi->eir;
 		temp = temp / 3725;
 	}
 	fmi_config->eir = cpu_to_le32((u32)temp);
@@ -1430,8 +1518,8 @@ int enetc_qci_fmi_set(struct net_device *ndev, u32 index, bool enable,
 
 /* CBD Class 10: Flow Meter Instance Query Descriptor - Short Format */
 int enetc_qci_fmi_get(struct net_device *ndev, u32 index,
-						struct tsn_qci_psfp_fmi *tsn_qci_fmi,
-						struct tsn_qci_psfp_fmi_counters *counters)
+		      struct tsn_qci_psfp_fmi *tsn_qci_fmi,
+		      struct tsn_qci_psfp_fmi_counters *counters)
 {
 	struct enetc_cbd *cbdr;
 	struct fmi_conf *fmi_config;
@@ -1492,11 +1580,12 @@ int enetc_qci_fmi_get(struct net_device *ndev, u32 index,
 	cbdr->status_flags = 0x0;
 
 	data_size = sizeof(struct fmi_query_stat_resp);
-	fmi_counter_data = (struct fmi_query_stat_resp *)kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
-	if (fmi_counter_data == NULL)
+	fmi_counter_data = kzalloc(data_size, __GFP_DMA | GFP_KERNEL);
+	if (!fmi_counter_data)
 		return -ENOMEM;
 
-	dma = dma_map_single(&priv->si->pdev->dev, fmi_counter_data, data_size, DMA_FROM_DEVICE);
+	dma = dma_map_single(&priv->si->pdev->dev, fmi_counter_data,
+			     data_size, DMA_FROM_DEVICE);
 	if (dma_mapping_error(&priv->si->pdev->dev, dma)) {
 		netdev_err(priv->si->ndev, "DMA mapping failed!\n");
 		kfree(fmi_counter_data);
@@ -1521,18 +1610,23 @@ int enetc_qbu_set(struct net_device *ndev, u8 ptvector)
 	int i;
 	struct enetc_ndev_priv *priv = netdev_priv(ndev);
 
-	temp = enetc_rd(&priv->si->hw, QBV_PTGCR_OFFSET);
-	if (temp & QBV_TGE)
-		enetc_wr(&priv->si->hw, QBV_PTGCR_OFFSET, temp & (~QBV_TGPE));
+	temp = enetc_rd(&priv->si->hw, ENETC_QBV_PTGCR_OFFSET);
+	if (temp & ENETC_QBV_TGE)
+		enetc_wr(&priv->si->hw, ENETC_QBV_PTGCR_OFFSET,
+			 temp & (~ENETC_QBV_TGPE));
 
 	for (i = 0; i < 8; i++) {
 		/* 1 Enabled. Traffic is transmitted on the preemptive MAC. */
 		temp = enetc_port_rd(&priv->si->hw, ENETC_PTCFPR(i));
 
 		if ((ptvector >> i) & 0x1)
-			enetc_port_wr(&priv->si->hw, ENETC_PTCFPR(i), temp | ENETC_FPE);
+			enetc_port_wr(&priv->si->hw,
+				      ENETC_PTCFPR(i),
+				      temp | ENETC_FPE);
 		else
-			enetc_port_wr(&priv->si->hw, ENETC_PTCFPR(i), temp & ~ENETC_FPE);
+			enetc_port_wr(&priv->si->hw,
+				      ENETC_PTCFPR(i),
+				      temp & ~ENETC_FPE);
 	}
 
 	return 0;
@@ -1546,7 +1640,8 @@ int enetc_qbu_get(struct net_device *ndev,
 
 	if (enetc_port_rd(&priv->si->hw, ENETC_PFPMR) & ENETC_PFPMR_PMACE) {
 		preemptstat->preemption_active = true;
-		if (enetc_rd(&priv->si->hw, QBV_PTGCR_OFFSET) & QBV_TGE)
+		if (enetc_rd(&priv->si->hw, ENETC_QBV_PTGCR_OFFSET)
+							& ENETC_QBV_TGE)
 			preemptstat->hold_request = 1;
 		else
 			preemptstat->hold_request = 2;
@@ -1560,9 +1655,9 @@ int enetc_qbu_get(struct net_device *ndev,
 			preemptstat->admin_state |= 1 << i;
 
 	preemptstat->hold_advance =
-		enetc_rd(&priv->si->hw, QBV_PTGCR_OFFSET) & 0xFFFF;
+		enetc_rd(&priv->si->hw, ENETC_QBV_PTGCR_OFFSET) & 0xFFFF;
 	preemptstat->release_advance =
-		enetc_rd(&priv->si->hw, QBV_PTGCR_OFFSET) & 0xFFFF;
+		enetc_rd(&priv->si->hw, ENETC_QBV_PTGCR_OFFSET) & 0xFFFF;
 
 	return 0;
 }
@@ -1596,6 +1691,33 @@ u32 enetc_tsn_get_capability(struct net_device *ndev)
 	return __enetc_tsn_get_cap(priv->si);
 }
 
+static int  __enetc_get_max_cap(struct enetc_si *si,
+				struct tsn_qci_psfp_stream_param *stream_para)
+{
+	u32 reg = 0;
+
+	/* Port stream filter capability */
+	reg = enetc_port_rd(&si->hw, ENETC_PSFCAPR);
+	stream_para->max_sf_instance = reg & ENETC_PSFCAPR_MSK;
+	/* Port stream filter capability */
+	reg = enetc_port_rd(&si->hw, ENETC_PSGCAPR);
+	stream_para->max_sg_instance = (reg & ENETC_PSGCAPR_SGIT_MSK);
+	stream_para->supported_list_max = (reg & ENETC_PSGCAPR_GCL_MSK) >> 16;
+	/* Port flow meter capability */
+	reg = enetc_port_rd(&si->hw, ENETC_PFMCAPR);
+	stream_para->max_fm_instance = reg & ENETC_PFMCAPR_MSK;
+
+	return 0;
+}
+
+int enetc_get_max_capa(struct net_device *ndev,
+		      struct tsn_qci_psfp_stream_param *stream_para)
+{
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+
+	return __enetc_get_max_cap(priv->si, stream_para);
+}
+
 static int enetc_set_cbs(struct net_device *ndev, u8 tc, u8 bw)
 {
 	struct enetc_ndev_priv *priv = netdev_priv(ndev);
@@ -1616,7 +1738,9 @@ static int enetc_set_cbs(struct net_device *ndev, u8 tc, u8 bw)
 	if (!ecbs)
 		return -ENOMEM;
 
-	port_transmit_rate = ecbs->port_transmit_rate;
+	port_transmit_rate = priv->speed;
+	if (port_transmit_rate != ecbs->port_transmit_rate)
+		ecbs->port_transmit_rate = port_transmit_rate;
 	port_frame_max_size = ecbs->port_max_size_frame;
 	tc_nums = ecbs->tc_nums;
 	cbs = ecbs->cbs;
@@ -1688,6 +1812,9 @@ static int enetc_set_cbs(struct net_device *ndev, u8 tc, u8 bw)
 		cbs[tc].max_interfrence_size = port_frame_max_size * 8;
 
 	} else if (tc == tc_nums - 2) {
+		if (!cbs[tc + 1].send_slope)
+			return -1;
+
 		cbs[tc].max_interfrence_size = (port_frame_max_size
 				+ cbs[tc + 1].tc_max_sized_frame
 				+ port_frame_max_size * (cbs[tc + 1].idle_slope
@@ -1699,11 +1826,17 @@ static int enetc_set_cbs(struct net_device *ndev, u8 tc, u8 bw)
 			send_slope += cbs[i].send_slope;
 			max_interfrence_size += cbs[i].tc_max_sized_frame;
 		}
+		if (!send_slope)
+			return -1;
+
 		max_interfrence_size = ((u64)port_transmit_rate
 				* max_interfrence_size) / send_slope;
 		cbs[tc].max_interfrence_size = max_interfrence_size * 8;
 	}
 
+	if (!port_transmit_rate)
+		return -1;
+
 	cbs[tc].hi_credit = cbs[tc].max_interfrence_size * cbs[tc].bw / 100;
 	cbs[tc].lo_credit = cbs[tc].tc_max_sized_frame * (cbs[tc].send_slope
 			/ port_transmit_rate);
@@ -1739,9 +1872,6 @@ static int enetc_get_cbs(struct net_device *ndev, u8 tc)
 	return cbs[tc].bw;
 }
 
-#define GET_CURRENT_TIME(si) (enetc_rd(&(si)->hw, ENETC_SICTR0) \
-		| ((u64)enetc_rd(&(si)->hw, ENETC_SICTR1) << 32))
-
 static int enetc_set_tsd(struct net_device *ndev, struct tsn_tsd *ttsd)
 {
 	return 0;
@@ -1752,35 +1882,12 @@ static int enetc_get_tsd(struct net_device *ndev, struct tsn_tsd_status *tts)
 	return 0;
 }
 
-static u32 get_ndev_speed(struct net_device *netdev)
-{
-	struct ethtool_link_ksettings ksettings;
-	int rc = -1;
-	if (netdev->ethtool_ops->get_link_ksettings) {
-
-		if (netdev->ethtool_ops->begin) {
-			if ((rc = netdev->ethtool_ops->begin(netdev) < 0))
-				return 0;
-		}
-
-		memset(&ksettings, 0, sizeof(ksettings));
-
-		if (!netdev->ethtool_ops->get_link_ksettings)
-			return 0;
-
-		rc = netdev->ethtool_ops->get_link_ksettings(netdev, &ksettings);
-
-		if (netdev->ethtool_ops->complete)
-			netdev->ethtool_ops->complete(netdev);
-	}
-	return (rc < 0) ? 0 : ksettings.base.speed;
-}
-
 static void enetc_cbs_init(struct enetc_si *si)
 {
+	struct enetc_ndev_priv *priv = netdev_priv(si->ndev);
 	u8 tc_nums;
 
-	tc_nums = ((enetc_port_rd(&si->hw, ENETC_PCAPR1) >> 4) & 0x7) + 1;
+	tc_nums = priv->num_tx_rings;
 	si->ecbs = kzalloc(sizeof(*si->ecbs) +
 			   sizeof(struct cbs) * tc_nums, GFP_KERNEL);
 	if (!si->ecbs)
@@ -1789,7 +1896,7 @@ static void enetc_cbs_init(struct enetc_si *si)
 	si->ecbs->port_max_size_frame = si->ndev->mtu + ETH_HLEN
 						+ VLAN_HLEN + ETH_FCS_LEN;
 	si->ecbs->tc_nums = tc_nums;
-	si->ecbs->port_transmit_rate = get_ndev_speed(si->ndev);
+	si->ecbs->port_transmit_rate = priv->speed;
 
 	/*This trick is used only for CFP*/
 	if (!si->ecbs->port_transmit_rate)
@@ -1800,14 +1907,15 @@ static void enetc_cbs_init(struct enetc_si *si)
 		kfree(si->ecbs);
 		si->ecbs = NULL;
 	}
-
-	return;
 }
 
 static void enetc_qbv_init(struct enetc_hw *hw)
 {
 	/* Set PSPEED to be 1Gbps */
-	enetc_port_wr(hw, ENETC_PMR, (enetc_port_rd(hw, ENETC_PMR) & (~0xf00)) | 0x200);
+	enetc_port_wr(hw, ENETC_PMR,
+		      (enetc_port_rd(hw, ENETC_PMR)
+		      & (~ENETC_PMR_PSPEED_MASK))
+		      | ENETC_PMR_PSPEED_1000M);
 }
 
 void enetc_tsn_init(struct net_device *ndev)
@@ -1832,7 +1940,10 @@ void enetc_tsn_init(struct net_device *ndev)
 
 void enetc_tsn_deinit(struct net_device *ndev)
 {
-	return;
+	struct enetc_ndev_priv *priv = netdev_priv(ndev);
+	struct enetc_si *si = priv->si;
+
+	dev_info(&si->pdev->dev, "%s: release\n", __func__);
 }
 
 static struct tsn_ops enetc_tsn_ops_full = {
@@ -1845,6 +1956,7 @@ static struct tsn_ops enetc_tsn_ops_full = {
 	.cb_streamid_set = enetc_cb_streamid_set,
 	.cb_streamid_get = enetc_cb_streamid_get,
 	.cb_streamid_counters_get = enetc_cb_streamid_counters_get,
+	.qci_get_maxcap = enetc_get_max_capa,
 	.qci_sfi_set = enetc_qci_sfi_set,
 	.qci_sfi_get = enetc_qci_sfi_get,
 	.qci_sfi_counters_get = enetc_qci_sfi_counters_get,
@@ -1868,6 +1980,7 @@ static struct tsn_ops enetc_tsn_ops_part = {
 	.cb_streamid_set = enetc_cb_streamid_set,
 	.cb_streamid_get = enetc_cb_streamid_get,
 	.cb_streamid_counters_get = enetc_cb_streamid_counters_get,
+	.qci_get_maxcap = enetc_get_max_capa,
 	.qci_sfi_set = enetc_qci_sfi_set,
 	.qci_sfi_get = enetc_qci_sfi_get,
 	.qci_sfi_counters_get = enetc_qci_sfi_counters_get,
@@ -1881,15 +1994,17 @@ static struct tsn_ops enetc_tsn_ops_part = {
 void enetc_tsn_pf_init(struct net_device *netdev, struct pci_dev *pdev)
 {
 	int port = pdev->devfn & 0x7;
+
 	if (port == 1 || port == 3)
-		tsn_port_register(netdev, &enetc_tsn_ops_part, (u16)pdev->bus->number);
+		tsn_port_register(netdev, &enetc_tsn_ops_part,
+				  (u16)pdev->bus->number);
 	else
-		tsn_port_register(netdev, &enetc_tsn_ops_full, (u16)pdev->bus->number);
+		tsn_port_register(netdev, &enetc_tsn_ops_full,
+				  (u16)pdev->bus->number);
 }
 
 void enetc_tsn_pf_deinit(struct net_device *netdev)
 {
 	tsn_port_unregister(netdev);
-
 }
 #endif	/* #if IS_ENABLED(CONFIG_ENETC_TSN) */
diff --git a/devices/enetc/enetc_vf.c b/devices/enetc/enetc_vf.c
index 0402e18..43bbaf4 100644
--- a/devices/enetc/enetc_vf.c
+++ b/devices/enetc/enetc_vf.c
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: (GPL-2.0+ OR BSD-3-Clause)
-/* Copyright 2017-2019 NXP */
+/* Copyright 2017-2021 NXP */
 
 #include <linux/module.h>
 #include "enetc.h"
@@ -14,105 +14,12 @@ static const char enetc_drv_ver[] = ENETC_DRV_VER_STR;
 #define ENETC_DRV_NAME_STR "ENETC VF driver"
 static const char enetc_drv_name[] = ENETC_DRV_NAME_STR;
 
-/* Messaging */
-static void enetc_msg_vsi_write_msg(struct enetc_hw *hw,
-				    struct enetc_msg_swbd *msg)
-{
-	u32 val;
-
-	val = enetc_vsi_set_msize(msg->size) | lower_32_bits(msg->dma);
-	enetc_wr(hw, ENETC_VSIMSGSNDAR1, upper_32_bits(msg->dma));
-	enetc_wr(hw, ENETC_VSIMSGSNDAR0, val);
-}
-
-static int enetc_msg_vsi_send(struct enetc_si *si, struct enetc_msg_swbd *msg)
-{
-	int timeout = 100;
-	u32 vsimsgsr;
-
-	enetc_msg_vsi_write_msg(&si->hw, msg);
-
-	do {
-		vsimsgsr = enetc_rd(&si->hw, ENETC_VSIMSGSR);
-		if (!(vsimsgsr & ENETC_VSIMSGSR_MB))
-			break;
-
-		usleep_range(1000, 2000);
-	} while (--timeout);
-
-	if (!timeout)
-		return -ETIMEDOUT;
-
-	/* check for message delivery error */
-	if (vsimsgsr & ENETC_VSIMSGSR_MS) {
-		dev_err(&si->pdev->dev, "VSI command execute error: %d\n",
-			ENETC_SIMSGSR_GET_MC(vsimsgsr));
-		return -EIO;
-	}
-
-	return 0;
-}
-
-static int enetc_msg_vsi_set_primary_mac_addr(struct enetc_ndev_priv *priv,
-					      struct sockaddr *saddr)
-{
-	struct enetc_msg_cmd_set_primary_mac *cmd;
-	struct enetc_msg_swbd msg;
-	int err;
-
-	msg.size = ALIGN(sizeof(struct enetc_msg_cmd_set_primary_mac), 64);
-	msg.vaddr = dma_zalloc_coherent(priv->dev, msg.size, &msg.dma,
-					GFP_KERNEL);
-	if (!msg.vaddr) {
-		dev_err(priv->dev, "Failed to alloc Tx msg (size: %d)\n",
-			msg.size);
-		return -ENOMEM;
-	}
-
-	cmd = (struct enetc_msg_cmd_set_primary_mac *)msg.vaddr;
-	cmd->header.type = ENETC_MSG_CMD_MNG_MAC;
-	cmd->header.id = ENETC_MSG_CMD_MNG_ADD;
-	memcpy(&cmd->mac, saddr, sizeof(struct sockaddr));
-
-	/* send the command and wait */
-	err = enetc_msg_vsi_send(priv->si, &msg);
-
-	dma_free_coherent(priv->dev, msg.size, msg.vaddr, msg.dma);
-
-	return err;
-}
-
-static int enetc_vf_set_mac_addr(struct net_device *ndev, void *addr)
-{
-	struct enetc_ndev_priv *priv = netdev_priv(ndev);
-	struct sockaddr *saddr = addr;
-	int err;
-
-	if (!is_valid_ether_addr(saddr->sa_data))
-		return -EADDRNOTAVAIL;
-
-	err = enetc_msg_vsi_set_primary_mac_addr(priv, saddr);
-	if (err)
-		return err;
-
-	return 0;
-}
-
-static int enetc_vf_set_features(struct net_device *ndev,
-				 netdev_features_t features)
-{
-	return enetc_set_features(ndev, features);
-}
 
 /* Probing/ Init */
 static const struct net_device_ops enetc_ndev_ops = {
 	.ndo_open		= enetc_open,
 	.ndo_stop		= enetc_close,
 	.ndo_start_xmit		= enetc_xmit,
-	.ndo_get_stats		= enetc_get_stats,
-	.ndo_set_mac_address	= enetc_vf_set_mac_addr,
-	.ndo_set_features	= enetc_vf_set_features,
-	.ndo_do_ioctl		= enetc_ioctl,
 };
 
 static void enetc_vf_netdev_setup(struct enetc_si *si, struct net_device *ndev,
@@ -163,6 +70,7 @@ static int enetc_vf_probe(struct pci_dev *pdev,
 			  const struct pci_device_id *ent)
 {
 	struct enetc_ndev_priv *priv;
+	struct pci_dev *ptp_pdev;
 	struct net_device *ndev;
 	struct enetc_si *si;
 	int err;
@@ -196,36 +104,32 @@ static int enetc_vf_probe(struct pci_dev *pdev,
 		goto err_alloc_si_res;
 	}
 
-	err = enetc_alloc_msix(priv);
-	if (err) {
-		dev_err(&pdev->dev, "MSIX alloc failed\n");
-		goto err_alloc_msix;
+	ptp_pdev = pci_get_device(PCI_VENDOR_ID_FREESCALE, ENETC_DEV_ID_PTP,
+				  NULL);
+	if (!ptp_pdev) {
+		dev_err(&pdev->dev, "no PTP device found\n");
+		err = -ENODEV;
+		goto err_get_ptp;
 	}
 
-	priv->ecdev = ecdev_offer(ndev, ec_poll, THIS_MODULE);
-	if (!priv->ecdev) {
-		err = register_netdev(ndev);
-		if (err)
-			goto err_reg_netdev;
-	}
+	priv->ptp_dev = &ptp_pdev->dev;
+
+	if (enetc_alloc_rings(priv) < 0)
+		goto err_get_ptp;
 
-	if (priv->ecdev) {
-		if (ecdev_open(priv->ecdev)) {
-			ecdev_withdraw(priv->ecdev);
-			goto err_reg_netdev;
-		}
-	} else {
-		netif_carrier_off(ndev);
+	priv->ecdev = ecdev_offer(ndev, ec_poll, THIS_MODULE);
+	if (ecdev_open(priv->ecdev)) {
+		ecdev_withdraw(priv->ecdev);
+		goto err_reg_ec_net;
 	}
 
 	netif_info(priv, probe, ndev, "%s v%s\n",
 		   enetc_drv_name, enetc_drv_ver);
 
 	return 0;
-
-err_reg_netdev:
-	enetc_free_msix(priv);
-err_alloc_msix:
+err_reg_ec_net:
+	enetc_free_rings(priv);
+err_get_ptp:
 	enetc_free_si_resources(priv);
 err_alloc_si_res:
 	si->ndev = NULL;
@@ -245,17 +149,11 @@ static void enetc_vf_remove(struct pci_dev *pdev)
 	netif_info(priv, drv, si->ndev, "%s v%s remove\n",
 		   enetc_drv_name, enetc_drv_ver);
 	
-	if (priv->ecdev){
-		ecdev_close(priv->ecdev);
-		ecdev_withdraw(priv->ecdev);
-	}
-	else{
-		unregister_netdev(si->ndev);
-	}
-	enetc_free_msix(priv);
+	ecdev_close(priv->ecdev);
+	ecdev_withdraw(priv->ecdev);
 
 	enetc_free_si_resources(priv);
-
+	enetc_free_rings(priv);
 	free_netdev(si->ndev);
 
 	enetc_pci_remove(pdev);
-- 
2.25.1

